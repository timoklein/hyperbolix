{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Hyperbolix","text":"<p>Hyperbolic Deep Learning in JAX</p> <p>Hyperbolix is a pure JAX implementation of hyperbolic deep learning, providing manifold operations, neural network layers, and Riemannian optimizers for hyperbolic geometry. Built with Flax NNX and Optax for modern JAX workflows.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>3 Manifolds: Euclidean, Poincar\u00e9 Ball, and Hyperboloid with complete geometric operations</li> <li>Neural Network Layers: 13+ hyperbolic layers including linear, convolutional, and regression layers</li> <li>Activation Functions: 4 hyperbolic activations (ReLU, Leaky ReLU, Tanh, Swish)</li> <li>Riemannian Optimizers: RAdam and RSGD with automatic manifold parameter detection</li> <li>Wrapped Normal Distributions: For probabilistic modeling on hyperbolic manifolds</li> <li>Pure JAX/Flax NNX: No PyTorch dependency, fully compatible with JAX ecosystem</li> <li>vmap-native API: Efficient batching through JAX's functional paradigm</li> <li>JIT-compatible: All operations support JIT compilation for performance</li> <li>Comprehensive Test Suite: 1,400+ tests with 100% pass rate</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom hyperbolix.manifolds import poincare\n\n# Create points on the Poincar\u00e9 ball\nx = jnp.array([0.1, 0.2])\ny = jnp.array([0.3, -0.1])\nc = 1.0  # Curvature parameter\n\n# Compute distance (single point operation)\ndistance = poincare.dist(x, y, c, version_idx=0)\nprint(f\"Distance: {distance}\")\n\n# Batch operations with vmap\nx_batch = jax.random.normal(jax.random.PRNGKey(0), (100, 2)) * 0.3\ny_batch = jax.random.normal(jax.random.PRNGKey(1), (100, 2)) * 0.3\n\n# Project to manifold and compute pairwise distances\nx_proj = jax.vmap(poincare.proj, in_axes=(0, None))(x_batch, c)\ny_proj = jax.vmap(poincare.proj, in_axes=(0, None))(y_batch, c)\ndistances = jax.vmap(poincare.dist, in_axes=(0, 0, None, None))(x_proj, y_proj, c, 0)\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<p>Install from source:</p> <pre><code>git clone https://github.com/hyperbolix/hyperbolix.git\ncd hyperbolix\nuv sync  # or pip install -e .\n</code></pre> <p>Requirements: Python 3.12+, JAX, Flax NNX, Optax</p>"},{"location":"#architecture","title":"Architecture","text":"<p>Hyperbolix follows a pure functional design:</p> <pre><code># Pure functions, no classes\nimport hyperbolix.manifolds.poincare as poincare\ndistance = poincare.dist(x, y, c, version)\n\n# Neural network layers as Flax NNX modules\nfrom flax import nnx\nfrom hyperbolix.nn_layers import HypLinearPoincare\n\nmodel = HypLinearPoincare(\n    manifold_module=poincare,\n    in_dim=32,\n    out_dim=16,\n    rngs=nnx.Rngs(0)\n)\noutput = model(input_data, c=1.0)\n</code></pre>"},{"location":"#project-status","title":"Project Status","text":"<p>100% Vibe-Coded Project</p> <p>This project was developed entirely through AI-assisted coding (\"vibe coding\"). While we have extensive test coverage (1,400+ tests), bugs and errors should be expected. Use with appropriate caution in production environments.</p> <p>All core functionality is complete!</p> <ul> <li>\u2705 Phase 1: Manifolds (978 passing tests)</li> <li>\u2705 Phase 2: Riemannian Optimizers (20 passing tests)</li> <li>\u2705 Phase 3a: Neural Network Layers (44 passing tests)</li> <li>\u2705 Phase 3b: Regression Layers (22 passing tests)</li> <li>\u2705 Hyperboloid Convolutions (68 passing tests)</li> <li>\u2705 Lorentz Convolutions (66 passing tests)</li> <li>\u2705 Hyperboloid Activations (86 passing tests)</li> <li>\u2705 CI/CD Pipeline with benchmarking</li> <li>\u2705 Clean, unified codebase structure</li> </ul>"},{"location":"#key-concepts","title":"Key Concepts","text":""},{"location":"#vmap-native-api","title":"vmap-native API","text":"<p>Functions operate on single points by design. Use <code>jax.vmap</code> for batching:</p> <pre><code># Single point operation\nresult = poincare.expmap(x, v, c)\n\n# Batched operation\nbatch_result = jax.vmap(poincare.expmap, in_axes=(0, 0, None))(x_batch, v_batch, c)\n</code></pre> <p>This design enables efficient JIT compilation and clear semantics.</p>"},{"location":"#curvature-parameter","title":"Curvature Parameter","text":"<p>The curvature <code>c</code> is passed at call time, not stored in objects:</p> <pre><code># Different curvatures for different calls\ndist_c1 = poincare.dist(x, y, c=1.0, version_idx=0)\ndist_c2 = poincare.dist(x, y, c=2.0, version_idx=0)\n</code></pre> <p>This allows for learnable curvature in neural networks.</p>"},{"location":"#manifold-operations","title":"Manifold Operations","text":"<p>Each manifold provides:</p> <ul> <li>proj: Project points onto the manifold</li> <li>dist: Compute distances (multiple versions for numerical stability)</li> <li>expmap/logmap: Exponential and logarithmic maps</li> <li>ptransp: Parallel transport</li> <li>egrad2rgrad: Convert Euclidean to Riemannian gradients</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started: Installation and first examples</li> <li>User Guide: Core concepts and patterns</li> <li>Tutorials: Hands-on learning</li> <li>API Reference: Complete API documentation</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use Hyperbolix in your research, please cite:</p> <pre><code>@software{hyperbolix2026,\n  title = {Hyperbolix: Hyperbolic Deep Learning in JAX},\n  author = {Klein, Timo and Lang, Thomas and Shkabrii, Andrii},\n  year = {2026},\n  url = {https://github.com/hyperbolix/hyperbolix}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>MIT License. See LICENSE for details.</p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>This library implements methods from several research papers:</p> <ul> <li>Ganea et al. (2018): \"Hyperbolic Neural Networks\"</li> <li>B\u00e9cigneul &amp; Ganea (2019): \"Riemannian Adaptive Optimization Methods\"</li> <li>Bdeir et al. (2023): \"Fully Hyperbolic Convolutional Neural Networks\"</li> <li>And many others (see references in individual modules)</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to Hyperbolix will be documented in this file.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>MkDocs Material documentation system</li> <li>Complete API reference documentation</li> <li>Getting Started guide</li> <li>CI/CD workflow for documentation builds</li> </ul>"},{"location":"changelog/#010-2026-01","title":"0.1.0 - 2026-01","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Pure JAX implementation of hyperbolic manifolds (Euclidean, Poincar\u00e9, Hyperboloid)</li> <li>13+ neural network layers (linear, convolutional, regression)</li> <li>4 hyperbolic activation functions (ReLU, Leaky ReLU, Tanh, Swish)</li> <li>Riemannian optimizers (RSGD, RAdam) with automatic manifold detection</li> <li>Wrapped normal distributions for VAEs</li> <li>HoroPCA for dimensionality reduction</li> <li>Comprehensive test suite (1,400+ tests)</li> <li>CI/CD pipeline with benchmarking</li> <li>vmap-native functional API design</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Migrated from PyTorch to pure JAX/Flax NNX</li> <li>Unified package structure: <code>hyperbolix_jax</code> \u2192 <code>hyperbolix</code></li> </ul>"},{"location":"changelog/#references","title":"References","text":"<ul> <li>Based on research by Ganea et al. (2018), B\u00e9cigneul &amp; Ganea (2019), Bdeir et al. (2023)</li> </ul>"},{"location":"developer-guide/","title":"Developer Guide","text":"<p>Contributing to Hyperbolix development.</p> <p>For detailed development instructions, see DEVELOPER_GUIDE.md in the repository.</p>"},{"location":"developer-guide/#quick-links","title":"Quick Links","text":"<ul> <li>Setup: Environment configuration with <code>uv</code></li> <li>Testing: Running test suites and benchmarks</li> <li>Linting: Pre-commit hooks and code quality</li> <li>CI/CD: GitHub Actions pipeline</li> <li>Contributing: Pull request guidelines</li> </ul>"},{"location":"developer-guide/#key-commands","title":"Key Commands","text":"<pre><code># Install dependencies\nuv sync --dev\n\n# Run tests\nuv run pytest tests/ -v\n\n# Run benchmarks\nuv run pytest benchmarks/ -v\n\n# Linting and formatting\nuv run pre-commit run --all-files\n\n# Type checking\nuv run pyright hyperbolix/\n</code></pre>"},{"location":"developer-guide/#project-structure","title":"Project Structure","text":"<pre><code>hyperbolix/\n\u251c\u2500\u2500 hyperbolix/           # Source code\n\u2502   \u251c\u2500\u2500 manifolds/        # Core geometry\n\u2502   \u251c\u2500\u2500 nn_layers/        # Neural network layers\n\u2502   \u251c\u2500\u2500 optim/            # Riemannian optimizers\n\u2502   \u251c\u2500\u2500 distributions/    # Probability distributions\n\u2502   \u2514\u2500\u2500 utils/            # Utilities\n\u251c\u2500\u2500 tests/                # Test suite\n\u251c\u2500\u2500 benchmarks/           # Performance benchmarks\n\u2514\u2500\u2500 docs/                 # Documentation source\n</code></pre>"},{"location":"developer-guide/#contributing","title":"Contributing","text":"<p>We welcome contributions! Please:</p> <ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes with tests</li> <li>Run pre-commit checks</li> <li>Submit a pull request</li> </ol> <p>See the full DEVELOPER_GUIDE.md for details.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you install Hyperbolix and run your first examples.</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#requirements","title":"Requirements","text":"<ul> <li>Python 3.12 or higher</li> <li>JAX 0.4.20+ (with CPU or GPU support)</li> <li>Flax NNX 0.12.0+</li> </ul>"},{"location":"getting-started/#install-from-source","title":"Install from Source","text":"<pre><code>git clone https://github.com/hyperbolix/hyperbolix.git\ncd hyperbolix\nuv sync  # or pip install -e .\n</code></pre> <p>For GPU support, install JAX with CUDA:</p> <pre><code>uv pip install \"jax[cuda12]&gt;=0.4.20\"\n</code></pre>"},{"location":"getting-started/#quick-start-distance-computation","title":"Quick Start: Distance Computation","text":"<p>Let's compute distances on the Poincar\u00e9 ball:</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom hyperbolix.manifolds import poincare\n\n# Create two points\nx = jnp.array([0.1, 0.2])\ny = jnp.array([0.3, -0.1])\nc = 1.0  # Curvature\n\n# Project to manifold (ensures points lie on Poincar\u00e9 ball)\nx_proj = poincare.proj(x, c)\ny_proj = poincare.proj(y, c)\n\n# Compute hyperbolic distance\ndistance = poincare.dist(x_proj, y_proj, c, version_idx=0)\nprint(f\"Distance: {distance:.4f}\")\n</code></pre>"},{"location":"getting-started/#batching-with-vmap","title":"Batching with vmap","text":"<p>Hyperbolix uses a vmap-native API: functions operate on single points, and you use <code>jax.vmap</code> for batching:</p> <pre><code># Batch of 100 points\nkey = jax.random.PRNGKey(0)\nx_batch = jax.random.normal(key, (100, 2)) * 0.3\ny_batch = jax.random.normal(jax.random.PRNGKey(1), (100, 2)) * 0.3\n\n# Project each point (batched operation)\nx_proj = jax.vmap(poincare.proj, in_axes=(0, None))(x_batch, c)\ny_proj = jax.vmap(poincare.proj, in_axes=(0, None))(y_batch, c)\n\n# Compute pairwise distances\ndistances = jax.vmap(poincare.dist, in_axes=(0, 0, None, None))(\n    x_proj, y_proj, c, 0\n)\nprint(f\"Distances shape: {distances.shape}\")  # (100,)\n</code></pre>"},{"location":"getting-started/#key-concepts","title":"Key Concepts","text":""},{"location":"getting-started/#manifolds","title":"Manifolds","text":"<p>Hyperbolix provides three manifold types:</p> <ul> <li>Euclidean: Flat space (baseline)</li> <li>Poincar\u00e9 Ball: Conformal model (angles preserved)</li> <li>Hyperboloid: Lorentz model (natural for convolutions)</li> </ul>"},{"location":"getting-started/#curvature-parameter","title":"Curvature Parameter","text":"<p>The curvature <code>c</code> controls the \"amount of hyperbolicity\":</p> <ul> <li><code>c = 0</code>: Euclidean space (flat)</li> <li><code>c = 1</code>: Unit curvature (standard hyperbolic space)</li> <li><code>c &gt; 1</code>: Higher curvature (more curved)</li> </ul> <p>Pass <code>c</code> at call time for maximum flexibility:</p> <pre><code># Different curvatures\ndist_c1 = poincare.dist(x, y, c=1.0, version_idx=0)\ndist_c2 = poincare.dist(x, y, c=2.0, version_idx=0)\n</code></pre>"},{"location":"getting-started/#version-parameter","title":"Version Parameter","text":"<p>Many manifold functions accept a <code>version</code> parameter for numerical stability:</p> <pre><code># Poincar\u00e9 distance has 4 versions\ndist_v0 = poincare.dist(x, y, c, version_idx=0)  # Fastest\ndist_v1 = poincare.dist(x, y, c, version_idx=1)  # Metric tensor\ndist_v2 = poincare.dist(x, y, c, version_idx=2)  # Lorentzian proxy\ndist_v3 = poincare.dist(x, y, c, version_idx=3)  # Conformal factor\n</code></pre>"},{"location":"getting-started/#building-a-neural-network","title":"Building a Neural Network","text":"<p>Here's a simple 2-layer hyperbolic network:</p> <pre><code>from flax import nnx\nfrom hyperbolix.nn_layers import HypLinearPoincare\nfrom hyperbolix.manifolds import poincare\n\nclass SimpleHypNet(nnx.Module):\n    def __init__(self, rngs):\n        self.layer1 = HypLinearPoincare(\n            manifold_module=poincare,\n            in_dim=32,\n            out_dim=16,\n            rngs=rngs\n        )\n        self.layer2 = HypLinearPoincare(\n            manifold_module=poincare,\n            in_dim=16,\n            out_dim=8,\n            rngs=rngs\n        )\n\n    def __call__(self, x, c=1.0):\n        x = self.layer1(x, c)\n        x = self.layer2(x, c)\n        return x\n\n# Create model\nmodel = SimpleHypNet(rngs=nnx.Rngs(0))\n\n# Forward pass\nx = jax.random.normal(nnx.Rngs(1).params(), (10, 32)) * 0.3\nx_proj = jax.vmap(poincare.proj, in_axes=(0, None))(x, 1.0)\n\noutput = model(x_proj, c=1.0)\nprint(output.shape)  # (10, 8)\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Batching &amp; JIT Guide: Learn efficient JAX patterns</li> <li>Tutorials: Hands-on Jupyter notebooks</li> <li>API Reference: Complete function documentation</li> <li>Training Workflows: Full training examples</li> </ul>"},{"location":"getting-started/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/#import-errors","title":"Import Errors","text":"<p>If you get import errors, ensure you've installed all dependencies:</p> <pre><code>uv sync --dev\n</code></pre>"},{"location":"getting-started/#float32-precision","title":"Float32 Precision","text":"<p>If you see <code>NaN</code> or <code>inf</code> values, try using float64:</p> <pre><code>from jax import config\nconfig.update(\"jax_enable_x64\", True)\n</code></pre> <p>See Numerical Stability for details.</p>"},{"location":"getting-started/#jit-compilation-issues","title":"JIT Compilation Issues","text":"<p>If JIT compilation fails, check that curvature <code>c</code> is not being traced:</p> <pre><code># Wrong: c is traced\n@jax.jit\ndef forward(x, c):\n    return poincare.dist(x, y, c, version_idx=0)\n\n# Better: use partial\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=(2,))\ndef forward(x, y, c):\n    return poincare.dist(x, y, c, version_idx=0)\n</code></pre>"},{"location":"mathematical-background/","title":"Mathematical Background","text":"<p>Mathematical foundations of hyperbolic geometry and Riemannian optimization.</p> <p>Work in Progress</p> <p>This page is under construction.</p>"},{"location":"mathematical-background/#topics-to-cover","title":"Topics to Cover","text":""},{"location":"mathematical-background/#manifold-theory","title":"Manifold Theory","text":"<ul> <li>Tangent spaces</li> <li>Exponential and logarithmic maps</li> <li>Parallel transport</li> <li>Riemannian metrics</li> </ul>"},{"location":"mathematical-background/#hyperbolic-geometry","title":"Hyperbolic Geometry","text":"<ul> <li>Models of hyperbolic space (Poincar\u00e9, Hyperboloid)</li> <li>Geodesics</li> <li>Curvature</li> </ul>"},{"location":"mathematical-background/#riemannian-optimization","title":"Riemannian Optimization","text":"<ul> <li>Riemannian gradients</li> <li>Exponential map updates</li> <li>Momentum on manifolds</li> </ul>"},{"location":"mathematical-background/#references","title":"References","text":"<p>Key papers implemented in Hyperbolix:</p> <ul> <li>Ganea et al. (2018): \"Hyperbolic Neural Networks\"</li> <li>B\u00e9cigneul &amp; Ganea (2019): \"Riemannian Adaptive Optimization Methods\"</li> <li>Bdeir et al. (2023): \"Fully Hyperbolic Convolutional Neural Networks\"</li> </ul> <p>[Detailed content coming soon]</p>"},{"location":"api-reference/distributions/","title":"Distributions API","text":"<p>Probability distributions on hyperbolic manifolds.</p>"},{"location":"api-reference/distributions/#overview","title":"Overview","text":"<p>Hyperbolix provides wrapped normal distributions for probabilistic modeling on hyperbolic manifolds via functional interfaces. These distributions are essential for:</p> <ul> <li>Variational Autoencoders (VAEs) with hyperbolic latent spaces</li> <li>Bayesian neural networks on manifolds</li> <li>Uncertainty quantification in hyperbolic embeddings</li> </ul>"},{"location":"api-reference/distributions/#wrapped-normal-distribution","title":"Wrapped Normal Distribution","text":"<p>The wrapped normal distribution extends the Gaussian distribution to hyperbolic manifolds by wrapping Euclidean Gaussians via the exponential map.</p>"},{"location":"api-reference/distributions/#poincare-wrapped-normal","title":"Poincar\u00e9 Wrapped Normal","text":""},{"location":"api-reference/distributions/#hyperbolix.distributions.wrapped_normal_poincare","title":"hyperbolix.distributions.wrapped_normal_poincare","text":"<p>Wrapped normal distribution on Poincar\u00e9 ball.</p> <p>Simpler implementation than hyperboloid - no parallel transport needed! Uses exponential map and M\u00f6bius addition.</p> <p>References:     Mathieu et al. \"Continuous Hierarchical Representations with Poincar\u00e9 Variational Auto-Encoders\"     NeurIPS 2019. https://arxiv.org/abs/1901.06033</p>"},{"location":"api-reference/distributions/#hyperbolix.distributions.wrapped_normal_poincare.sample","title":"sample","text":"<pre><code>sample(\n    key: PRNGKeyArray,\n    mu: Float[Array, \"... n\"],\n    sigma: Float[Array, ...] | float,\n    c: float,\n    sample_shape: tuple[int, ...] = (),\n    dtype=None,\n) -&gt; Float[Array, ...]\n</code></pre> <p>Sample from wrapped normal distribution on Poincar\u00e9 ball.</p> <p>Simpler than hyperboloid version - no parallel transport needed!</p> <p>Algorithm: 1. Sample v ~ N(0, \u03a3) \u2208 R^n (directly in tangent space, no embedding) 2. Map to ball at origin: z_0 = exp_0(v) 3. Move to mean: z = \u03bc \u2295 z_0 (M\u00f6bius addition)</p> <p>Args:     key: JAX random key     mu: Mean point on Poincar\u00e9 ball, shape (..., n)     sigma: Covariance parameterization. Can be:         - Scalar: isotropic covariance sigma^2 I (n x n)         - 1D array of length n: diagonal covariance diag(sigma_1^2, ..., sigma_n^2)         - 2D array (n, n): full covariance matrix (must be SPD)     c: Curvature (positive scalar)     sample_shape: Shape of samples to draw, prepended to output. Default: ()     dtype: Output dtype. Default: infer from mu</p> <p>Returns:     Samples from wrapped normal distribution, shape sample_shape + mu.shape</p> <p>Examples:     &gt;&gt;&gt; import jax     &gt;&gt;&gt; import jax.numpy as jnp     &gt;&gt;&gt; from hyperbolix.distributions import wrapped_normal_poincare     &gt;&gt;&gt;     &gt;&gt;&gt; # Single sample with isotropic covariance     &gt;&gt;&gt; key = jax.random.PRNGKey(0)     &gt;&gt;&gt; mu = jnp.array([0.0, 0.0])  # Origin in Poincar\u00e9 ball     &gt;&gt;&gt; sigma = 0.1  # Isotropic     &gt;&gt;&gt; z = wrapped_normal_poincare.sample(key, mu, sigma, c=1.0)     &gt;&gt;&gt; z.shape     (2,)     &gt;&gt;&gt;     &gt;&gt;&gt; # Multiple samples with diagonal covariance     &gt;&gt;&gt; sigma_diag = jnp.array([0.1, 0.2])  # Diagonal     &gt;&gt;&gt; z = wrapped_normal_poincare.sample(key, mu, sigma_diag, c=1.0, sample_shape=(5,))     &gt;&gt;&gt; z.shape     (5, 2)     &gt;&gt;&gt;     &gt;&gt;&gt; # Batch of means     &gt;&gt;&gt; mu_batch = jnp.array([[0.0, 0.0], [0.1, 0.1]])  # (2, 2)     &gt;&gt;&gt; z = wrapped_normal_poincare.sample(key, mu_batch, 0.1, c=1.0)     &gt;&gt;&gt; z.shape     (2, 2)</p> Source code in <code>hyperbolix/distributions/wrapped_normal_poincare.py</code> <pre><code>def sample(\n    key: PRNGKeyArray,\n    mu: Float[Array, \"... n\"],\n    sigma: Float[Array, \"...\"] | float,\n    c: float,\n    sample_shape: tuple[int, ...] = (),\n    dtype=None,\n) -&gt; Float[Array, \"...\"]:\n    \"\"\"Sample from wrapped normal distribution on Poincar\u00e9 ball.\n\n    Simpler than hyperboloid version - no parallel transport needed!\n\n    Algorithm:\n    1. Sample v ~ N(0, \u03a3) \u2208 R^n (directly in tangent space, no embedding)\n    2. Map to ball at origin: z_0 = exp_0(v)\n    3. Move to mean: z = \u03bc \u2295 z_0 (M\u00f6bius addition)\n\n    Args:\n        key: JAX random key\n        mu: Mean point on Poincar\u00e9 ball, shape (..., n)\n        sigma: Covariance parameterization. Can be:\n            - Scalar: isotropic covariance sigma^2 I (n x n)\n            - 1D array of length n: diagonal covariance diag(sigma_1^2, ..., sigma_n^2)\n            - 2D array (n, n): full covariance matrix (must be SPD)\n        c: Curvature (positive scalar)\n        sample_shape: Shape of samples to draw, prepended to output. Default: ()\n        dtype: Output dtype. Default: infer from mu\n\n    Returns:\n        Samples from wrapped normal distribution, shape sample_shape + mu.shape\n\n    Examples:\n        &gt;&gt;&gt; import jax\n        &gt;&gt;&gt; import jax.numpy as jnp\n        &gt;&gt;&gt; from hyperbolix.distributions import wrapped_normal_poincare\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Single sample with isotropic covariance\n        &gt;&gt;&gt; key = jax.random.PRNGKey(0)\n        &gt;&gt;&gt; mu = jnp.array([0.0, 0.0])  # Origin in Poincar\u00e9 ball\n        &gt;&gt;&gt; sigma = 0.1  # Isotropic\n        &gt;&gt;&gt; z = wrapped_normal_poincare.sample(key, mu, sigma, c=1.0)\n        &gt;&gt;&gt; z.shape\n        (2,)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Multiple samples with diagonal covariance\n        &gt;&gt;&gt; sigma_diag = jnp.array([0.1, 0.2])  # Diagonal\n        &gt;&gt;&gt; z = wrapped_normal_poincare.sample(key, mu, sigma_diag, c=1.0, sample_shape=(5,))\n        &gt;&gt;&gt; z.shape\n        (5, 2)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Batch of means\n        &gt;&gt;&gt; mu_batch = jnp.array([[0.0, 0.0], [0.1, 0.1]])  # (2, 2)\n        &gt;&gt;&gt; z = wrapped_normal_poincare.sample(key, mu_batch, 0.1, c=1.0)\n        &gt;&gt;&gt; z.shape\n        (2, 2)\n    \"\"\"\n    # Determine output dtype\n    if dtype is None:\n        dtype = mu.dtype\n\n    # Extract dimension\n    n = mu.shape[-1]  # Dimension of Poincar\u00e9 ball\n\n    # Determine batch shape from mu (all dims except the last one)\n    # mu.shape = batch_shape + (n,)\n    mu_batch_shape = mu.shape[:-1]\n\n    # Step 1: Sample v ~ N(0, \u03a3) \u2208 R^n (directly in tangent space at origin)\n    # Need to sample enough noise vectors for sample_shape AND batch dimensions of mu\n    # Full noise shape: sample_shape + mu_batch_shape + (n,)\n    cov = sigma_to_cov(sigma, n, dtype)\n    full_sample_shape = sample_shape + mu_batch_shape\n    v = sample_gaussian(key, cov, sample_shape=full_sample_shape, dtype=dtype)\n\n    # Step 2: Map to ball at origin: z_0 = exp_0(v)\n    # Step 3: Move to mean: z = \u03bc \u2295 z_0\n\n    def transform_single(v_single, mu_single):\n        \"\"\"Transform a single (v, mu) pair.\"\"\"\n        # Map to ball at origin\n        z_0 = poincare.expmap_0(v_single, c)\n        # Move to mean using M\u00f6bius addition\n        z = poincare.addition(mu_single, z_0, c)\n        return z\n\n    if len(sample_shape) == 0 and len(mu_batch_shape) == 0:\n        # Single point, no batching\n        z = transform_single(v, mu)\n    elif len(sample_shape) == 0:\n        # No sample_shape but batched mu\n        # v has shape mu_batch_shape + (n,), mu has shape mu_batch_shape + (n,)\n        # vmap over all batch dimensions\n        vmapped_fn = transform_single\n        for _ in mu_batch_shape:\n            vmapped_fn = jax.vmap(vmapped_fn)\n        z = vmapped_fn(v, mu)\n    else:\n        # Have sample_shape (and possibly batched mu)\n        # v has shape sample_shape + mu_batch_shape + (n,)\n        # mu has shape mu_batch_shape + (n,)\n        # Need to vmap over sample_shape dims (broadcasting mu) then over batch dims\n\n        # First, vmap over mu_batch_shape dimensions (both v and mu have these)\n        vmapped_fn = transform_single\n        for _ in mu_batch_shape:\n            vmapped_fn = jax.vmap(vmapped_fn)\n\n        # Then vmap over sample_shape dimensions (only v has these, mu is broadcast)\n        for _ in sample_shape:\n            vmapped_fn = jax.vmap(vmapped_fn, in_axes=(0, None))\n\n        z = vmapped_fn(v, mu)\n\n    return z\n</code></pre>"},{"location":"api-reference/distributions/#hyperbolix.distributions.wrapped_normal_poincare.log_prob","title":"log_prob","text":"<pre><code>log_prob(\n    z: Float[Array, \"... n\"],\n    mu: Float[Array, \"... n\"],\n    sigma: Float[Array, ...] | float,\n    c: float,\n) -&gt; Float[Array, ...]\n</code></pre> <p>Compute log probability of wrapped normal distribution on Poincar\u00e9 ball.</p> <p>Implements Algorithm 2 from the paper adapted for Poincar\u00e9 ball: 1. Map z to u = log_\u03bc(z) \u2208 T_\u03bcB^n (logarithmic map) 2. Move u to v = PT_{\u03bc\u21920}(u) \u2208 T_0B^n (parallel transport to origin) 3. Calculate log p(z) = log p(v) - log det(\u2202proj_\u03bc(v)/\u2202v)</p> <p>Args:     z: Sample point(s) on Poincar\u00e9 ball, shape (..., n)     mu: Mean point on Poincar\u00e9 ball, shape (..., n)     sigma: Covariance parameterization. Can be:         - Scalar: isotropic covariance sigma^2 I (n x n)         - 1D array of length n: diagonal covariance diag(sigma_1^2, ..., sigma_n^2)         - 2D array (n, n): full covariance matrix (must be SPD)     c: Curvature (positive scalar)</p> <p>Returns:     Log probability, shape (...) (spatial dimension removed)</p> <p>Examples:     &gt;&gt;&gt; import jax     &gt;&gt;&gt; import jax.numpy as jnp     &gt;&gt;&gt; from hyperbolix.distributions import wrapped_normal_poincare     &gt;&gt;&gt;     &gt;&gt;&gt; # Compute log probability of samples     &gt;&gt;&gt; key = jax.random.PRNGKey(0)     &gt;&gt;&gt; mu = jnp.array([0.0, 0.0])     &gt;&gt;&gt; sigma = 0.1     &gt;&gt;&gt; z = wrapped_normal_poincare.sample(key, mu, sigma, c=1.0)     &gt;&gt;&gt; log_p = wrapped_normal_poincare.log_prob(z, mu, sigma, c=1.0)     &gt;&gt;&gt; log_p.shape     ()     &gt;&gt;&gt;     &gt;&gt;&gt; # Batch computation     &gt;&gt;&gt; z_batch = wrapped_normal_poincare.sample(key, mu, sigma, c=1.0, sample_shape=(10,))     &gt;&gt;&gt; log_p_batch = wrapped_normal_poincare.log_prob(z_batch, mu, sigma, c=1.0)     &gt;&gt;&gt; log_p_batch.shape     (10,)</p> Source code in <code>hyperbolix/distributions/wrapped_normal_poincare.py</code> <pre><code>def log_prob(\n    z: Float[Array, \"... n\"],\n    mu: Float[Array, \"... n\"],\n    sigma: Float[Array, \"...\"] | float,\n    c: float,\n) -&gt; Float[Array, \"...\"]:\n    \"\"\"Compute log probability of wrapped normal distribution on Poincar\u00e9 ball.\n\n    Implements Algorithm 2 from the paper adapted for Poincar\u00e9 ball:\n    1. Map z to u = log_\u03bc(z) \u2208 T_\u03bcB^n (logarithmic map)\n    2. Move u to v = PT_{\u03bc\u21920}(u) \u2208 T_0B^n (parallel transport to origin)\n    3. Calculate log p(z) = log p(v) - log det(\u2202proj_\u03bc(v)/\u2202v)\n\n    Args:\n        z: Sample point(s) on Poincar\u00e9 ball, shape (..., n)\n        mu: Mean point on Poincar\u00e9 ball, shape (..., n)\n        sigma: Covariance parameterization. Can be:\n            - Scalar: isotropic covariance sigma^2 I (n x n)\n            - 1D array of length n: diagonal covariance diag(sigma_1^2, ..., sigma_n^2)\n            - 2D array (n, n): full covariance matrix (must be SPD)\n        c: Curvature (positive scalar)\n\n    Returns:\n        Log probability, shape (...) (spatial dimension removed)\n\n    Examples:\n        &gt;&gt;&gt; import jax\n        &gt;&gt;&gt; import jax.numpy as jnp\n        &gt;&gt;&gt; from hyperbolix.distributions import wrapped_normal_poincare\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compute log probability of samples\n        &gt;&gt;&gt; key = jax.random.PRNGKey(0)\n        &gt;&gt;&gt; mu = jnp.array([0.0, 0.0])\n        &gt;&gt;&gt; sigma = 0.1\n        &gt;&gt;&gt; z = wrapped_normal_poincare.sample(key, mu, sigma, c=1.0)\n        &gt;&gt;&gt; log_p = wrapped_normal_poincare.log_prob(z, mu, sigma, c=1.0)\n        &gt;&gt;&gt; log_p.shape\n        ()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Batch computation\n        &gt;&gt;&gt; z_batch = wrapped_normal_poincare.sample(key, mu, sigma, c=1.0, sample_shape=(10,))\n        &gt;&gt;&gt; log_p_batch = wrapped_normal_poincare.log_prob(z_batch, mu, sigma, c=1.0)\n        &gt;&gt;&gt; log_p_batch.shape\n        (10,)\n    \"\"\"\n    import jax.numpy as jnp\n\n    # Determine dtype\n    dtype = z.dtype\n\n    # Extract dimension\n    n = mu.shape[-1]  # Dimension of Poincar\u00e9 ball\n\n    # Step 1: Map z to tangent space at mu using logarithmic map\n    # u = log_\u03bc(z)\n    if z.ndim &gt; mu.ndim:\n        # z has sample dimensions, need to vmap over them\n        n_sample_dims = z.ndim - mu.ndim\n\n        # Create vmapped version of logmap\n        logmap_fn = poincare.logmap\n        for _ in range(n_sample_dims):\n            logmap_fn = jax.vmap(logmap_fn, in_axes=(0, None, None))\n\n        u = logmap_fn(z, mu, c)\n    elif z.ndim == mu.ndim and mu.ndim &gt; 1:\n        # Both are batched, vmap over batch dimension\n        u = jax.vmap(lambda zz, mm: poincare.logmap(zz, mm, c))(z, mu)\n    else:\n        # Single point\n        u = poincare.logmap(z, mu, c)\n\n    # Step 2: Parallel transport from mu to origin\n    # v = PT_{\u03bc\u21920}(u)\n    mu_0 = jnp.zeros(n, dtype=dtype)  # Origin is zero vector in Poincar\u00e9 ball\n\n    if u.ndim &gt; 1:\n        # Batched, need to vmap\n        if mu.ndim &gt; 1:\n            # mu is also batched\n            v = jax.vmap(lambda uu, mm: poincare.ptransp(uu, mm, mu_0, c))(u, mu)\n        else:\n            # Only u is batched (from sample_shape)\n            v = jax.vmap(lambda uu: poincare.ptransp(uu, mu, mu_0, c))(u)\n    else:\n        # Single point\n        v = poincare.ptransp(u, mu, mu_0, c)\n\n    # Step 3: Compute log p(v) where v ~ N(0, \u03a3)\n    # Note: v is already in R^n, no need to extract spatial components like in hyperboloid\n    log_p_v = _gaussian_log_prob(v, sigma, n, dtype)\n\n    # Step 4: Compute log det Jacobian\n    log_det_jac = _log_det_jacobian(v, c, n)\n\n    # Step 5: Compute final log probability\n    # log p(z) = log p(v) - log det(\u2202proj_\u03bc(v)/\u2202v)\n    log_p_z = log_p_v - log_det_jac\n\n    return log_p_z\n</code></pre>"},{"location":"api-reference/distributions/#hyperboloid-wrapped-normal","title":"Hyperboloid Wrapped Normal","text":""},{"location":"api-reference/distributions/#hyperbolix.distributions.wrapped_normal_hyperboloid","title":"hyperbolix.distributions.wrapped_normal_hyperboloid","text":"<p>Wrapped normal distribution on hyperboloid manifold.</p> <p>Implementation of the wrapped normal distribution that wraps a Gaussian from the tangent space at the origin onto the hyperboloid via parallel transport and exponential map.</p> <p>References:     Mathieu et al. \"Continuous Hierarchical Representations with Poincar\u00e9 Variational Auto-Encoders\"     NeurIPS 2019. https://arxiv.org/abs/1901.06033</p>"},{"location":"api-reference/distributions/#hyperbolix.distributions.wrapped_normal_hyperboloid.sample","title":"sample","text":"<pre><code>sample(\n    key: PRNGKeyArray,\n    mu: Float[Array, \"... n_plus_1\"],\n    sigma: Float[Array, ...] | float,\n    c: float,\n    sample_shape: tuple[int, ...] = (),\n    dtype=None,\n) -&gt; Float[Array, ...]\n</code></pre> <p>Sample from wrapped normal distribution on hyperboloid.</p> <p>Implements Algorithm 1 from the paper: 1. Sample v_bar ~ N(0, \u03a3) \u2208 R^n 2. Embed as tangent vector v = [0, v_bar] \u2208 T_{\u03bc\u2080}\u210d\u207f at origin 3. Parallel transport to mean: u = PT_{\u03bc\u2080\u2192\u03bc}(v) \u2208 T_\u03bc\u210d\u207f 4. Map to manifold: z = exp_\u03bc(u) \u2208 \u210d\u207f</p> <p>Args:     key: JAX random key     mu: Mean point on hyperboloid, shape (..., n+1) in ambient coordinates     sigma: Covariance parameterization in spatial coordinates. Can be:         - Scalar: isotropic covariance sigma^2 I (n x n)         - 1D array of length n: diagonal covariance diag(sigma_1^2, ..., sigma_n^2)         - 2D array (n, n): full covariance matrix (must be SPD)     c: Curvature (positive scalar)     sample_shape: Shape of samples to draw, prepended to output. Default: ()     dtype: Output dtype. Default: infer from mu</p> <p>Returns:     Samples from wrapped normal distribution, shape sample_shape + mu.shape</p> <p>Examples:     &gt;&gt;&gt; import jax     &gt;&gt;&gt; import jax.numpy as jnp     &gt;&gt;&gt; from hyperbolix.distributions import wrapped_normal_hyperboloid     &gt;&gt;&gt;     &gt;&gt;&gt; # Single sample with isotropic covariance     &gt;&gt;&gt; key = jax.random.PRNGKey(0)     &gt;&gt;&gt; mu = jnp.array([1.0, 0.0, 0.0])  # Origin in H^2     &gt;&gt;&gt; sigma = 0.1  # Isotropic     &gt;&gt;&gt; z = wrapped_normal_hyperboloid.sample(key, mu, sigma, c=1.0)     &gt;&gt;&gt; z.shape     (3,)     &gt;&gt;&gt;     &gt;&gt;&gt; # Multiple samples with diagonal covariance     &gt;&gt;&gt; sigma_diag = jnp.array([0.1, 0.2])  # Diagonal     &gt;&gt;&gt; z = wrapped_normal_hyperboloid.sample(key, mu, sigma_diag, c=1.0, sample_shape=(5,))     &gt;&gt;&gt; z.shape     (5, 3)     &gt;&gt;&gt;     &gt;&gt;&gt; # Batch of means     &gt;&gt;&gt; mu_batch = jnp.array([[1.0, 0.0, 0.0], [1.0, 0.1, 0.1]])  # (2, 3)     &gt;&gt;&gt; z = wrapped_normal_hyperboloid.sample(key, mu_batch, 0.1, c=1.0)     &gt;&gt;&gt; z.shape     (2, 3)</p> Source code in <code>hyperbolix/distributions/wrapped_normal_hyperboloid.py</code> <pre><code>def sample(\n    key: PRNGKeyArray,\n    mu: Float[Array, \"... n_plus_1\"],\n    sigma: Float[Array, \"...\"] | float,\n    c: float,\n    sample_shape: tuple[int, ...] = (),\n    dtype=None,\n) -&gt; Float[Array, \"...\"]:\n    \"\"\"Sample from wrapped normal distribution on hyperboloid.\n\n    Implements Algorithm 1 from the paper:\n    1. Sample v_bar ~ N(0, \u03a3) \u2208 R^n\n    2. Embed as tangent vector v = [0, v_bar] \u2208 T_{\u03bc\u2080}\u210d\u207f at origin\n    3. Parallel transport to mean: u = PT_{\u03bc\u2080\u2192\u03bc}(v) \u2208 T_\u03bc\u210d\u207f\n    4. Map to manifold: z = exp_\u03bc(u) \u2208 \u210d\u207f\n\n    Args:\n        key: JAX random key\n        mu: Mean point on hyperboloid, shape (..., n+1) in ambient coordinates\n        sigma: Covariance parameterization in spatial coordinates. Can be:\n            - Scalar: isotropic covariance sigma^2 I (n x n)\n            - 1D array of length n: diagonal covariance diag(sigma_1^2, ..., sigma_n^2)\n            - 2D array (n, n): full covariance matrix (must be SPD)\n        c: Curvature (positive scalar)\n        sample_shape: Shape of samples to draw, prepended to output. Default: ()\n        dtype: Output dtype. Default: infer from mu\n\n    Returns:\n        Samples from wrapped normal distribution, shape sample_shape + mu.shape\n\n    Examples:\n        &gt;&gt;&gt; import jax\n        &gt;&gt;&gt; import jax.numpy as jnp\n        &gt;&gt;&gt; from hyperbolix.distributions import wrapped_normal_hyperboloid\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Single sample with isotropic covariance\n        &gt;&gt;&gt; key = jax.random.PRNGKey(0)\n        &gt;&gt;&gt; mu = jnp.array([1.0, 0.0, 0.0])  # Origin in H^2\n        &gt;&gt;&gt; sigma = 0.1  # Isotropic\n        &gt;&gt;&gt; z = wrapped_normal_hyperboloid.sample(key, mu, sigma, c=1.0)\n        &gt;&gt;&gt; z.shape\n        (3,)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Multiple samples with diagonal covariance\n        &gt;&gt;&gt; sigma_diag = jnp.array([0.1, 0.2])  # Diagonal\n        &gt;&gt;&gt; z = wrapped_normal_hyperboloid.sample(key, mu, sigma_diag, c=1.0, sample_shape=(5,))\n        &gt;&gt;&gt; z.shape\n        (5, 3)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Batch of means\n        &gt;&gt;&gt; mu_batch = jnp.array([[1.0, 0.0, 0.0], [1.0, 0.1, 0.1]])  # (2, 3)\n        &gt;&gt;&gt; z = wrapped_normal_hyperboloid.sample(key, mu_batch, 0.1, c=1.0)\n        &gt;&gt;&gt; z.shape\n        (2, 3)\n    \"\"\"\n    # Determine output dtype\n    if dtype is None:\n        dtype = mu.dtype\n\n    # Extract spatial dimension\n    n = mu.shape[-1] - 1  # Spatial dimension (n for H^n in R^(n+1))\n\n    # Determine batch shape from mu (all dims except the last one)\n    # mu.shape = batch_shape + (n+1,)\n    mu_batch_shape = mu.shape[:-1]\n\n    # Step 1: Sample v_bar ~ N(0, \u03a3) \u2208 R^n\n    # Need to sample enough noise vectors for sample_shape AND batch dimensions of mu\n    # Full noise shape: sample_shape + mu_batch_shape + (n,)\n    cov = sigma_to_cov(sigma, n, dtype)\n    full_sample_shape = sample_shape + mu_batch_shape\n    v_spatial = sample_gaussian(key, cov, sample_shape=full_sample_shape, dtype=dtype)\n\n    # Step 2: Embed as tangent vector v = [0, v_bar] \u2208 T_{\u03bc\u2080}\u210d\u207f at origin\n    # v has shape: sample_shape + mu_batch_shape + (n+1,)\n    v = hyperboloid.embed_spatial_0(v_spatial)\n\n    # Step 3 &amp; 4: Parallel transport and exponential map\n    # We need to apply ptransp_0 and expmap element-wise, matching v and mu\n\n    def transform_single(v_single, mu_single):\n        \"\"\"Transform a single (v, mu) pair.\"\"\"\n        # Step 3: Parallel transport from origin to mu\n        u = hyperboloid.ptransp_0(v_single, mu_single, c)\n        # Step 4: Exponential map at mu\n        z = hyperboloid.expmap(u, mu_single, c)\n        return z\n\n    if len(sample_shape) == 0 and len(mu_batch_shape) == 0:\n        # Single point, no batching\n        z = transform_single(v, mu)\n    elif len(sample_shape) == 0:\n        # No sample_shape but batched mu\n        # v has shape mu_batch_shape + (n+1,), mu has shape mu_batch_shape + (n+1,)\n        # vmap over all batch dimensions\n        vmapped_fn = transform_single\n        for _ in mu_batch_shape:\n            vmapped_fn = jax.vmap(vmapped_fn)\n        z = vmapped_fn(v, mu)\n    else:\n        # Have sample_shape (and possibly batched mu)\n        # v has shape sample_shape + mu_batch_shape + (n+1,)\n        # mu has shape mu_batch_shape + (n+1,)\n        # Need to vmap over sample_shape dims (broadcasting mu) then over batch dims\n\n        # First, vmap over mu_batch_shape dimensions (both v and mu have these)\n        vmapped_fn = transform_single\n        for _ in mu_batch_shape:\n            vmapped_fn = jax.vmap(vmapped_fn)\n\n        # Then vmap over sample_shape dimensions (only v has these, mu is broadcast)\n        for _ in sample_shape:\n            vmapped_fn = jax.vmap(vmapped_fn, in_axes=(0, None))\n\n        z = vmapped_fn(v, mu)\n\n    return z\n</code></pre>"},{"location":"api-reference/distributions/#hyperbolix.distributions.wrapped_normal_hyperboloid.log_prob","title":"log_prob","text":"<pre><code>log_prob(\n    z: Float[Array, \"... n_plus_1\"],\n    mu: Float[Array, \"... n_plus_1\"],\n    sigma: Float[Array, ...] | float,\n    c: float,\n) -&gt; Float[Array, ...]\n</code></pre> <p>Compute log probability of wrapped normal distribution.</p> <p>Implements Algorithm 2 from the paper: 1. Map z to u = exp_\u03bc\u207b\u00b9(z) \u2208 T_\u03bc\u210d\u207f (logarithmic map) 2. Move u to v = PT_{\u03bc\u2192\u03bc\u2080}(u) \u2208 T_{\u03bc\u2080}\u210d\u207f (parallel transport to origin) 3. Calculate log p(z) = log p(v) - log det(\u2202proj_\u03bc(v)/\u2202v)</p> <p>Args:     z: Sample point(s) on hyperboloid, shape (..., n+1)     mu: Mean point on hyperboloid, shape (..., n+1)     sigma: Covariance parameterization. Can be:         - Scalar: isotropic covariance sigma^2 I (n x n)         - 1D array of length n: diagonal covariance diag(sigma_1^2, ..., sigma_n^2)         - 2D array (n, n): full covariance matrix (must be SPD)     c: Curvature (positive scalar)</p> <p>Returns:     Log probability, shape (...) (manifold dimension removed)</p> <p>Examples:     &gt;&gt;&gt; import jax     &gt;&gt;&gt; import jax.numpy as jnp     &gt;&gt;&gt; from hyperbolix.distributions import wrapped_normal_hyperboloid     &gt;&gt;&gt;     &gt;&gt;&gt; # Compute log probability of samples     &gt;&gt;&gt; key = jax.random.PRNGKey(0)     &gt;&gt;&gt; mu = jnp.array([1.0, 0.0, 0.0])     &gt;&gt;&gt; sigma = 0.1     &gt;&gt;&gt; z = wrapped_normal_hyperboloid.sample(key, mu, sigma, c=1.0)     &gt;&gt;&gt; log_p = wrapped_normal_hyperboloid.log_prob(z, mu, sigma, c=1.0)     &gt;&gt;&gt; log_p.shape     ()     &gt;&gt;&gt;     &gt;&gt;&gt; # Batch computation     &gt;&gt;&gt; z_batch = wrapped_normal_hyperboloid.sample(key, mu, sigma, c=1.0, sample_shape=(10,))     &gt;&gt;&gt; log_p_batch = wrapped_normal_hyperboloid.log_prob(z_batch, mu, sigma, c=1.0)     &gt;&gt;&gt; log_p_batch.shape     (10,)</p> Source code in <code>hyperbolix/distributions/wrapped_normal_hyperboloid.py</code> <pre><code>def log_prob(\n    z: Float[Array, \"... n_plus_1\"],\n    mu: Float[Array, \"... n_plus_1\"],\n    sigma: Float[Array, \"...\"] | float,\n    c: float,\n) -&gt; Float[Array, \"...\"]:\n    \"\"\"Compute log probability of wrapped normal distribution.\n\n    Implements Algorithm 2 from the paper:\n    1. Map z to u = exp_\u03bc\u207b\u00b9(z) \u2208 T_\u03bc\u210d\u207f (logarithmic map)\n    2. Move u to v = PT_{\u03bc\u2192\u03bc\u2080}(u) \u2208 T_{\u03bc\u2080}\u210d\u207f (parallel transport to origin)\n    3. Calculate log p(z) = log p(v) - log det(\u2202proj_\u03bc(v)/\u2202v)\n\n    Args:\n        z: Sample point(s) on hyperboloid, shape (..., n+1)\n        mu: Mean point on hyperboloid, shape (..., n+1)\n        sigma: Covariance parameterization. Can be:\n            - Scalar: isotropic covariance sigma^2 I (n x n)\n            - 1D array of length n: diagonal covariance diag(sigma_1^2, ..., sigma_n^2)\n            - 2D array (n, n): full covariance matrix (must be SPD)\n        c: Curvature (positive scalar)\n\n    Returns:\n        Log probability, shape (...) (manifold dimension removed)\n\n    Examples:\n        &gt;&gt;&gt; import jax\n        &gt;&gt;&gt; import jax.numpy as jnp\n        &gt;&gt;&gt; from hyperbolix.distributions import wrapped_normal_hyperboloid\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compute log probability of samples\n        &gt;&gt;&gt; key = jax.random.PRNGKey(0)\n        &gt;&gt;&gt; mu = jnp.array([1.0, 0.0, 0.0])\n        &gt;&gt;&gt; sigma = 0.1\n        &gt;&gt;&gt; z = wrapped_normal_hyperboloid.sample(key, mu, sigma, c=1.0)\n        &gt;&gt;&gt; log_p = wrapped_normal_hyperboloid.log_prob(z, mu, sigma, c=1.0)\n        &gt;&gt;&gt; log_p.shape\n        ()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Batch computation\n        &gt;&gt;&gt; z_batch = wrapped_normal_hyperboloid.sample(key, mu, sigma, c=1.0, sample_shape=(10,))\n        &gt;&gt;&gt; log_p_batch = wrapped_normal_hyperboloid.log_prob(z_batch, mu, sigma, c=1.0)\n        &gt;&gt;&gt; log_p_batch.shape\n        (10,)\n    \"\"\"\n    # Determine dtype\n    dtype = z.dtype\n\n    # Extract spatial dimension\n    n = mu.shape[-1] - 1  # Spatial dimension\n\n    # Handle batching with vmap if needed\n    # For now, assume z and mu have compatible shapes\n    # If z has extra leading dimensions (samples), we need to vmap\n\n    # Step 1: Map z to tangent space at mu using logarithmic map\n    # u = log_\u03bc(z) = exp_\u03bc\u207b\u00b9(z)\n    if z.ndim &gt; mu.ndim:\n        # z has sample dimensions, need to vmap over them\n        # Figure out how many sample dimensions\n        n_sample_dims = z.ndim - mu.ndim\n\n        # Create vmapped version of logmap\n        logmap_fn = hyperboloid.logmap\n        for _ in range(n_sample_dims):\n            logmap_fn = jax.vmap(logmap_fn, in_axes=(0, None, None))\n\n        u = logmap_fn(z, mu, c)\n    elif z.ndim == mu.ndim and mu.ndim &gt; 1:\n        # Both are batched, vmap over batch dimension\n        u = jax.vmap(lambda zz, mm: hyperboloid.logmap(zz, mm, c))(z, mu)\n    else:\n        # Single point\n        u = hyperboloid.logmap(z, mu, c)\n\n    # Step 2: Parallel transport from mu to origin\n    # v = PT_{\u03bc\u2192\u03bc\u2080}(u)\n    mu_0 = hyperboloid._create_origin(c, n, dtype)\n\n    if u.ndim &gt; 1:\n        # Batched, need to vmap\n        if mu.ndim &gt; 1:\n            # mu is also batched\n            v = jax.vmap(lambda uu, mm: hyperboloid.ptransp(uu, mm, mu_0, c))(u, mu)\n        else:\n            # Only u is batched (from sample_shape)\n            v = jax.vmap(lambda uu: hyperboloid.ptransp(uu, mu, mu_0, c))(u)\n    else:\n        # Single point\n        v = hyperboloid.ptransp(u, mu, mu_0, c)\n\n    # Step 3: Extract spatial components from v (remove temporal component)\n    # v = [0, v_bar] at origin, so v_spatial = v[..., 1:]\n    v_spatial = v[..., 1:]\n\n    # Step 4: Compute log p(v) where v ~ N(0, \u03a3)\n    log_p_v = _gaussian_log_prob(v_spatial, sigma, n, dtype)\n\n    # Step 5: Compute log det Jacobian\n    log_det_jac = _log_det_jacobian(v, c, n)\n\n    # Step 6: Compute final log probability\n    # log p(z) = log p(v) - log det(\u2202proj_\u03bc(v)/\u2202v)\n    log_p_z = log_p_v - log_det_jac\n\n    return log_p_z\n</code></pre>"},{"location":"api-reference/distributions/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/distributions/#basic-sampling-poincare","title":"Basic Sampling (Poincar\u00e9)","text":"<pre><code>from hyperbolix.distributions import wrapped_normal_poincare\nfrom hyperbolix.manifolds import poincare\nimport jax\nimport jax.numpy as jnp\n\n# Mean on Poincar\u00e9 ball\nmean = jnp.array([0.2, 0.3])\nmean_proj = poincare.proj(mean, c=1.0)\n\n# Standard deviation\nstd = 0.1\n\n# Sample\nkey = jax.random.PRNGKey(42)\nsamples = wrapped_normal_poincare.sample(mean_proj, std, c=1.0, key=key, sample_shape=(100,))\nprint(samples.shape)  # (100, 2)\n\n# Samples lie on Poincar\u00e9 ball\nnorms = jnp.linalg.norm(samples, axis=-1)\nprint(jnp.all(norms &lt; 1.0 / jnp.sqrt(1.0)))  # True\n</code></pre>"},{"location":"api-reference/distributions/#log-probability","title":"Log Probability","text":"<pre><code># Compute log probability of samples\nlog_probs = jax.vmap(\n    lambda x: wrapped_normal_poincare.log_prob(x, mean_proj, std, c=1.0)\n)(samples)\nprint(log_probs.shape)  # (100,)\n\n# Higher probability near mean\npoint_near_mean = poincare.proj(jnp.array([0.21, 0.29]), c=1.0)\npoint_far = poincare.proj(jnp.array([0.7, 0.7]), c=1.0)\n\nprint(f\"Log prob (near): {wrapped_normal_poincare.log_prob(point_near_mean, mean_proj, std, c=1.0):.4f}\")\nprint(f\"Log prob (far): {wrapped_normal_poincare.log_prob(point_far, mean_proj, std, c=1.0):.4f}\")\n</code></pre>"},{"location":"api-reference/distributions/#hyperboloid-distribution","title":"Hyperboloid Distribution","text":"<pre><code>from hyperbolix.distributions import wrapped_normal_hyperboloid\nfrom hyperbolix.manifolds import hyperboloid\n\n# Mean on hyperboloid (ambient coordinates)\nmean_space = jnp.array([0.2, 0.3, -0.1])\nmean_ambient = jnp.concatenate([\n    jnp.array([jnp.sqrt(jnp.sum(mean_space**2) + 1.0)]),\n    mean_space\n])\n\n# Sample\nkey = jax.random.PRNGKey(123)\nsamples = wrapped_normal_hyperboloid.sample(mean_ambient, std=0.15, c=1.0, key=key, sample_shape=(50,))\n\n# Compute log probabilities\nlog_probs = jax.vmap(\n    lambda x: wrapped_normal_hyperboloid.log_prob(x, mean_ambient, 0.15, c=1.0)\n)(samples)\n</code></pre>"},{"location":"api-reference/distributions/#vae-example","title":"VAE Example","text":"<p>Using wrapped normal distributions in a Variational Autoencoder:</p> <pre><code>from flax import nnx\nfrom hyperbolix.distributions import wrapped_normal_poincare\nfrom hyperbolix.nn_layers import HypLinearPoincare\nfrom hyperbolix.manifolds import poincare\nimport jax\nimport jax.numpy as jnp\n\nclass HyperbolicVAE(nnx.Module):\n    def __init__(self, latent_dim, rngs):\n        self.latent_dim = latent_dim\n\n        # Encoder: Euclidean \u2192 Hyperbolic\n        self.encoder = nnx.Linear(784, 128, rngs=rngs)\n        self.enc_hyp = HypLinearPoincare(\n            manifold_module=poincare,\n            in_dim=128,\n            out_dim=latent_dim,\n            rngs=rngs\n        )\n\n        # Decoder: Hyperbolic \u2192 Euclidean\n        self.dec_hyp = HypLinearPoincare(\n            manifold_module=poincare,\n            in_dim=latent_dim,\n            out_dim=128,\n            rngs=rngs\n        )\n        self.decoder = nnx.Linear(128, 784, rngs=rngs)\n\n    def encode(self, x, c):\n        # Returns mean and log_std for latent distribution\n        h = jax.nn.relu(self.encoder(x))\n\n        # Project to Poincar\u00e9 ball\n        h_proj = jax.vmap(poincare.proj, in_axes=(0, None, None))(h, c, None)\n\n        # Mean on Poincar\u00e9 ball\n        mean = self.enc_hyp(h_proj, c)\n\n        # Std in tangent space (Euclidean)\n        log_std_layer = nnx.Linear(128, self.latent_dim, rngs=nnx.Rngs(0))\n        log_std = log_std_layer(h)\n        std = jnp.exp(log_std)\n\n        return mean, std\n\n    def decode(self, z, c):\n        h = self.dec_hyp(z, c)\n\n        # Logmap to tangent space for Euclidean decoder\n        h_tangent = jax.vmap(poincare.logmap, in_axes=(None, 0, None))(\n            jnp.zeros(self.latent_dim), h, c\n        )\n\n        return jax.nn.sigmoid(self.decoder(h_tangent))\n\n    def __call__(self, x, key, c=1.0):\n        # Encode\n        mean, std = self.encode(x, c)\n\n        # Sample latent code\n        keys = jax.random.split(key, mean.shape[0])\n        z = jax.vmap(\n            lambda m, s, k: wrapped_normal_poincare.sample(m, s, c, k, ())\n        )(mean, std, keys)\n\n        # Decode\n        recon = self.decode(z, c)\n\n        return recon, mean, std, z\n\n# Loss function\ndef vae_loss(model, x, key, c):\n    recon, mean, std, z = model(x, key, c)\n\n    # Reconstruction loss\n    recon_loss = jnp.mean((x - recon) ** 2)\n\n    # KL divergence (approximate for wrapped normal)\n    # Use standard Gaussian prior in tangent space at origin\n    kl_loss = -0.5 * jnp.mean(\n        1 + 2 * jnp.log(std) - jnp.sum(mean**2, axis=-1) - std**2\n    )\n\n    return recon_loss + kl_loss\n</code></pre>"},{"location":"api-reference/distributions/#mathematical-background","title":"Mathematical Background","text":""},{"location":"api-reference/distributions/#wrapped-normal-definition","title":"Wrapped Normal Definition","text":"<p>Given a mean \\(\\mu \\in \\mathcal{M}\\) on manifold \\(\\mathcal{M}\\) and standard deviation \\(\\sigma\\), the wrapped normal distribution is defined as:</p> <ol> <li>Sample \\(v \\sim \\mathcal{N}(0, \\sigma^2 I)\\) in tangent space \\(T_\\mu \\mathcal{M}\\)</li> <li>Wrap to manifold: \\(x = \\exp_\\mu(v)\\)</li> </ol> <p>The log probability is:</p> \\[ \\log p(x) = -\\frac{1}{2\\sigma^2} \\|\\log_\\mu(x)\\|^2 - \\frac{d}{2}\\log(2\\pi\\sigma^2) \\] <p>where \\(\\log_\\mu\\) is the logarithmic map at \\(\\mu\\).</p>"},{"location":"api-reference/distributions/#sampling-algorithm","title":"Sampling Algorithm","text":"<pre><code>def sample(mean, std, c, key, sample_shape):\n    # 1. Sample in tangent space at mean\n    tangent_sample = std * jax.random.normal(key, sample_shape + mean.shape)\n\n    # 2. Exponential map to manifold\n    manifold_sample = manifold.expmap(mean, tangent_sample, c)\n\n    return manifold_sample\n</code></pre>"},{"location":"api-reference/distributions/#numerical-considerations","title":"Numerical Considerations","text":"<p>Numerical Stability</p> <p>For small standard deviations and/or high curvatures, the exponential map can become numerically unstable. Consider:</p> <ul> <li>Using float64 for very small \\(\\sigma\\) (&lt; 0.01)</li> <li>Clipping standard deviations to reasonable range: \\(\\sigma \\in [0.01, 1.0]\\)</li> <li>Using version parameter in manifold operations for better stability</li> </ul> <p>Curvature Choice</p> <p>The curvature parameter \\(c\\) affects the distribution:</p> <ul> <li>Higher \\(c\\) \u2192 More concentrated distributions</li> <li>Lower \\(c\\) \u2192 More spread out distributions</li> </ul> <p>Tune \\(c\\) based on your application's needs.</p>"},{"location":"api-reference/distributions/#references","title":"References","text":"<p>Wrapped distributions on manifolds are discussed in:</p> <ul> <li>Nagano, Y., et al. (2019). \"A Wrapped Normal Distribution on Hyperbolic Space for Gradient-Based Learning\"</li> <li>Davidson, T., et al. (2018). \"Hyperspherical Variational Auto-Encoders\"</li> </ul> <p>See also:</p> <ul> <li>Manifolds API: Exponential and logarithmic maps</li> <li>NN Layers API: Building VAEs with hyperbolic layers</li> </ul>"},{"location":"api-reference/manifolds/","title":"Manifolds API","text":"<p>This page documents the core manifold operations in Hyperbolix. Each manifold module provides a consistent API for geometric operations on hyperbolic and Euclidean spaces.</p>"},{"location":"api-reference/manifolds/#overview","title":"Overview","text":"<p>Hyperbolix provides three manifold implementations:</p> <ul> <li>Euclidean: Flat Euclidean space (baseline)</li> <li>Poincar\u00e9 Ball: Conformal model of hyperbolic space</li> <li>Hyperboloid: Lorentz/Minkowski model of hyperbolic space</li> </ul> <p>All manifolds follow a pure functional design with vmap-native operations.</p>"},{"location":"api-reference/manifolds/#common-operations","title":"Common Operations","text":"<p>Each manifold provides these core operations:</p> <ul> <li><code>proj</code>: Project points onto the manifold</li> <li><code>dist</code>: Compute distances between points</li> <li><code>expmap</code>: Exponential map (tangent \u2192 manifold)</li> <li><code>logmap</code>: Logarithmic map (manifold \u2192 tangent)</li> <li><code>ptransp</code>: Parallel transport of tangent vectors</li> <li><code>egrad2rgrad</code>: Convert Euclidean to Riemannian gradients</li> </ul>"},{"location":"api-reference/manifolds/#euclidean","title":"Euclidean","text":"<p>Flat Euclidean space (identity operations).</p>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.euclidean","title":"hyperbolix.manifolds.euclidean","text":"<p>Euclidean manifold - vmap-native pure functional implementation.</p> <p>JAX port with vmap-native API. All functions operate on single points/vectors with shape (dim,). Use jax.vmap for batch operations.</p> JIT Compilation &amp; Batching <p>All functions work with single points and return scalars or vectors. Use jax.vmap for batching:</p> <pre><code>&gt;&gt;&gt; import jax\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from hyperbolix.manifolds import euclidean\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Single point operations\n&gt;&gt;&gt; x = jnp.array([1.0, 2.0])\n&gt;&gt;&gt; y = jnp.array([3.0, 4.0])\n&gt;&gt;&gt; distance = euclidean.dist(x, y, c=0.0)  # Returns scalar\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Batch operations with vmap\n&gt;&gt;&gt; x_batch = jnp.array([[1.0, 2.0], [0.5, 1.5]])  # (batch, dim)\n&gt;&gt;&gt; y_batch = jnp.array([[3.0, 4.0], [1.0, 2.0]])\n&gt;&gt;&gt; dist_batched = jax.vmap(euclidean.dist, in_axes=(0, 0, None))\n&gt;&gt;&gt; distances = dist_batched(x_batch, y_batch, 0.0)  # Returns (batch,)\n</code></pre> <p>See jax_migration.md for comprehensive usage patterns.</p>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.euclidean.proj","title":"proj","text":"<pre><code>proj(\n    x: Float[Array, dim], c: float = 0.0\n) -&gt; Float[Array, dim]\n</code></pre> <p>Project point onto Euclidean space (identity operation).</p> <p>Args:     x: Point in Euclidean space, shape (dim,)     c: Curvature (ignored, kept for consistency with other manifolds)</p> <p>Returns:     Projected point (identity), shape (dim,)</p> Source code in <code>hyperbolix/manifolds/euclidean.py</code> <pre><code>def proj(x: Float[Array, \"dim\"], c: float = 0.0) -&gt; Float[Array, \"dim\"]:\n    \"\"\"Project point onto Euclidean space (identity operation).\n\n    Args:\n        x: Point in Euclidean space, shape (dim,)\n        c: Curvature (ignored, kept for consistency with other manifolds)\n\n    Returns:\n        Projected point (identity), shape (dim,)\n    \"\"\"\n    return x\n</code></pre>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.euclidean.dist","title":"dist","text":"<pre><code>dist(\n    x: Float[Array, dim],\n    y: Float[Array, dim],\n    c: float = 0.0,\n) -&gt; Float[Array, \"\"]\n</code></pre> <p>Compute geodesic distance between Euclidean points x and y.</p> <p>Args:     x: Euclidean point, shape (dim,)     y: Euclidean point, shape (dim,)     c: Curvature (ignored, kept for consistency with other manifolds)</p> <p>Returns:     Euclidean distance ||x - y||, scalar</p> Source code in <code>hyperbolix/manifolds/euclidean.py</code> <pre><code>def dist(\n    x: Float[Array, \"dim\"],\n    y: Float[Array, \"dim\"],\n    c: float = 0.0,\n) -&gt; Float[Array, \"\"]:\n    \"\"\"Compute geodesic distance between Euclidean points x and y.\n\n    Args:\n        x: Euclidean point, shape (dim,)\n        y: Euclidean point, shape (dim,)\n        c: Curvature (ignored, kept for consistency with other manifolds)\n\n    Returns:\n        Euclidean distance ||x - y||, scalar\n    \"\"\"\n    return jnp.linalg.norm(x - y)\n</code></pre>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.euclidean.expmap","title":"expmap","text":"<pre><code>expmap(\n    v: Float[Array, dim],\n    x: Float[Array, dim],\n    c: float = 0.0,\n) -&gt; Float[Array, dim]\n</code></pre> <p>Exponential map: map tangent vector v at point x to manifold.</p> <p>In Euclidean space, this is simply addition.</p> <p>Args:     v: Tangent vector at x, shape (dim,)     x: Euclidean point, shape (dim,)     c: Curvature (ignored, kept for consistency with other manifolds)</p> <p>Returns:     Point x + v, shape (dim,)</p> Source code in <code>hyperbolix/manifolds/euclidean.py</code> <pre><code>def expmap(v: Float[Array, \"dim\"], x: Float[Array, \"dim\"], c: float = 0.0) -&gt; Float[Array, \"dim\"]:\n    \"\"\"Exponential map: map tangent vector v at point x to manifold.\n\n    In Euclidean space, this is simply addition.\n\n    Args:\n        v: Tangent vector at x, shape (dim,)\n        x: Euclidean point, shape (dim,)\n        c: Curvature (ignored, kept for consistency with other manifolds)\n\n    Returns:\n        Point x + v, shape (dim,)\n    \"\"\"\n    return x + v\n</code></pre>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.euclidean.logmap","title":"logmap","text":"<pre><code>logmap(\n    y: Float[Array, dim],\n    x: Float[Array, dim],\n    c: float = 0.0,\n) -&gt; Float[Array, dim]\n</code></pre> <p>Logarithmic map: map point y to tangent space at point x.</p> <p>In Euclidean space, this is subtraction.</p> <p>Args:     y: Euclidean point, shape (dim,)     x: Euclidean point, shape (dim,)     c: Curvature (ignored, kept for consistency with other manifolds)</p> <p>Returns:     Tangent vector y - x, shape (dim,)</p> Source code in <code>hyperbolix/manifolds/euclidean.py</code> <pre><code>def logmap(y: Float[Array, \"dim\"], x: Float[Array, \"dim\"], c: float = 0.0) -&gt; Float[Array, \"dim\"]:\n    \"\"\"Logarithmic map: map point y to tangent space at point x.\n\n    In Euclidean space, this is subtraction.\n\n    Args:\n        y: Euclidean point, shape (dim,)\n        x: Euclidean point, shape (dim,)\n        c: Curvature (ignored, kept for consistency with other manifolds)\n\n    Returns:\n        Tangent vector y - x, shape (dim,)\n    \"\"\"\n    return y - x\n</code></pre>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.euclidean.ptransp","title":"ptransp","text":"<pre><code>ptransp(\n    v: Float[Array, dim],\n    x: Float[Array, dim],\n    y: Float[Array, dim],\n    c: float = 0.0,\n) -&gt; Float[Array, dim]\n</code></pre> <p>Parallel transport tangent vector v from point x to point y.</p> <p>In Euclidean space, tangent spaces are identical everywhere (identity).</p> <p>Args:     v: Tangent vector at x, shape (dim,)     x: Euclidean point (ignored), shape (dim,)     y: Euclidean point (ignored), shape (dim,)     c: Curvature (ignored, kept for consistency with other manifolds)</p> <p>Returns:     Tangent vector v (unchanged), shape (dim,)</p> Source code in <code>hyperbolix/manifolds/euclidean.py</code> <pre><code>def ptransp(\n    v: Float[Array, \"dim\"],\n    x: Float[Array, \"dim\"],\n    y: Float[Array, \"dim\"],\n    c: float = 0.0,\n) -&gt; Float[Array, \"dim\"]:\n    \"\"\"Parallel transport tangent vector v from point x to point y.\n\n    In Euclidean space, tangent spaces are identical everywhere (identity).\n\n    Args:\n        v: Tangent vector at x, shape (dim,)\n        x: Euclidean point (ignored), shape (dim,)\n        y: Euclidean point (ignored), shape (dim,)\n        c: Curvature (ignored, kept for consistency with other manifolds)\n\n    Returns:\n        Tangent vector v (unchanged), shape (dim,)\n    \"\"\"\n    return v\n</code></pre>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.euclidean.egrad2rgrad","title":"egrad2rgrad","text":"<pre><code>egrad2rgrad(\n    grad: Float[Array, dim],\n    x: Float[Array, dim],\n    c: float = 0.0,\n) -&gt; Float[Array, dim]\n</code></pre> <p>Convert Euclidean gradient to Riemannian gradient.</p> <p>In Euclidean space, these are identical.</p> <p>Args:     grad: Euclidean gradient, shape (dim,)     x: Euclidean point (ignored), shape (dim,)     c: Curvature (ignored, kept for consistency with other manifolds)</p> <p>Returns:     Riemannian gradient (same as Euclidean gradient), shape (dim,)</p> Source code in <code>hyperbolix/manifolds/euclidean.py</code> <pre><code>def egrad2rgrad(grad: Float[Array, \"dim\"], x: Float[Array, \"dim\"], c: float = 0.0) -&gt; Float[Array, \"dim\"]:\n    \"\"\"Convert Euclidean gradient to Riemannian gradient.\n\n    In Euclidean space, these are identical.\n\n    Args:\n        grad: Euclidean gradient, shape (dim,)\n        x: Euclidean point (ignored), shape (dim,)\n        c: Curvature (ignored, kept for consistency with other manifolds)\n\n    Returns:\n        Riemannian gradient (same as Euclidean gradient), shape (dim,)\n    \"\"\"\n    return grad\n</code></pre>"},{"location":"api-reference/manifolds/#poincare-ball","title":"Poincar\u00e9 Ball","text":"<p>The Poincar\u00e9 ball model with M\u00f6bius operations.</p> <p>Distance Versions</p> <p>The Poincar\u00e9 ball provides 4 distance computation methods via the <code>version</code> parameter:</p> <ul> <li><code>version=0</code>: M\u00f6bius addition formula (fastest)</li> <li><code>version=1</code>: Direct metric tensor integration</li> <li><code>version=2</code>: Lorentzian model proxy</li> <li><code>version=3</code>: Conformal factor integration</li> </ul> <p>Different versions offer trade-offs between speed and numerical stability.</p>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.poincare","title":"hyperbolix.manifolds.poincare","text":"<p>Poincar\u00e9 Ball manifold - vmap-native pure functional implementation.</p> <p>JAX port with vmap-native API. All functions operate on single points/vectors with shape (dim,). Use jax.vmap for batch operations.</p> <p>Convention: ||x||^2 &lt; 1/c with c &gt; 0 and sectional curvature -c.</p> JIT Compilation &amp; Batching <p>All functions work with single points and return scalars or vectors. Use jax.vmap for batching and jax.jit for compilation:</p> <pre><code>&gt;&gt;&gt; import jax\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from hyperbolix.manifolds import poincare\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Single point operations\n&gt;&gt;&gt; x = jnp.array([0.1, 0.2])\n&gt;&gt;&gt; y = jnp.array([0.3, 0.4])\n&gt;&gt;&gt; distance = poincare.dist(x, y, c=1.0, version_idx=poincare.VERSION_MOBIUS_DIRECT)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Batch operations with vmap\n&gt;&gt;&gt; x_batch = jnp.array([[0.1, 0.2], [0.15, 0.25]])  # (batch, dim)\n&gt;&gt;&gt; y_batch = jnp.array([[0.3, 0.4], [0.35, 0.45]])\n&gt;&gt;&gt; dist_batched = jax.vmap(poincare.dist, in_axes=(0, 0, None, None))\n&gt;&gt;&gt; distances = dist_batched(x_batch, y_batch, 1.0, poincare.VERSION_MOBIUS_DIRECT)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # JIT compilation\n&gt;&gt;&gt; dist_jit = jax.jit(poincare.dist, static_argnames=['version_idx'])\n&gt;&gt;&gt; distance = dist_jit(x, y, c=1.0, version_idx=poincare.VERSION_MOBIUS_DIRECT)\n</code></pre> <p>Version Constants:     VERSION_MOBIUS_DIRECT (0): Direct M\u00f6bius distance formula (fastest)     VERSION_MOBIUS (1): M\u00f6bius distance via addition     VERSION_METRIC_TENSOR (2): Metric tensor induced distance     VERSION_LORENTZIAN_PROXY (3): Lorentzian proxy distance</p> <p>Note: Keep curvature parameter 'c' dynamic to support learnable curvature. Use version_idx as static argument for JIT (static_argnames=['version_idx']).</p> Numerical Precision and Float32 Limitations <p>Operations involving points near the boundary (||x|| \u2248 1/\u221ac) can suffer from numerical instability, especially with float32. The conformal factor \u03bb(x) = 2/(1-c||x||\u00b2) grows exponentially as points approach the boundary:</p> <ul> <li>At d(0,x) \u2248 5: \u03bb(x) \u2248 100</li> <li>At d(0,x) \u2248 7: \u03bb(x) \u2248 1,000</li> <li>At d(0,x) \u2248 10: \u03bb(x) \u2248 10,000+</li> </ul> <p>Float32 (~7 significant digits) loses precision in operations like: - logmap/tangent_norm: divide by \u03bb(x), then multiply by \u03bb(x) - expmap: multiplies by large \u03bb(x) values - addition: combines terms with vastly different scales</p> <p>For numerical accuracy with large distances or near-boundary points: - Use float64 when possible - Expect ~3% relative error with float32 for distances &gt; 10 - Consider projection after operations to maintain manifold constraints</p>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.poincare.proj","title":"proj","text":"<pre><code>proj(x: Float[Array, dim], c: float) -&gt; Float[Array, dim]\n</code></pre> <p>Project point onto Poincar\u00e9 ball by clipping norm.</p> <p>Args:     x: Point to project, shape (dim,)     c: Curvature (positive)</p> <p>Returns:     Projected point with ||x|| &lt; 1/\u221ac, shape (dim,)</p> Source code in <code>hyperbolix/manifolds/poincare.py</code> <pre><code>def proj(x: Float[Array, \"dim\"], c: float) -&gt; Float[Array, \"dim\"]:\n    \"\"\"Project point onto Poincar\u00e9 ball by clipping norm.\n\n    Args:\n        x: Point to project, shape (dim,)\n        c: Curvature (positive)\n\n    Returns:\n        Projected point with ||x|| &lt; 1/\u221ac, shape (dim,)\n    \"\"\"\n    max_norm_eps = _get_max_norm_eps(x)\n    norm = jnp.linalg.norm(x)\n    max_norm = (1.0 / jnp.sqrt(c)) - max_norm_eps\n    cond = norm &gt; max_norm\n    return jnp.where(cond, x * (max_norm / jnp.maximum(norm, MIN_NORM)), x)\n</code></pre>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.poincare.dist","title":"dist","text":"<pre><code>dist(\n    x: Float[Array, dim],\n    y: Float[Array, dim],\n    c: float,\n    version_idx: int = VERSION_MOBIUS_DIRECT,\n) -&gt; Float[Array, \"\"]\n</code></pre> <p>Compute geodesic distance between Poincar\u00e9 ball points.</p> <p>Args:     x: Poincar\u00e9 ball point, shape (dim,)     y: Poincar\u00e9 ball point, shape (dim,)     c: Curvature (positive)     version_idx: Distance version index (use VERSION_* constants)</p> <p>Returns:     Geodesic distance d(x, y), scalar</p> <p>References:     Ganea et al. \"Hyperbolic neural networks.\" NeurIPS 2018.     Law et al. \"Lorentzian distance learning.\" ICML 2019.</p> Source code in <code>hyperbolix/manifolds/poincare.py</code> <pre><code>def dist(\n    x: Float[Array, \"dim\"],\n    y: Float[Array, \"dim\"],\n    c: float,\n    version_idx: int = VERSION_MOBIUS_DIRECT,\n) -&gt; Float[Array, \"\"]:\n    \"\"\"Compute geodesic distance between Poincar\u00e9 ball points.\n\n    Args:\n        x: Poincar\u00e9 ball point, shape (dim,)\n        y: Poincar\u00e9 ball point, shape (dim,)\n        c: Curvature (positive)\n        version_idx: Distance version index (use VERSION_* constants)\n\n    Returns:\n        Geodesic distance d(x, y), scalar\n\n    References:\n        Ganea et al. \"Hyperbolic neural networks.\" NeurIPS 2018.\n        Law et al. \"Lorentzian distance learning.\" ICML 2019.\n    \"\"\"\n    return lax.switch(version_idx, [_dist_mobius_direct, _dist_mobius, _dist_metric_tensor, _dist_lorentzian_proxy], x, y, c)\n</code></pre>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.poincare.expmap","title":"expmap","text":"<pre><code>expmap(\n    v: Float[Array, dim], x: Float[Array, dim], c: float\n) -&gt; Float[Array, dim]\n</code></pre> <p>Exponential map: map tangent vector v at point x to manifold.</p> <p>Args:     v: Tangent vector at x, shape (dim,)     x: Poincar\u00e9 ball point, shape (dim,)     c: Curvature (positive)</p> <p>Returns:     Point exp_x(v), shape (dim,)</p> <p>References:     Ganea et al. \"Hyperbolic neural networks.\" NeurIPS 2018.</p> Source code in <code>hyperbolix/manifolds/poincare.py</code> <pre><code>def expmap(v: Float[Array, \"dim\"], x: Float[Array, \"dim\"], c: float) -&gt; Float[Array, \"dim\"]:\n    \"\"\"Exponential map: map tangent vector v at point x to manifold.\n\n    Args:\n        v: Tangent vector at x, shape (dim,)\n        x: Poincar\u00e9 ball point, shape (dim,)\n        c: Curvature (positive)\n\n    Returns:\n        Point exp_x(v), shape (dim,)\n\n    References:\n        Ganea et al. \"Hyperbolic neural networks.\" NeurIPS 2018.\n    \"\"\"\n    v_norm = jnp.linalg.norm(v)\n    c_norm_prod = jnp.maximum(jnp.sqrt(c) * v_norm, MIN_NORM)\n    lambda_x = _conformal_factor(x, c)\n    second_term = jnp.tanh(c_norm_prod * lambda_x / 2) / c_norm_prod * v\n    second_term = proj(second_term, c)\n    res = addition(x, second_term, c)\n    return res\n</code></pre>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.poincare.logmap","title":"logmap","text":"<pre><code>logmap(\n    y: Float[Array, dim], x: Float[Array, dim], c: float\n) -&gt; Float[Array, dim]\n</code></pre> <p>Logarithmic map: map point y to tangent space at point x.</p> <p>Args:     y: Poincar\u00e9 ball point, shape (dim,)     x: Poincar\u00e9 ball point, shape (dim,)     c: Curvature (positive)</p> <p>Returns:     Tangent vector log_x(y), shape (dim,)</p> <p>References:     Ganea et al. \"Hyperbolic neural networks.\" NeurIPS 2018.</p> Source code in <code>hyperbolix/manifolds/poincare.py</code> <pre><code>def logmap(y: Float[Array, \"dim\"], x: Float[Array, \"dim\"], c: float) -&gt; Float[Array, \"dim\"]:\n    \"\"\"Logarithmic map: map point y to tangent space at point x.\n\n    Args:\n        y: Poincar\u00e9 ball point, shape (dim,)\n        x: Poincar\u00e9 ball point, shape (dim,)\n        c: Curvature (positive)\n\n    Returns:\n        Tangent vector log_x(y), shape (dim,)\n\n    References:\n        Ganea et al. \"Hyperbolic neural networks.\" NeurIPS 2018.\n    \"\"\"\n    sub = addition(-x, y, c)\n    x2y2 = jnp.dot(x, x) * jnp.dot(y, y)\n    xy = jnp.dot(x, y)\n    num = jnp.linalg.norm(y - x)\n    denom = jnp.sqrt(jnp.maximum(1 - 2 * c * xy + c**2 * x2y2, MIN_NORM))\n    sub_norm = num / denom\n    c_norm_prod = jnp.maximum(jnp.sqrt(c) * sub_norm, MIN_NORM)\n    lambda_x = _conformal_factor(x, c)\n    res = 2 * atanh(c_norm_prod) / (c_norm_prod * lambda_x) * sub\n    return res\n</code></pre>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.poincare.ptransp","title":"ptransp","text":"<pre><code>ptransp(\n    v: Float[Array, dim],\n    x: Float[Array, dim],\n    y: Float[Array, dim],\n    c: float,\n) -&gt; Float[Array, dim]\n</code></pre> <p>Parallel transport tangent vector v from point x to point y.</p> <p>Args:     v: Tangent vector at x, shape (dim,)     x: Poincar\u00e9 ball point, shape (dim,)     y: Poincar\u00e9 ball point, shape (dim,)     c: Curvature (positive)</p> <p>Returns:     Parallel transported tangent vector, shape (dim,)</p> <p>References:     Ganea et al. \"Hyperbolic neural networks.\" NeurIPS 2018.</p> Source code in <code>hyperbolix/manifolds/poincare.py</code> <pre><code>def ptransp(v: Float[Array, \"dim\"], x: Float[Array, \"dim\"], y: Float[Array, \"dim\"], c: float) -&gt; Float[Array, \"dim\"]:\n    \"\"\"Parallel transport tangent vector v from point x to point y.\n\n    Args:\n        v: Tangent vector at x, shape (dim,)\n        x: Poincar\u00e9 ball point, shape (dim,)\n        y: Poincar\u00e9 ball point, shape (dim,)\n        c: Curvature (positive)\n\n    Returns:\n        Parallel transported tangent vector, shape (dim,)\n\n    References:\n        Ganea et al. \"Hyperbolic neural networks.\" NeurIPS 2018.\n    \"\"\"\n    lambda_x = _conformal_factor(x, c)\n    lambda_y = _conformal_factor(y, c)\n    return _gyration(y, -x, v, c) * (lambda_x / lambda_y)\n</code></pre>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.poincare.egrad2rgrad","title":"egrad2rgrad","text":"<pre><code>egrad2rgrad(\n    grad: Float[Array, dim], x: Float[Array, dim], c: float\n) -&gt; Float[Array, dim]\n</code></pre> <p>Convert Euclidean gradient to Riemannian gradient.</p> <p>Args:     grad: Euclidean gradient, shape (dim,)     x: Poincar\u00e9 ball point, shape (dim,)     c: Curvature (positive)</p> <p>Returns:     Riemannian gradient, shape (dim,)</p> <p>References:     Ganea et al. \"Hyperbolic neural networks.\" NeurIPS 2018.</p> Source code in <code>hyperbolix/manifolds/poincare.py</code> <pre><code>def egrad2rgrad(grad: Float[Array, \"dim\"], x: Float[Array, \"dim\"], c: float) -&gt; Float[Array, \"dim\"]:\n    \"\"\"Convert Euclidean gradient to Riemannian gradient.\n\n    Args:\n        grad: Euclidean gradient, shape (dim,)\n        x: Poincar\u00e9 ball point, shape (dim,)\n        c: Curvature (positive)\n\n    Returns:\n        Riemannian gradient, shape (dim,)\n\n    References:\n        Ganea et al. \"Hyperbolic neural networks.\" NeurIPS 2018.\n    \"\"\"\n    lambda_x = _conformal_factor(x, c)\n    return grad / (lambda_x**2)\n</code></pre>"},{"location":"api-reference/manifolds/#hyperboloid","title":"Hyperboloid","text":"<p>The hyperboloid (Lorentz) model with Minkowski geometry.</p> <p>Lorentz Operations</p> <p>The hyperboloid module includes specialized operations for convolutional layers:</p> <ul> <li><code>lorentz_boost</code>: Lorentz boost transformation</li> <li><code>distance_rescale</code>: Distance-based rescaling</li> <li><code>hcat</code>: Lorentz direct concatenation for convolutions</li> </ul>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.hyperboloid","title":"hyperbolix.manifolds.hyperboloid","text":"<p>Hyperboloid manifold - vmap-native pure functional implementation.</p> <p>JAX port with vmap-native API. All functions operate on single points/vectors in ambient (dim+1)-dimensional space. Use jax.vmap for batch operations.</p> <p>Convention: -x\u2080\u00b2 + ||x_rest||\u00b2 = -1/c with c &gt; 0, x\u2080 &gt; 0, and sectional curvature -c.</p> JIT Compilation &amp; Batching <p>All functions work with single points and return scalars or vectors. Use jax.vmap for batching and jax.jit for compilation:</p> <pre><code>&gt;&gt;&gt; import jax\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from hyperbolix.manifolds import hyperboloid\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Single point operations (points in ambient R^(dim+1))\n&gt;&gt;&gt; x = jnp.array([1.0, 0.1, 0.2])  # Will be projected\n&gt;&gt;&gt; y = jnp.array([1.0, 0.3, 0.4])\n&gt;&gt;&gt; x = hyperboloid.proj(x, c=1.0)\n&gt;&gt;&gt; y = hyperboloid.proj(y, c=1.0)\n&gt;&gt;&gt; distance = hyperboloid.dist(x, y, c=1.0, version_idx=hyperboloid.VERSION_DEFAULT)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Batch operations with vmap\n&gt;&gt;&gt; x_batch = jnp.array([[1.0, 0.1, 0.2], [1.0, 0.15, 0.25]])  # (batch, dim+1)\n&gt;&gt;&gt; y_batch = jnp.array([[1.0, 0.3, 0.4], [1.0, 0.35, 0.45]])\n&gt;&gt;&gt; dist_batched = jax.vmap(hyperboloid.dist, in_axes=(0, 0, None, None))\n&gt;&gt;&gt; distances = dist_batched(x_batch, y_batch, 1.0, hyperboloid.VERSION_DEFAULT)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # JIT compilation\n&gt;&gt;&gt; dist_jit = jax.jit(hyperboloid.dist, static_argnames=['version_idx'])\n&gt;&gt;&gt; distance = dist_jit(x, y, c=1.0, version_idx=hyperboloid.VERSION_DEFAULT)\n</code></pre> <p>Version Constants:     VERSION_DEFAULT (0): Standard acosh distance with hard clipping     VERSION_SMOOTHENED (1): Smoothened distance with soft clamping</p> <p>Note: Keep curvature parameter 'c' dynamic to support learnable curvature. Use version_idx as static argument for JIT (static_argnames=['version_idx']).</p>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.hyperboloid.proj","title":"proj","text":"<pre><code>proj(\n    x: Float[Array, dim_plus_1], c: Float[Array, \"\"] | float\n) -&gt; Float[Array, dim_plus_1]\n</code></pre> <p>Project point onto hyperboloid by adjusting temporal component.</p> <p>Args:     x: Point to project, shape (dim+1,)     c: Curvature (positive)</p> <p>Returns:     Projected point with -x\u2080\u00b2 + ||x_rest||\u00b2 = -1/c, x\u2080 &gt; 0, shape (dim+1,)</p> Source code in <code>hyperbolix/manifolds/hyperboloid.py</code> <pre><code>def proj(x: Float[Array, \"dim_plus_1\"], c: Float[Array, \"\"] | float) -&gt; Float[Array, \"dim_plus_1\"]:\n    \"\"\"Project point onto hyperboloid by adjusting temporal component.\n\n    Args:\n        x: Point to project, shape (dim+1,)\n        c: Curvature (positive)\n\n    Returns:\n        Projected point with -x\u2080\u00b2 + ||x_rest||\u00b2 = -1/c, x\u2080 &gt; 0, shape (dim+1,)\n    \"\"\"\n    x_rest = x[1:]\n    x_rest_sqnorm = jnp.dot(x_rest, x_rest)\n    x0_new = jnp.sqrt(jnp.maximum(1.0 / c + x_rest_sqnorm, MIN_NORM))\n    return jnp.concatenate([x0_new[None], x_rest])\n</code></pre>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.hyperboloid.dist","title":"dist","text":"<pre><code>dist(\n    x: Float[Array, dim_plus_1],\n    y: Float[Array, dim_plus_1],\n    c: Float[Array, \"\"] | float,\n    version_idx: int = VERSION_DEFAULT,\n) -&gt; Float[Array, \"\"]\n</code></pre> <p>Compute geodesic distance between hyperboloid points.</p> <p>Args:     x: Hyperboloid point, shape (dim+1,)     y: Hyperboloid point, shape (dim+1,)     c: Curvature (positive)     version_idx: Distance version index (use VERSION_* constants)</p> <p>Returns:     Geodesic distance d(x, y), scalar</p> <p>References:     Nickel &amp; Kiela. \"Poincar\u00e9 embeddings for learning hierarchical representations.\" NeurIPS 2017.</p> Source code in <code>hyperbolix/manifolds/hyperboloid.py</code> <pre><code>def dist(\n    x: Float[Array, \"dim_plus_1\"],\n    y: Float[Array, \"dim_plus_1\"],\n    c: Float[Array, \"\"] | float,\n    version_idx: int = VERSION_DEFAULT,\n) -&gt; Float[Array, \"\"]:\n    \"\"\"Compute geodesic distance between hyperboloid points.\n\n    Args:\n        x: Hyperboloid point, shape (dim+1,)\n        y: Hyperboloid point, shape (dim+1,)\n        c: Curvature (positive)\n        version_idx: Distance version index (use VERSION_* constants)\n\n    Returns:\n        Geodesic distance d(x, y), scalar\n\n    References:\n        Nickel &amp; Kiela. \"Poincar\u00e9 embeddings for learning hierarchical representations.\" NeurIPS 2017.\n    \"\"\"\n    return lax.switch(version_idx, [_dist_default, _dist_smoothened], x, y, c)\n</code></pre>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.hyperboloid.expmap","title":"expmap","text":"<pre><code>expmap(\n    v: Float[Array, dim_plus_1],\n    x: Float[Array, dim_plus_1],\n    c: Float[Array, \"\"] | float,\n) -&gt; Float[Array, dim_plus_1]\n</code></pre> <p>Exponential map: map tangent vector v at point x to manifold.</p> <p>Args:     v: Tangent vector at x, shape (dim+1,)     x: Hyperboloid point, shape (dim+1,)     c: Curvature (positive)</p> <p>Returns:     Point exp_x(v), shape (dim+1,)</p> <p>References:     Ganea et al. \"Hyperbolic neural networks.\" NeurIPS 2018.</p> Source code in <code>hyperbolix/manifolds/hyperboloid.py</code> <pre><code>def expmap(\n    v: Float[Array, \"dim_plus_1\"], x: Float[Array, \"dim_plus_1\"], c: Float[Array, \"\"] | float\n) -&gt; Float[Array, \"dim_plus_1\"]:\n    \"\"\"Exponential map: map tangent vector v at point x to manifold.\n\n    Args:\n        v: Tangent vector at x, shape (dim+1,)\n        x: Hyperboloid point, shape (dim+1,)\n        c: Curvature (positive)\n\n    Returns:\n        Point exp_x(v), shape (dim+1,)\n\n    References:\n        Ganea et al. \"Hyperbolic neural networks.\" NeurIPS 2018.\n    \"\"\"\n    sqrt_c = jnp.sqrt(c)\n    v_sqnorm = jnp.clip(_minkowski_inner(v, v), min=0.0)\n    v_norm = jnp.sqrt(v_sqnorm)\n    c_norm_prod = sqrt_c * v_norm\n\n    denom = jnp.maximum(c_norm_prod, MIN_NORM)\n    cosh_term = jnp.cosh(c_norm_prod) * x\n    sinh_term = jnp.sinh(c_norm_prod) / denom * v\n\n    res = cosh_term + sinh_term\n    res = proj(res, c)\n    return res\n</code></pre>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.hyperboloid.logmap","title":"logmap","text":"<pre><code>logmap(\n    y: Float[Array, dim_plus_1],\n    x: Float[Array, dim_plus_1],\n    c: Float[Array, \"\"] | float,\n) -&gt; Float[Array, dim_plus_1]\n</code></pre> <p>Logarithmic map: map point y to tangent space at point x.</p> <p>Args:     y: Hyperboloid point, shape (dim+1,)     x: Hyperboloid point, shape (dim+1,)     c: Curvature (positive)</p> <p>Returns:     Tangent vector log_x(y), shape (dim+1,)</p> <p>References:     Ganea et al. \"Hyperbolic neural networks.\" NeurIPS 2018.</p> Source code in <code>hyperbolix/manifolds/hyperboloid.py</code> <pre><code>def logmap(\n    y: Float[Array, \"dim_plus_1\"], x: Float[Array, \"dim_plus_1\"], c: Float[Array, \"\"] | float\n) -&gt; Float[Array, \"dim_plus_1\"]:\n    \"\"\"Logarithmic map: map point y to tangent space at point x.\n\n    Args:\n        y: Hyperboloid point, shape (dim+1,)\n        x: Hyperboloid point, shape (dim+1,)\n        c: Curvature (positive)\n\n    Returns:\n        Tangent vector log_x(y), shape (dim+1,)\n\n    References:\n        Ganea et al. \"Hyperbolic neural networks.\" NeurIPS 2018.\n    \"\"\"\n    mink_inner = _minkowski_inner(x, y)\n    dist_xy = dist(x, y, c=c)\n    direction = y + c * mink_inner * x\n\n    dir_sqnorm = _minkowski_inner(direction, direction)\n    dir_norm = jnp.sqrt(jnp.maximum(dir_sqnorm, MIN_NORM))\n    res = dist_xy * direction / dir_norm\n    res = tangent_proj(res, x, c)\n    return res\n</code></pre>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.hyperboloid.ptransp","title":"ptransp","text":"<pre><code>ptransp(\n    v: Float[Array, dim_plus_1],\n    x: Float[Array, dim_plus_1],\n    y: Float[Array, dim_plus_1],\n    c: float,\n) -&gt; Float[Array, dim_plus_1]\n</code></pre> <p>Parallel transport tangent vector v from point x to point y.</p> <p>Args:     v: Tangent vector at x, shape (dim+1,)     x: Hyperboloid point, shape (dim+1,)     y: Hyperboloid point, shape (dim+1,)     c: Curvature (positive)</p> <p>Returns:     Parallel transported tangent vector, shape (dim+1,)</p> <p>References:     Aaron Lou, et al. \"Differentiating through the fr\u00e9chet mean.\"         International conference on machine learning (2020).</p> Source code in <code>hyperbolix/manifolds/hyperboloid.py</code> <pre><code>def ptransp(\n    v: Float[Array, \"dim_plus_1\"],\n    x: Float[Array, \"dim_plus_1\"],\n    y: Float[Array, \"dim_plus_1\"],\n    c: float,\n) -&gt; Float[Array, \"dim_plus_1\"]:\n    \"\"\"Parallel transport tangent vector v from point x to point y.\n\n    Args:\n        v: Tangent vector at x, shape (dim+1,)\n        x: Hyperboloid point, shape (dim+1,)\n        y: Hyperboloid point, shape (dim+1,)\n        c: Curvature (positive)\n\n    Returns:\n        Parallel transported tangent vector, shape (dim+1,)\n\n    References:\n        Aaron Lou, et al. \"Differentiating through the fr\u00e9chet mean.\"\n            International conference on machine learning (2020).\n    \"\"\"\n    # Compute Minkowski inner products\n    vy = _minkowski_inner(v, y)  # \u27e8v, y\u27e9_L\n    xy = _minkowski_inner(x, y)  # \u27e8x, y\u27e9_L\n\n    # denom = 1/c - \u27e8x, y\u27e9_L\n    denom = 1.0 / c - xy\n    denom = jnp.maximum(denom, MIN_NORM)  # Numerical stability\n\n    # scale = \u27e8v, y\u27e9_L / denom\n    scale = vy / denom\n\n    # res = v + scale * (x + y)\n    res = v + scale * (x + y)\n    res = tangent_proj(res, y, c)\n    return res\n</code></pre>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.hyperboloid.egrad2rgrad","title":"egrad2rgrad","text":"<pre><code>egrad2rgrad(\n    grad: Float[Array, dim_plus_1],\n    x: Float[Array, dim_plus_1],\n    c: float,\n) -&gt; Float[Array, dim_plus_1]\n</code></pre> <p>Convert Euclidean gradient to Riemannian gradient.</p> <p>Projects Euclidean gradient onto tangent space.</p> <p>Args:     grad: Euclidean gradient, shape (dim+1,)     x: Hyperboloid point, shape (dim+1,)     c: Curvature (positive)</p> <p>Returns:     Riemannian gradient, shape (dim+1,)</p> <p>References:     Ganea et al. \"Hyperbolic neural networks.\" NeurIPS 2018.</p> Source code in <code>hyperbolix/manifolds/hyperboloid.py</code> <pre><code>def egrad2rgrad(grad: Float[Array, \"dim_plus_1\"], x: Float[Array, \"dim_plus_1\"], c: float) -&gt; Float[Array, \"dim_plus_1\"]:\n    \"\"\"Convert Euclidean gradient to Riemannian gradient.\n\n    Projects Euclidean gradient onto tangent space.\n\n    Args:\n        grad: Euclidean gradient, shape (dim+1,)\n        x: Hyperboloid point, shape (dim+1,)\n        c: Curvature (positive)\n\n    Returns:\n        Riemannian gradient, shape (dim+1,)\n\n    References:\n        Ganea et al. \"Hyperbolic neural networks.\" NeurIPS 2018.\n    \"\"\"\n    # In Lorentzian signature the temporal component carries a negative sign.\n    # Flip it before projecting so we project the Riemannian gradient, matching PyTorch.\n    grad_lorentz = grad.at[0].set(-grad[0])\n\n    # Orthogonally project the Lorentzian gradient onto the tangent space.\n    inner_xx = _minkowski_inner(x, x)\n    scale = jnp.sqrt(jnp.maximum(-c * inner_xx, MIN_NORM))\n    x_normed = x / scale\n\n    denom = _minkowski_inner(x_normed, x_normed)\n    coeff = _minkowski_inner(x_normed, grad_lorentz) / denom\n    return grad_lorentz - coeff * x_normed\n</code></pre>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.hyperboloid.lorentz_boost","title":"lorentz_boost","text":"<pre><code>lorentz_boost(\n    x: Float[Array, dim_plus_1],\n    v_raw: Float[Array, dim],\n    c: float,\n) -&gt; Float[Array, dim_plus_1]\n</code></pre> <p>Apply Lorentz boost to a single hyperboloid point.</p> <p>The Lorentz boost matrix is:     B = [[gamma,       -gammav^T              ],          [-gammav,     I + (gamma^2/(1+gamma))vv^T ]]</p> <p>where gamma = 1/sqrt(1 - ||v||^2), ||v|| &lt; 1.</p> <p>Args:     x: Single hyperboloid point in ambient space, shape (dim+1,)         Satisfies: -x\u2080\u00b2 + \u2016x_rest\u2016\u00b2 = -1/c     v_raw: Velocity vector (unclipped), shape (dim,)     c: Manifold curvature (positive)</p> <p>Returns:     Boosted point on hyperboloid, shape (dim+1,)</p> <p>Notes:     The velocity is projected to ensure \u2016v\u2016 &lt; 1 - \u03b5 for numerical stability.     The output is projected onto the manifold to correct numerical drift.</p> <pre><code>For batch operations, use jax.vmap:\n    &gt;&gt;&gt; lorentz_boost_batched = jax.vmap(lorentz_boost, in_axes=(0, None, None))\n    &gt;&gt;&gt; boosted = lorentz_boost_batched(x_batch, v, c)\n</code></pre> <p>References:     Ahmad Bdeir, et al. \"Fully hyperbolic convolutional neural networks for computer vision.\"         arXiv preprint arXiv:2303.15919 (2023).</p> Source code in <code>hyperbolix/manifolds/hyperboloid.py</code> <pre><code>def lorentz_boost(\n    x: Float[Array, \"dim_plus_1\"],\n    v_raw: Float[Array, \"dim\"],\n    c: float,\n) -&gt; Float[Array, \"dim_plus_1\"]:\n    \"\"\"Apply Lorentz boost to a single hyperboloid point.\n\n    The Lorentz boost matrix is:\n        B = [[gamma,       -gamma*v^T              ],\n             [-gamma*v,     I + (gamma^2/(1+gamma))*v*v^T ]]\n\n    where gamma = 1/sqrt(1 - ||v||^2), ||v|| &lt; 1.\n\n    Args:\n        x: Single hyperboloid point in ambient space, shape (dim+1,)\n            Satisfies: -x\u2080\u00b2 + \u2016x_rest\u2016\u00b2 = -1/c\n        v_raw: Velocity vector (unclipped), shape (dim,)\n        c: Manifold curvature (positive)\n\n    Returns:\n        Boosted point on hyperboloid, shape (dim+1,)\n\n    Notes:\n        The velocity is projected to ensure \u2016v\u2016 &lt; 1 - \u03b5 for numerical stability.\n        The output is projected onto the manifold to correct numerical drift.\n\n        For batch operations, use jax.vmap:\n            &gt;&gt;&gt; lorentz_boost_batched = jax.vmap(lorentz_boost, in_axes=(0, None, None))\n            &gt;&gt;&gt; boosted = lorentz_boost_batched(x_batch, v, c)\n\n    References:\n        Ahmad Bdeir, et al. \"Fully hyperbolic convolutional neural networks for computer vision.\"\n            arXiv preprint arXiv:2303.15919 (2023).\n    \"\"\"\n    # Numerical stability constant for velocity projection\n    epsilon = 1e-5\n\n    # Project velocity to ensure norm &lt; 1 - epsilon\n    # Formula: v = v_raw * min(1, (1 - epsilon) / |v_raw|)\n    v_norm = jnp.linalg.norm(v_raw)\n    scale = jnp.minimum(1.0, (1.0 - epsilon) / jnp.maximum(v_norm, MIN_NORM))\n    v = v_raw * scale\n\n    # Compute gamma = 1/\u221a(1 - \u2016v\u2016\u00b2)\n    v_sqnorm = jnp.dot(v, v)\n    gamma = 1.0 / jnp.sqrt(jnp.maximum(1.0 - v_sqnorm, MIN_NORM))\n\n    # Split input into time and spatial components\n    x_t = x[0]  # scalar\n    x_s = x[1:]  # Shape: (dim,)\n\n    # Compute v\u00b7x_s\n    v_dot_xs = jnp.dot(v, x_s)  # scalar\n\n    # New time component: gamma*x_t - gamma*(v.x_s)\n    new_t = gamma * x_t - gamma * v_dot_xs  # scalar\n\n    # New spatial component: -gamma*v*x_t + x_s + (gamma^2/(1+gamma))*v*(v.x_s)\n    new_s = -gamma * v * x_t + x_s + (gamma**2 / (1.0 + gamma)) * v * v_dot_xs  # Shape: (dim,)\n\n    # Concatenate time and spatial components\n    result = jnp.concatenate([new_t[None], new_s])  # Shape: (dim+1,)\n\n    # Project onto manifold to correct numerical drift\n    return proj(result, c)\n</code></pre>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.hyperboloid.distance_rescale","title":"distance_rescale","text":"<pre><code>distance_rescale(\n    x: Float[Array, dim_plus_1],\n    c: float,\n    x_t_max: float = 2000.0,\n    slope: float = 1.0,\n) -&gt; Float[Array, dim_plus_1]\n</code></pre> <p>Apply distance rescaling to bound hyperbolic distances (Eq. 2-3).</p> <p>Rescales distances from origin using:     D_rescaled = m \u00b7 tanh(D \u00b7 atanh(0.99) / (s\u00b7m))</p> <p>where m = D_max (computed from x_t_max) and s = slope.</p> <p>Then reconstructs spatial components:     x_s_rescaled = x_s \u00b7 sinh(\u221ac \u00b7 D_rescaled) / sinh(\u221ac \u00b7 D)</p> <p>Args:     x: Single hyperboloid point, shape (dim+1,)     c: Manifold curvature (positive)     x_t_max: Maximum time coordinate (default: 2000.0)     slope: Rescaling slope parameter (default: 1.0)</p> <p>Returns:     Rescaled point on hyperboloid, shape (dim+1,)</p> <p>Notes:     For points at or very near the origin (D \u2248 0), the point is returned     unchanged to avoid numerical instability from 0/0 in the scale computation.     The output is projected onto the manifold to correct numerical drift.</p> <pre><code>For batch operations, use jax.vmap:\n    &gt;&gt;&gt; distance_rescale_batched = jax.vmap(distance_rescale, in_axes=(0, None, None, None))\n    &gt;&gt;&gt; rescaled = distance_rescale_batched(x_batch, c, x_t_max, slope)\n</code></pre> <p>References:     Equation 2-3 from \"Fully Hyperbolic CNNs\" (Ahmad Bdeir et al., 2023).</p> Source code in <code>hyperbolix/manifolds/hyperboloid.py</code> <pre><code>def distance_rescale(\n    x: Float[Array, \"dim_plus_1\"],\n    c: float,\n    x_t_max: float = 2000.0,\n    slope: float = 1.0,\n) -&gt; Float[Array, \"dim_plus_1\"]:\n    \"\"\"Apply distance rescaling to bound hyperbolic distances (Eq. 2-3).\n\n    Rescales distances from origin using:\n        D_rescaled = m \u00b7 tanh(D \u00b7 atanh(0.99) / (s\u00b7m))\n\n    where m = D_max (computed from x_t_max) and s = slope.\n\n    Then reconstructs spatial components:\n        x_s_rescaled = x_s \u00b7 sinh(\u221ac \u00b7 D_rescaled) / sinh(\u221ac \u00b7 D)\n\n    Args:\n        x: Single hyperboloid point, shape (dim+1,)\n        c: Manifold curvature (positive)\n        x_t_max: Maximum time coordinate (default: 2000.0)\n        slope: Rescaling slope parameter (default: 1.0)\n\n    Returns:\n        Rescaled point on hyperboloid, shape (dim+1,)\n\n    Notes:\n        For points at or very near the origin (D \u2248 0), the point is returned\n        unchanged to avoid numerical instability from 0/0 in the scale computation.\n        The output is projected onto the manifold to correct numerical drift.\n\n        For batch operations, use jax.vmap:\n            &gt;&gt;&gt; distance_rescale_batched = jax.vmap(distance_rescale, in_axes=(0, None, None, None))\n            &gt;&gt;&gt; rescaled = distance_rescale_batched(x_batch, c, x_t_max, slope)\n\n    References:\n        Equation 2-3 from \"Fully Hyperbolic CNNs\" (Ahmad Bdeir et al., 2023).\n    \"\"\"\n    sqrt_c = jnp.sqrt(c)\n\n    # Extract time and spatial components\n    x_t = x[0]  # scalar\n    x_s = x[1:]  # Shape: (dim,)\n\n    # Compute distance from origin: D = acosh(\u221ac \u00b7 x_t) / \u221ac\n    arg_acosh = smooth_clamp_min(sqrt_c * x_t, 1.0)\n    D = acosh(arg_acosh) / sqrt_c  # scalar\n\n    # Compute max distance: D_max = acosh(\u221ac \u00b7 x_t_max) / \u221ac\n    arg_acosh_max = smooth_clamp_min(sqrt_c * x_t_max, 1.0)\n    D_max = acosh(arg_acosh_max) / sqrt_c\n\n    # Apply Eq. 2: D_rescaled = D_max \u00b7 tanh(D \u00b7 atanh(0.99) / (slope \u00b7 D_max))\n    # Clamp to avoid division by zero\n    D_max_safe = jnp.maximum(D_max, MIN_NORM)\n    slope_safe = jnp.maximum(slope, MIN_NORM)\n\n    atanh_val = jnp.atanh(0.99)\n    arg_tanh = D * atanh_val / (slope_safe * D_max_safe)\n    D_rescaled = D_max * jnp.tanh(arg_tanh)  # scalar\n\n    # Rescale spatial components (Eq. 3): scale = sinh(\u221ac \u00b7 D_rescaled) / sinh(\u221ac \u00b7 D)\n    sinh_rescaled = sinh(sqrt_c * D_rescaled)  # scalar\n    sinh_original = sinh(sqrt_c * D)  # scalar\n\n    # Handle origin edge case: when D \u2248 0, both sinh values are \u2248 0\n    # Use L'Hopital's rule: lim(D-&gt;0) sinh(a*D_rescaled)/sinh(a*D) = D_rescaled/D\n    # For D \u2248 0, D_rescaled \u2248 D * atanh(0.99) / slope, so scale \u2248 atanh(0.99) / slope\n    # We use a smooth transition: when sinh_original is small, fall back to this limit\n    is_near_origin = sinh_original &lt; 1e-6\n    scale_normal = sinh_rescaled / jnp.maximum(sinh_original, MIN_NORM)\n    scale_origin = atanh_val / slope_safe  # Limit value at origin\n    scale = cast(Array, jnp.where(is_near_origin, scale_origin, scale_normal))  # scalar\n\n    x_s_rescaled = x_s * scale  # Shape: (dim,)\n\n    # Reconstruct time component: x_t_rescaled = \u221a(\u2016x_s_rescaled\u2016\u00b2 + 1/c)\n    x_s_sqnorm = jnp.dot(x_s_rescaled, x_s_rescaled)  # scalar\n    x_t_rescaled = jnp.sqrt(jnp.maximum(x_s_sqnorm + 1.0 / c, MIN_NORM))  # scalar\n\n    # Combine time and spatial components\n    result = jnp.concatenate([x_t_rescaled[None], x_s_rescaled])  # Shape: (dim+1,)\n\n    # Project onto manifold to correct numerical drift\n    return proj(result, c)\n</code></pre>"},{"location":"api-reference/manifolds/#hyperbolix.manifolds.hyperboloid.hcat","title":"hcat","text":"<pre><code>hcat(\n    points: Float[Array, \"N n\"], c: float = 1.0\n) -&gt; Float[Array, dN_plus_1]\n</code></pre> <p>Lorentz direct concatenation for Hyperboloid points.</p> <p>Given N points on a d-dimensional Hyperboloid manifold (living in (d+1)-dimensional ambient space), concatenates them into a single point on a (dN)-dimensional Hyperboloid manifold (living in (dN+1)-dimensional ambient space).</p> <p>The formula is: y = [sqrt(sum(x_i[0]^2) - (N-1)/c), x_1[1:], ..., x_N[1:]]</p> <p>where x_i[0] is the time component and x_i[1:] are the space components.</p> <p>Args:     points: N points in (d+1)-dimensional ambient space, shape (N, d+1).             Each point satisfies: -x[0]^2 + sum(x[1:]^2) = -1/c     c: Manifold curvature (positive)</p> <p>Returns:     Single point in (dN+1)-dimensional ambient space, shape (dN+1,).     - Time coordinate: sqrt(sum(x_i[0]^2) - (N-1)/c)     - Space coordinates: concatenation of all input space components</p> <p>References:     Qu, M., &amp; Zou, J. (2022). Hyperbolic Hierarchical Knowledge Graph Embeddings for Link Prediction.     Ahmad Bdeir, et al. \"Fully hyperbolic convolutional neural networks for computer vision.\"         arXiv preprint arXiv:2303.15919 (2023).</p> <p>Notes:     The operation preserves the manifold structure: the output satisfies the Lorentz     constraint for the (dN)-dimensional manifold.</p> Source code in <code>hyperbolix/manifolds/hyperboloid.py</code> <pre><code>def hcat(\n    points: Float[Array, \"N n\"],\n    c: float = 1.0,\n) -&gt; Float[Array, \"dN_plus_1\"]:\n    \"\"\"Lorentz direct concatenation for Hyperboloid points.\n\n    Given N points on a d-dimensional Hyperboloid manifold (living in (d+1)-dimensional\n    ambient space), concatenates them into a single point on a (dN)-dimensional Hyperboloid\n    manifold (living in (dN+1)-dimensional ambient space).\n\n    The formula is:\n    y = [sqrt(sum(x_i[0]^2) - (N-1)/c), x_1[1:], ..., x_N[1:]]\n\n    where x_i[0] is the time component and x_i[1:] are the space components.\n\n    Args:\n        points: N points in (d+1)-dimensional ambient space, shape (N, d+1).\n                Each point satisfies: -x[0]^2 + sum(x[1:]^2) = -1/c\n        c: Manifold curvature (positive)\n\n    Returns:\n        Single point in (dN+1)-dimensional ambient space, shape (dN+1,).\n        - Time coordinate: sqrt(sum(x_i[0]^2) - (N-1)/c)\n        - Space coordinates: concatenation of all input space components\n\n    References:\n        Qu, M., &amp; Zou, J. (2022). Hyperbolic Hierarchical Knowledge Graph Embeddings for Link Prediction.\n        Ahmad Bdeir, et al. \"Fully hyperbolic convolutional neural networks for computer vision.\"\n            arXiv preprint arXiv:2303.15919 (2023).\n\n    Notes:\n        The operation preserves the manifold structure: the output satisfies the Lorentz\n        constraint for the (dN)-dimensional manifold.\n    \"\"\"\n    N, _ambient_dim = points.shape\n\n    # Extract time components (first coordinate of each point)\n    time_components = points[:, 0]  # (N,)\n\n    # Extract space components (remaining coordinates of each point)\n    space_components = points[:, 1:]  # (N, d)\n\n    # Compute new time coordinate using the formula\n    # Note: MINUS (N-1)/c, not plus!\n    # Numerical stability: clamp to MIN_NORM to handle points very close to origin\n    time_sq_sum = jnp.sum(time_components**2) - (N - 1) / c\n    time_new = jnp.sqrt(jnp.maximum(time_sq_sum, MIN_NORM))  # scalar\n\n    # Concatenate all space components: [x_1[1:], x_2[1:], ..., x_N[1:]]\n    space_concatenated = space_components.reshape(-1)  # (N*d,)\n\n    # Combine: [time_new, space_concatenated]\n    # Use time_new[None] instead of jnp.array([time_new]) to avoid extra allocation\n    result = jnp.concatenate([time_new[None], space_concatenated])  # (1 + N*d,) = (dN+1,)\n\n    return result\n</code></pre>"},{"location":"api-reference/manifolds/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/manifolds/#basic-distance-computation","title":"Basic Distance Computation","text":"<pre><code>import jax.numpy as jnp\nfrom hyperbolix.manifolds import poincare\n\nx = jnp.array([0.1, 0.2])\ny = jnp.array([0.3, -0.1])\nc = 1.0\n\n# Compute distance\ndistance = poincare.dist(x, y, c, version_idx=0)\n</code></pre>"},{"location":"api-reference/manifolds/#batched-operations-with-vmap","title":"Batched Operations with vmap","text":"<pre><code>import jax\nfrom hyperbolix.manifolds import hyperboloid\n\n# Batch of points\nx_batch = jax.random.normal(jax.random.PRNGKey(0), (100, 3))\ny_batch = jax.random.normal(jax.random.PRNGKey(1), (100, 3))\n\n# Project to hyperboloid\nx_proj = jax.vmap(hyperboloid.proj, in_axes=(0, None))(x_batch, c)\ny_proj = jax.vmap(hyperboloid.proj, in_axes=(0, None))(y_batch, c)\n\n# Compute distances\ndistances = jax.vmap(hyperboloid.dist, in_axes=(0, 0, None, None))(\n    x_proj, y_proj, c, 0\n)\n</code></pre>"},{"location":"api-reference/manifolds/#exponential-and-logarithmic-maps","title":"Exponential and Logarithmic Maps","text":"<pre><code>from hyperbolix.manifolds import poincare\n\n# Point on manifold\nx = poincare.proj(jnp.array([0.2, 0.3]), c=1.0)\n\n# Tangent vector\nv = jnp.array([0.1, -0.05])\n\n# Exponential map (move along geodesic)\ny = poincare.expmap(x, v, c=1.0)\n\n# Logarithmic map (inverse operation)\nv_recovered = poincare.logmap(x, y, c=1.0)\n</code></pre>"},{"location":"api-reference/manifolds/#numerical-considerations","title":"Numerical Considerations","text":"<p>Float32 Precision</p> <p>Float32 can cause numerical issues, especially in the Poincar\u00e9 ball near the boundary. Consider using float64 for:</p> <ul> <li>High curvature values (<code>c &gt; 1.0</code>)</li> <li>Points near manifold boundaries</li> <li>Deep neural networks with many layers</li> </ul> <p>See the Numerical Stability guide for details.</p>"},{"location":"api-reference/nn-layers/","title":"Neural Network Layers API","text":"<p>Hyperbolic neural network layers built with Flax NNX.</p>"},{"location":"api-reference/nn-layers/#overview","title":"Overview","text":"<p>Hyperbolix provides 15+ neural network layer classes and 5 activation functions for building hyperbolic deep learning models:</p> <ul> <li>Linear Layers: Poincar\u00e9 and Hyperboloid linear transformations</li> <li>Convolutional Layers: HCat-based and HRC-based hyperbolic convolutions (2D and 3D)</li> <li>Hypformer Components: HTC (Hyperbolic Transformation Component) and HRC (Hyperbolic Regularization Component) with curvature-change support</li> <li>Regression Layers: Single-layer classifiers with Riemannian geometry</li> <li>Activation Functions: Hyperbolic ReLU, Leaky ReLU, Tanh, Swish, GELU</li> <li>Helper Functions: Utilities for regression and conformal factor computation</li> </ul> <p>All layers follow Flax NNX conventions and store manifold module references.</p>"},{"location":"api-reference/nn-layers/#linear-layers","title":"Linear Layers","text":""},{"location":"api-reference/nn-layers/#poincare-linear","title":"Poincar\u00e9 Linear","text":""},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HypLinearPoincare","title":"hyperbolix.nn_layers.HypLinearPoincare","text":"<pre><code>HypLinearPoincare(\n    manifold_module: Any,\n    in_dim: int,\n    out_dim: int,\n    *,\n    rngs: Rngs,\n    input_space: str = \"manifold\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Hyperbolic Neural Networks fully connected layer (Poincar\u00e9 ball model).</p> <p>Computation steps:     0) Project the input tensor to the tangent space (optional)     1) Perform matrix vector multiplication in the tangent space at the origin.     2) Map the result to the manifold.     3) Add the manifold bias to the result.</p> <p>Parameters:</p> Name Type Description Default <code>manifold_module</code> <code>module</code> <p>The PoincareBall manifold module</p> required <code>in_dim</code> <code>int</code> <p>Dimension of the input space</p> required <code>out_dim</code> <code>int</code> <p>Dimension of the output space</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators for parameter initialization</p> required <code>input_space</code> <code>str</code> <p>Type of the input tensor, either 'tangent' or 'manifold' (default: 'manifold'). Note: This is a static configuration - changing it after initialization requires recompilation.</p> <code>'manifold'</code> Notes <p>JIT Compatibility:     This layer is designed to work with nnx.jit. Configuration parameters (input_space)     are treated as static and will be baked into the compiled function. Changing these values after     JIT compilation will trigger automatic recompilation.</p> References <p>Ganea Octavian, Gary B\u00e9cigneul, and Thomas Hofmann. \"Hyperbolic neural networks.\"     Advances in neural information processing systems 31 (2018).</p> Source code in <code>hyperbolix/nn_layers/poincare_linear.py</code> <pre><code>def __init__(\n    self,\n    manifold_module: Any,\n    in_dim: int,\n    out_dim: int,\n    *,\n    rngs: nnx.Rngs,\n    input_space: str = \"manifold\",\n):\n    if input_space not in [\"tangent\", \"manifold\"]:\n        raise ValueError(f\"input_space must be either 'tangent' or 'manifold', got '{input_space}'\")\n\n    # Static configuration (treated as compile-time constants for JIT)\n    self.manifold = manifold_module\n    self.in_dim = in_dim\n    self.out_dim = out_dim\n    self.input_space = input_space\n\n    # Trainable parameters\n    # Tangent space weight (Euclidean)\n    self.weight = nnx.Param(jax.random.normal(rngs.params(), (out_dim, in_dim)))\n    # Manifold bias (initialized to small random values to avoid gradient issues at origin)\n    # Mark as manifold parameter for Riemannian optimization\n    self.bias = mark_manifold_param(\n        nnx.Param(jax.random.normal(rngs.params(), (1, out_dim)) * 0.01),\n        manifold_type=\"poincare\",\n        curvature=1.0,  # Default curvature, will be overridden by c parameter in forward pass\n    )\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HypLinearPoincare.__call__","title":"__call__","text":"<pre><code>__call__(\n    x: Float[Array, \"batch in_dim\"], c: float = 1.0\n) -&gt; Float[Array, \"batch out_dim\"]\n</code></pre> <p>Forward pass through the hyperbolic linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (batch, in_dim)</code> <p>Input tensor where the hyperbolic_axis is last</p> required <code>c</code> <code>float</code> <p>Manifold curvature (default: 1.0)</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>res</code> <code>Array of shape (batch, out_dim)</code> <p>Output on the Poincar\u00e9 ball manifold</p> Source code in <code>hyperbolix/nn_layers/poincare_linear.py</code> <pre><code>def __call__(\n    self,\n    x: Float[Array, \"batch in_dim\"],\n    c: float = 1.0,\n) -&gt; Float[Array, \"batch out_dim\"]:\n    \"\"\"\n    Forward pass through the hyperbolic linear layer.\n\n    Parameters\n    ----------\n    x : Array of shape (batch, in_dim)\n        Input tensor where the hyperbolic_axis is last\n    c : float\n        Manifold curvature (default: 1.0)\n\n    Returns\n    -------\n    res : Array of shape (batch, out_dim)\n        Output on the Poincar\u00e9 ball manifold\n    \"\"\"\n    # Project bias to manifold (bias is (1, out_dim), squeeze to (out_dim,))\n    bias = self.manifold.proj(self.bias.squeeze(0), c)\n\n    # Map to tangent space if needed (static branch - JIT friendly)\n    if self.input_space == \"manifold\":\n        x = jax.vmap(self.manifold.logmap_0, in_axes=(0, None), out_axes=0)(x, c)\n\n    # Matrix-vector multiplication in tangent space at origin\n    # (batch, in_dim) @ (in_dim, out_dim) -&gt; (batch, out_dim)\n    x = jnp.einsum(\"bi,oi-&gt;bo\", x, self.weight)\n\n    # Map back to manifold\n    x = jax.vmap(self.manifold.expmap_0, in_axes=(0, None), out_axes=0)(x, c)\n\n    # Manifold bias addition (M\u00f6bius addition for Poincar\u00e9)\n    # Broadcast bias to match batch dimension\n    res = jax.vmap(self.manifold.addition, in_axes=(0, None, None), out_axes=0)(x, bias, c)\n    return res\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HypLinearPoincarePP","title":"hyperbolix.nn_layers.HypLinearPoincarePP","text":"<pre><code>HypLinearPoincarePP(\n    manifold_module: Any,\n    in_dim: int,\n    out_dim: int,\n    *,\n    rngs: Rngs,\n    input_space: str = \"manifold\",\n    clamping_factor: float = 1.0,\n    smoothing_factor: float = 50.0,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Hyperbolic Neural Networks ++ fully connected layer (Poincar\u00e9 ball model).</p> <p>Computation steps:     0) Project the input tensor onto the manifold (optional)     1) Compute the multinomial linear regression score(s)     2) Calculate the generalized linear transformation from the regression score(s)</p> <p>Parameters:</p> Name Type Description Default <code>manifold_module</code> <code>module</code> <p>The PoincareBall manifold module</p> required <code>in_dim</code> <code>int</code> <p>Dimension of the input space</p> required <code>out_dim</code> <code>int</code> <p>Dimension of the output space</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators for parameter initialization</p> required <code>input_space</code> <code>str</code> <p>Type of the input tensor, either 'tangent' or 'manifold' (default: 'manifold'). Note: This is a static configuration - changing it after initialization requires recompilation.</p> <code>'manifold'</code> <code>clamping_factor</code> <code>float</code> <p>Clamping factor for the multinomial linear regression output (default: 1.0)</p> <code>1.0</code> <code>smoothing_factor</code> <code>float</code> <p>Smoothing factor for the multinomial linear regression output (default: 50.0)</p> <code>50.0</code> Notes <p>JIT Compatibility:     This layer is designed to work with nnx.jit. Configuration parameters (input_space,     clamping_factor, smoothing_factor) are treated as static and will be baked into the compiled function.</p> References <p>Shimizu Ryohei, Yusuke Mukuta, and Tatsuya Harada. \"Hyperbolic neural networks++.\"     arXiv preprint arXiv:2006.08210 (2020).</p> Source code in <code>hyperbolix/nn_layers/poincare_linear.py</code> <pre><code>def __init__(\n    self,\n    manifold_module: Any,\n    in_dim: int,\n    out_dim: int,\n    *,\n    rngs: nnx.Rngs,\n    input_space: str = \"manifold\",\n    clamping_factor: float = 1.0,\n    smoothing_factor: float = 50.0,\n):\n    if input_space not in [\"tangent\", \"manifold\"]:\n        raise ValueError(f\"input_space must be either 'tangent' or 'manifold', got '{input_space}'\")\n\n    # Static configuration (treated as compile-time constants for JIT)\n    self.manifold = manifold_module\n    self.in_dim = in_dim\n    self.out_dim = out_dim\n    self.input_space = input_space\n    self.clamping_factor = clamping_factor\n    self.smoothing_factor = smoothing_factor\n\n    # Trainable parameters\n    # Tangent space weight\n    self.weight = nnx.Param(jax.random.normal(rngs.params(), (out_dim, in_dim)))\n    # Scalar bias\n    self.bias = nnx.Param(jnp.zeros((out_dim, 1)))\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HypLinearPoincarePP.__call__","title":"__call__","text":"<pre><code>__call__(\n    x: Float[Array, \"batch in_dim\"], c: float = 1.0\n) -&gt; Float[Array, \"batch out_dim\"]\n</code></pre> <p>Forward pass through the HNN++ hyperbolic linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (batch, in_dim)</code> <p>Input tensor where the hyperbolic_axis is last</p> required <code>c</code> <code>float</code> <p>Manifold curvature (default: 1.0)</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>res</code> <code>Array of shape (batch, out_dim)</code> <p>Output on the Poincar\u00e9 ball manifold</p> Source code in <code>hyperbolix/nn_layers/poincare_linear.py</code> <pre><code>def __call__(\n    self,\n    x: Float[Array, \"batch in_dim\"],\n    c: float = 1.0,\n) -&gt; Float[Array, \"batch out_dim\"]:\n    \"\"\"\n    Forward pass through the HNN++ hyperbolic linear layer.\n\n    Parameters\n    ----------\n    x : Array of shape (batch, in_dim)\n        Input tensor where the hyperbolic_axis is last\n    c : float\n        Manifold curvature (default: 1.0)\n\n    Returns\n    -------\n    res : Array of shape (batch, out_dim)\n        Output on the Poincar\u00e9 ball manifold\n    \"\"\"\n    # Map to manifold if needed (static branch - JIT friendly)\n    if self.input_space == \"tangent\":\n        x = jax.vmap(self.manifold.expmap_0, in_axes=(0, None), out_axes=0)(x, c)\n\n    # Compute multinomial linear regression\n    v = compute_mlr_poincare_pp(\n        x,\n        self.weight[...],\n        self.bias[...],\n        c,\n        self.clamping_factor,\n        self.smoothing_factor,\n    )\n\n    # Generalized linear transformation\n    sqrt_c = jnp.sqrt(c)\n    w = sinh(sqrt_c * v) / sqrt_c  # (batch, out_dim)\n    w2 = jnp.sum(w**2, axis=-1, keepdims=True)  # (batch, 1)\n    denom = 1 + jnp.sqrt(1 + c * w2)  # (batch, 1)\n    res = w / denom  # (batch, out_dim)\n\n    # Project results to the manifold\n    res = jax.vmap(self.manifold.proj, in_axes=(0, None), out_axes=0)(res, c)\n\n    return res\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperboloid-linear","title":"Hyperboloid Linear","text":""},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HypLinearHyperboloidFHCNN","title":"hyperbolix.nn_layers.HypLinearHyperboloidFHCNN","text":"<pre><code>HypLinearHyperboloidFHCNN(\n    manifold_module: Any,\n    in_dim: int,\n    out_dim: int,\n    *,\n    rngs: Rngs,\n    input_space: str = \"manifold\",\n    init_scale: float = 2.3,\n    learnable_scale: bool = False,\n    eps: float = 1e-05,\n    activation: Callable[[Array], Array] | None = None,\n    normalize: bool = False,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Fully Hyperbolic Convolutional Neural Networks fully connected layer (Hyperboloid model).</p> <p>Computation steps:     0) Project the input tensor to the manifold (optional)     1) Apply activation (optional)     2) a) If normalize is True, compute the time and space coordinates of the output by applying a scaled sigmoid           of the weight and biases transformed coordinates of the input or the result of the previous step.        b) If normalize is False, compute the weight and biases transformed space coordinates of the input or the           result of the previous step and set the time coordinate such that the result lies on the manifold.</p> <p>Parameters:</p> Name Type Description Default <code>manifold_module</code> <code>module</code> <p>The Hyperboloid manifold module</p> required <code>in_dim</code> <code>int</code> <p>Dimension of the input space</p> required <code>out_dim</code> <code>int</code> <p>Dimension of the output space</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators for parameter initialization</p> required <code>input_space</code> <code>str</code> <p>Type of the input tensor, either 'tangent' or 'manifold' (default: 'manifold'). Note: This is a static configuration - changing it after initialization requires recompilation.</p> <code>'manifold'</code> <code>init_scale</code> <code>float</code> <p>Initial value for the sigmoid scale parameter (default: 2.3)</p> <code>2.3</code> <code>learnable_scale</code> <code>bool</code> <p>Whether the scale parameter should be learnable (default: False)</p> <code>False</code> <code>eps</code> <code>float</code> <p>Small value to ensure that the time coordinate is bigger than 1/sqrt(c) (default: 1e-5)</p> <code>1e-05</code> <code>activation</code> <code>callable or None</code> <p>Activation function to apply before the linear transformation (default: None). Note: This is a static configuration - changing it after initialization requires recompilation.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the space coordinates before rescaling (default: False). Note: This is a static configuration - changing it after initialization requires recompilation.</p> <code>False</code> Notes <p>JIT Compatibility:     This layer is designed to work with nnx.jit. Configuration parameters (input_space, activation,     normalize) are treated as static and will be baked into the compiled function.</p> <p>Relationship to HTC/HRC:     When <code>normalize=False</code> and <code>c_in = c_out</code>, this layer uses the same time reconstruction     pattern as <code>htc</code>: <code>time = sqrt(||space||^2 + 1/c)</code>. The key difference is that FHCNN     applies a linear transform to the full input and discards the computed time, while <code>htc</code>     uses the linear output directly as spatial components. When <code>normalize=True</code>, FHCNN uses     a learned sigmoid scaling which differs from both htc and hrc.</p> See Also <p>htc : Hyperbolic Transformation Component with curvature change support.     Similar time reconstruction pattern when normalize=False. HTCLinear : Module wrapper for htc with learnable linear transformation.</p> References <p>Ahmad Bdeir, Kristian Schwethelm, and Niels Landwehr. \"Fully hyperbolic convolutional neural networks for computer vision.\"     arXiv preprint arXiv:2303.15919 (2023).</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_linear.py</code> <pre><code>def __init__(\n    self,\n    manifold_module: Any,\n    in_dim: int,\n    out_dim: int,\n    *,\n    rngs: nnx.Rngs,\n    input_space: str = \"manifold\",\n    init_scale: float = 2.3,\n    learnable_scale: bool = False,\n    eps: float = 1e-5,\n    activation: Callable[[Array], Array] | None = None,\n    normalize: bool = False,\n):\n    if input_space not in [\"tangent\", \"manifold\"]:\n        raise ValueError(f\"input_space must be either 'tangent' or 'manifold', got '{input_space}'\")\n\n    # Static configuration (treated as compile-time constants for JIT)\n    self.manifold = manifold_module\n    self.in_dim = in_dim\n    self.out_dim = out_dim\n    self.input_space = input_space\n    self.eps = eps\n    self.activation = activation\n    self.normalize = normalize\n\n    # Trainable parameters\n    bound = 0.02\n    weight_init = jax.random.uniform(rngs.params(), (out_dim, in_dim), minval=-bound, maxval=bound)\n    self.weight = nnx.Param(weight_init)\n    self.bias = nnx.Param(jnp.zeros((1, out_dim)))\n\n    # Scale parameter for sigmoid\n    if learnable_scale:\n        self.scale = nnx.Param(jnp.array(init_scale))\n    else:\n        # For non-learnable scale, store as regular Python float (static)\n        self.scale = init_scale\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HypLinearHyperboloidFHCNN.__call__","title":"__call__","text":"<pre><code>__call__(\n    x: Float[Array, \"batch in_dim\"], c: float = 1.0\n) -&gt; Float[Array, \"batch out_dim\"]\n</code></pre> <p>Forward pass through the FHCNN hyperbolic linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (batch, in_dim)</code> <p>Input tensor where the hyperbolic_axis is last. x.shape[-1] must equal self.in_dim.</p> required <code>c</code> <code>float</code> <p>Manifold curvature (default: 1.0)</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>res</code> <code>Array of shape (batch, out_dim)</code> <p>Output on the Hyperboloid manifold</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_linear.py</code> <pre><code>def __call__(\n    self,\n    x: Float[Array, \"batch in_dim\"],\n    c: float = 1.0,\n) -&gt; Float[Array, \"batch out_dim\"]:\n    \"\"\"\n    Forward pass through the FHCNN hyperbolic linear layer.\n\n    Parameters\n    ----------\n    x : Array of shape (batch, in_dim)\n        Input tensor where the hyperbolic_axis is last. x.shape[-1] must equal self.in_dim.\n    c : float\n        Manifold curvature (default: 1.0)\n\n    Returns\n    -------\n    res : Array of shape (batch, out_dim)\n        Output on the Hyperboloid manifold\n    \"\"\"\n    # Map to manifold if needed (static branch - JIT friendly)\n    if self.input_space == \"tangent\":\n        x = jax.vmap(self.manifold.expmap_0, in_axes=(0, None), out_axes=0)(x, c)\n\n    # Apply activation if provided (static branch - JIT friendly)\n    if self.activation is not None:\n        x = self.activation(x)\n\n    # Linear transformation\n    x = jnp.einsum(\"bi,oi-&gt;bo\", x, self.weight) + self.bias  # (batch, out_dim)\n\n    # Extract time and space coordinates\n    x0 = x[:, 0:1]  # (batch, 1)\n    x_rem = x[:, 1:]  # (batch, out_dim - 1)\n\n    # Static branch - JIT friendly\n    if self.normalize:\n        # Normalize space coordinates\n        x_rem_norm = jnp.linalg.norm(x_rem, ord=2, axis=-1, keepdims=True)  # (batch, 1)\n\n        # Ensure scale is positive (handle both learnable and non-learnable scale)\n        scale_val = self.scale[...] if isinstance(self.scale, nnx.Param) else self.scale\n        scale = jnp.exp(scale_val) * jax.nn.sigmoid(x0)  # (batch, 1)\n\n        # Compute time coordinate\n        res0 = jnp.sqrt(scale**2 + 1 / c + self.eps)  # (batch, 1)\n\n        # Compute normalized space coordinates\n        res_rem = scale * x_rem / x_rem_norm  # (batch, out_dim - 1)\n\n        res = jnp.concatenate([res0, res_rem], axis=-1)  # (batch, out_dim)\n\n        # Cast vectors with small space norm to the origin\n        # Create origin point\n        origin_time = jnp.sqrt(1 / c) * jnp.ones_like(res0)\n        origin_space = jnp.zeros_like(res_rem)\n        origin = jnp.concatenate([origin_time, origin_space], axis=-1)\n\n        # Apply mask where x_rem_norm is very small (data-dependent control flow - JIT compatible with jnp.where)\n        mask = x_rem_norm &lt;= 1e-5\n        res = jnp.where(mask, origin, res)\n    else:\n        # Compute the time component from the space component and concatenate\n        res0 = jnp.sqrt(jnp.sum(x_rem**2, axis=-1, keepdims=True) + 1 / c)  # (batch, 1)\n        res = jnp.concatenate([res0, x_rem], axis=-1)  # (batch, out_dim)\n\n    return res\n</code></pre>"},{"location":"api-reference/nn-layers/#usage-example","title":"Usage Example","text":"<pre><code>from flax import nnx\nfrom hyperbolix.nn_layers import HypLinearPoincare\nfrom hyperbolix.manifolds import poincare\nimport jax.numpy as jnp\n\n# Create hyperbolic linear layer\nlayer = HypLinearPoincare(\n    manifold_module=poincare,\n    in_dim=32,\n    out_dim=16,\n    rngs=nnx.Rngs(0)\n)\n\n# Forward pass\nx = jax.random.normal(nnx.Rngs(1).params(), (10, 32)) * 0.3\nx_proj = jax.vmap(poincare.proj, in_axes=(0, None))(x, 1.0)\n\noutput = layer(x_proj, c=1.0)\nprint(output.shape)  # (10, 16)\n</code></pre>"},{"location":"api-reference/nn-layers/#convolutional-layers","title":"Convolutional Layers","text":""},{"location":"api-reference/nn-layers/#hyperboloid-convolutions","title":"Hyperboloid Convolutions","text":""},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HypConv2DHyperboloid","title":"hyperbolix.nn_layers.HypConv2DHyperboloid","text":"<pre><code>HypConv2DHyperboloid(\n    manifold_module: Any,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int | tuple[int, int],\n    *,\n    rngs: Rngs,\n    stride: int | tuple[int, int] = 1,\n    padding: str = \"SAME\",\n    input_space: str = \"manifold\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Hyperbolic 2D Convolutional Layer for Hyperboloid model.</p> <p>This layer implements fully hyperbolic convolution as described in \"Fully Hyperbolic Convolutional Neural Networks for Computer Vision\".</p> <p>Computation steps:     1) Extract receptive field (kernel_size x kernel_size) of hyperbolic points     2) Apply HCat (Lorentz direct concatenation) to combine receptive field points     3) Pass through hyperbolic linear layer (LFC)</p> <p>Parameters:</p> Name Type Description Default <code>manifold_module</code> <code>module</code> <p>The Hyperboloid manifold module</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels</p> required <code>kernel_size</code> <code>int or tuple[int, int]</code> <p>Size of the convolutional kernel</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators for parameter initialization</p> required <code>stride</code> <code>int or tuple[int, int]</code> <p>Stride of the convolution (default: 1)</p> <code>1</code> <code>padding</code> <code>str</code> <p>Padding mode, either 'SAME' or 'VALID' (default: 'SAME')</p> <code>'SAME'</code> <code>input_space</code> <code>str</code> <p>Type of the input tensor, either 'tangent' or 'manifold' (default: 'manifold'). Note: This is a static configuration - changing it after initialization requires recompilation.</p> <code>'manifold'</code> Notes <p>JIT Compatibility:     This layer is designed to work with nnx.jit. Configuration parameters (padding, input_space)     are treated as static and will be baked into the compiled function.</p> References <p>Ahmad Bdeir, Kristian Schwethelm, and Niels Landwehr. \"Fully hyperbolic convolutional neural networks for computer vision.\"     arXiv preprint arXiv:2303.15919 (2023).</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_conv.py</code> <pre><code>def __init__(\n    self,\n    manifold_module: Any,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int | tuple[int, int],\n    *,\n    rngs: nnx.Rngs,\n    stride: int | tuple[int, int] = 1,\n    padding: str = \"SAME\",\n    input_space: str = \"manifold\",\n):\n    if padding not in [\"SAME\", \"VALID\"]:\n        raise ValueError(f\"padding must be either 'SAME' or 'VALID', got '{padding}'\")\n    if input_space not in [\"tangent\", \"manifold\"]:\n        raise ValueError(f\"input_space must be either 'tangent' or 'manifold', got '{input_space}'\")\n\n    # Static configuration\n    self.manifold = manifold_module\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n    self.input_space = input_space\n    self.padding = padding\n\n    # Handle kernel_size as int or tuple\n    if isinstance(kernel_size, int):\n        self.kernel_size = (kernel_size, kernel_size)\n    else:\n        self.kernel_size = kernel_size\n\n    # Handle stride as int or tuple\n    if isinstance(stride, int):\n        self.stride = (stride, stride)\n    else:\n        self.stride = stride\n\n    # Compute dimensions for the linear layer\n    # Receptive field: kernel_h x kernel_w pixels, each is a point in in_channels-dim ambient space\n    # HCat input: N = kernel_h x kernel_w points, each in in_channels ambient dimensions\n    # HCat output: ambient dim = (in_channels - 1) x N + 1\n    #            = (in_channels - 1) x (kernel_h x kernel_w) + 1\n    kernel_h, kernel_w = self.kernel_size\n    N = kernel_h * kernel_w  # Number of points in receptive field\n    d = in_channels - 1  # Input manifold dimension\n    hcat_out_ambient_dim = d * N + 1  # HCat output ambient dimension\n\n    # Create the linear transformation layer\n    # Input: hcat_out_ambient_dim, Output: out_channels\n    self.linear = HypLinearHyperboloidFHCNN(\n        manifold_module=manifold_module,\n        in_dim=hcat_out_ambient_dim,\n        out_dim=out_channels,\n        rngs=rngs,\n        input_space=\"manifold\",  # HCat output is always on manifold\n        learnable_scale=False,\n        normalize=False,\n    )\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HypConv2DHyperboloid.__call__","title":"__call__","text":"<pre><code>__call__(\n    x: Float[Array, \"batch height width in_channels\"],\n    c: float = 1.0,\n) -&gt; Float[\n    Array, \"batch out_height out_width out_channels\"\n]\n</code></pre> <p>Forward pass through the hyperbolic convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (batch, height, width, in_channels)</code> <p>Input feature map where each pixel is a point on the Hyperboloid manifold</p> required <code>c</code> <code>float</code> <p>Manifold curvature (default: 1.0)</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Array of shape (batch, out_height, out_width, out_channels)</code> <p>Output feature map on the Hyperboloid manifold</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_conv.py</code> <pre><code>def __call__(\n    self,\n    x: Float[Array, \"batch height width in_channels\"],\n    c: float = 1.0,\n) -&gt; Float[Array, \"batch out_height out_width out_channels\"]:\n    \"\"\"\n    Forward pass through the hyperbolic convolutional layer.\n\n    Parameters\n    ----------\n    x : Array of shape (batch, height, width, in_channels)\n        Input feature map where each pixel is a point on the Hyperboloid manifold\n    c : float\n        Manifold curvature (default: 1.0)\n\n    Returns\n    -------\n    out : Array of shape (batch, out_height, out_width, out_channels)\n        Output feature map on the Hyperboloid manifold\n    \"\"\"\n    # Map to manifold if needed (static branch - JIT friendly)\n    if self.input_space == \"tangent\":\n        # 1. Flatten batch and spatial dims: (B*H*W, C)\n        x_flat = x.reshape(-1, x.shape[-1])\n\n        # 2. Apply single vmap over the list of vectors\n        x_mapped = jax.vmap(self.manifold.expmap_0, in_axes=(0, None))(x_flat, c)\n\n        # 3. Reshape back to (B, H, W, C)\n        x = x_mapped.reshape(x.shape)\n\n    # Extract patches (receptive fields)\n    # Shape: (batch, out_h, out_w, kernel_h, kernel_w, in_channels)\n    patches = self._extract_patches(x)\n\n    batch, out_h, out_w, kh, kw, in_c = patches.shape\n\n    # Flatten batch and spatial dims for parallel processing\n    # Shape: (batch * out_h * out_w, kh * kw, in_channels)\n    patches_flat = patches.reshape(-1, kh * kw, in_c)\n\n    # 1. Apply HCat\n    # vmap over the flattened spatial/batch dimension\n    # Input to hcat: (N_points, in_channels) -&gt; Output: (hcat_dim,)\n    hcat_out = jax.vmap(self.manifold.hcat, in_axes=(0, None))(patches_flat, c)  # (batch*out_h*out_w, hcat_dim)\n\n    # 2. Apply Linear\n    # Input: (hcat_dim,) -&gt; Output: (out_channels,)\n    linear_out = self.linear(hcat_out, c)\n\n    # 3. Reshape back\n    output = linear_out.reshape(batch, out_h, out_w, self.out_channels)\n\n    return output\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HypConv3DHyperboloid","title":"hyperbolix.nn_layers.HypConv3DHyperboloid","text":"<pre><code>HypConv3DHyperboloid(\n    manifold_module: Any,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int | tuple[int, int, int],\n    *,\n    rngs: Rngs,\n    stride: int | tuple[int, int, int] = 1,\n    padding: str = \"SAME\",\n    input_space: str = \"manifold\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Hyperbolic 3D Convolutional Layer for Hyperboloid model.</p> <p>This layer implements fully hyperbolic 3D convolution, extending the 2D approach from \"Fully Hyperbolic Convolutional Neural Networks for Computer Vision\" to volumetric data.</p> <p>Computation steps:     1) Extract receptive field (kernel_size x kernel_size x kernel_size) of hyperbolic points     2) Apply HCat (Lorentz direct concatenation) to combine receptive field points     3) Pass through hyperbolic linear layer (LFC)</p> <p>Parameters:</p> Name Type Description Default <code>manifold_module</code> <code>module</code> <p>The Hyperboloid manifold module</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels</p> required <code>kernel_size</code> <code>int or tuple[int, int, int]</code> <p>Size of the convolutional kernel (depth, height, width)</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators for parameter initialization</p> required <code>stride</code> <code>int or tuple[int, int, int]</code> <p>Stride of the convolution (default: 1)</p> <code>1</code> <code>padding</code> <code>str</code> <p>Padding mode, either 'SAME' or 'VALID' (default: 'SAME')</p> <code>'SAME'</code> <code>input_space</code> <code>str</code> <p>Type of the input tensor, either 'tangent' or 'manifold' (default: 'manifold'). Note: This is a static configuration - changing it after initialization requires recompilation.</p> <code>'manifold'</code> Notes <p>JIT Compatibility:     This layer is designed to work with nnx.jit. Configuration parameters (padding, input_space)     are treated as static and will be baked into the compiled function.</p> References <p>Ahmad Bdeir, Kristian Schwethelm, and Niels Landwehr. \"Fully hyperbolic convolutional neural networks for computer vision.\"     arXiv preprint arXiv:2303.15919 (2023).</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_conv.py</code> <pre><code>def __init__(\n    self,\n    manifold_module: Any,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int | tuple[int, int, int],\n    *,\n    rngs: nnx.Rngs,\n    stride: int | tuple[int, int, int] = 1,\n    padding: str = \"SAME\",\n    input_space: str = \"manifold\",\n):\n    if padding not in [\"SAME\", \"VALID\"]:\n        raise ValueError(f\"padding must be either 'SAME' or 'VALID', got '{padding}'\")\n    if input_space not in [\"tangent\", \"manifold\"]:\n        raise ValueError(f\"input_space must be either 'tangent' or 'manifold', got '{input_space}'\")\n\n    # Static configuration\n    self.manifold = manifold_module\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n    self.input_space = input_space\n    self.padding = padding\n\n    # Handle kernel_size as int or tuple\n    if isinstance(kernel_size, int):\n        self.kernel_size = (kernel_size, kernel_size, kernel_size)\n    else:\n        self.kernel_size = kernel_size\n\n    # Handle stride as int or tuple\n    if isinstance(stride, int):\n        self.stride = (stride, stride, stride)\n    else:\n        self.stride = stride\n\n    # Compute dimensions for the linear layer\n    # Receptive field: kernel_d x kernel_h x kernel_w voxels, each is a point in in_channels-dim ambient space\n    # HCat input: N = kernel_d x kernel_h x kernel_w points, each in in_channels ambient dimensions\n    # HCat output: ambient dim = (in_channels - 1) x N + 1\n    #            = (in_channels - 1) x (kernel_d x kernel_h x kernel_w) + 1\n    kernel_d, kernel_h, kernel_w = self.kernel_size\n    N = kernel_d * kernel_h * kernel_w  # Number of points in receptive field\n    d = in_channels - 1  # Input manifold dimension\n    hcat_out_ambient_dim = d * N + 1  # HCat output ambient dimension\n\n    # Create the linear transformation layer\n    # Input: hcat_out_ambient_dim, Output: out_channels\n    self.linear = HypLinearHyperboloidFHCNN(\n        manifold_module=manifold_module,\n        in_dim=hcat_out_ambient_dim,\n        out_dim=out_channels,\n        rngs=rngs,\n        input_space=\"manifold\",  # HCat output is always on manifold\n        learnable_scale=False,\n        normalize=False,\n    )\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HypConv3DHyperboloid.__call__","title":"__call__","text":"<pre><code>__call__(\n    x: Float[Array, \"batch depth height width in_channels\"],\n    c: float = 1.0,\n) -&gt; Float[\n    Array,\n    \"batch out_depth out_height out_width out_channels\",\n]\n</code></pre> <p>Forward pass through the hyperbolic 3D convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (batch, depth, height, width, in_channels)</code> <p>Input feature map where each voxel is a point on the Hyperboloid manifold</p> required <code>c</code> <code>float</code> <p>Manifold curvature (default: 1.0)</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Array of shape (batch, out_depth, out_height, out_width, out_channels)</code> <p>Output feature map on the Hyperboloid manifold</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_conv.py</code> <pre><code>def __call__(\n    self,\n    x: Float[Array, \"batch depth height width in_channels\"],\n    c: float = 1.0,\n) -&gt; Float[Array, \"batch out_depth out_height out_width out_channels\"]:\n    \"\"\"\n    Forward pass through the hyperbolic 3D convolutional layer.\n\n    Parameters\n    ----------\n    x : Array of shape (batch, depth, height, width, in_channels)\n        Input feature map where each voxel is a point on the Hyperboloid manifold\n    c : float\n        Manifold curvature (default: 1.0)\n\n    Returns\n    -------\n    out : Array of shape (batch, out_depth, out_height, out_width, out_channels)\n        Output feature map on the Hyperboloid manifold\n    \"\"\"\n    # Map to manifold if needed (static branch - JIT friendly)\n    if self.input_space == \"tangent\":\n        # 1. Flatten batch and spatial dims: (B*D*H*W, C)\n        x_flat = x.reshape(-1, x.shape[-1])\n\n        # 2. Apply single vmap over the list of vectors\n        x_mapped = jax.vmap(self.manifold.expmap_0, in_axes=(0, None))(x_flat, c)\n\n        # 3. Reshape back to (B, D, H, W, C)\n        x = x_mapped.reshape(x.shape)\n\n    # Extract patches (receptive fields)\n    # Shape: (batch, out_d, out_h, out_w, kernel_d, kernel_h, kernel_w, in_channels)\n    patches = self._extract_patches(x)\n\n    batch, out_d, out_h, out_w, kd, kh, kw, in_c = patches.shape\n\n    # Flatten batch and spatial dims for parallel processing\n    # Shape: (batch * out_d * out_h * out_w, kd * kh * kw, in_channels)\n    patches_flat = patches.reshape(-1, kd * kh * kw, in_c)\n\n    # 1. Apply HCat\n    # vmap over the flattened spatial/batch dimension\n    hcat_out = jax.vmap(self.manifold.hcat, in_axes=(0, None))(patches_flat, c)  # (batch*out_d*out_h*out_w, hcat_dim)\n\n    # 2. Apply Linear\n    linear_out = self.linear(hcat_out, c)\n\n    # 3. Reshape back\n    output = linear_out.reshape(batch, out_d, out_h, out_w, self.out_channels)\n\n    return output\n</code></pre>"},{"location":"api-reference/nn-layers/#usage-example_1","title":"Usage Example","text":"<pre><code>from hyperbolix.nn_layers import HypConv2DHyperboloid\nfrom hyperbolix.manifolds import hyperboloid\nfrom flax import nnx\nimport jax.numpy as jnp\n\n# Create 2D hyperbolic convolution\nconv = HypConv2DHyperboloid(\n    manifold_module=hyperboloid,\n    out_channels=32,\n    kernel_size=(3, 3),\n    stride=(1, 1),\n    rngs=nnx.Rngs(0)\n)\n\n# Input: (batch, height, width, in_channels)\nx = jax.random.normal(nnx.Rngs(1).params(), (8, 28, 28, 16))\n\n# Project to hyperboloid\nx_ambient = jnp.concatenate([\n    jnp.sqrt(jnp.sum(x**2, axis=-1, keepdims=True) + 1.0),\n    x\n], axis=-1)\n\n# Forward pass\noutput = conv(x_ambient, c=1.0, use_tangent_input=False)\nprint(output.shape)  # (8, 28, 28, 32\u00d79+1) - dimension grows!\n</code></pre> <p>Dimensional Growth</p> <p>Hyperboloid convolutions increase dimensionality via HCat operation:</p> <ul> <li>Input: <code>d+1</code> dimensions</li> <li>Output: <code>(d\u00d7N)+1</code> dimensions where <code>N = kernel_height \u00d7 kernel_width</code></li> </ul> <p>For 3\u00d73 kernel: 3D input \u2192 28D output. Use small kernels or add dimensionality reduction layers.</p>"},{"location":"api-reference/nn-layers/#lorentzconv2d-hrc-based","title":"LorentzConv2D (HRC-Based)","text":"<p>LorentzConv2D provides a simpler, more efficient alternative to HCat-based convolutions by using the Hyperbolic Regularization Component (HRC) pattern from the Hypformer paper.</p> <p>Key Differences from HypConv2DHyperboloid:</p> Feature HypConv2DHyperboloid (HCat) LorentzConv2D (HRC) Method HCat concatenation + linear Euclidean conv on space components Dimension Grows: <code>(d-1)\u00d7N+1</code> Preserved Speed Slower (~80s/epoch) 2.5x faster (~32s/epoch) Accuracy Higher (~71% on MNIST) Lower (~46% on MNIST) Use Case Maximum accuracy Speed/memory efficiency <p>Theoretical Connection:</p> <p>LorentzConv2D implements the Hyperbolic Layer (HL) pattern from LResNet, which is mathematically equivalent to the Hyperbolic Regularization Component (HRC) from Hypformer:</p> <pre><code># Both approaches:\n# 1. Extract space components: x_s = x[..., 1:]\n# 2. Apply Euclidean function: y_s = f(x_s)\n# 3. Reconstruct time: y_t = sqrt(||y_s||^2 + 1/c)\n</code></pre> <p>Usage Example:</p> <pre><code>from hyperbolix.nn_layers import LorentzConv2D\nfrom flax import nnx\nimport jax.numpy as jnp\n\n# Create efficient hyperbolic convolution\nconv = LorentzConv2D(\n    in_channels=33,    # Including time component\n    out_channels=65,   # Including time component\n    kernel_size=3,\n    stride=2,\n    padding=\"SAME\",\n    rngs=nnx.Rngs(0)\n)\n\n# Input: points on Lorentz manifold (batch, height, width, in_channels)\nx = jnp.ones((8, 28, 28, 33))\nx_space = x[..., 1:]\nx_time = jnp.sqrt(jnp.sum(x_space**2, axis=-1, keepdims=True) + 1.0)\nx = jnp.concatenate([x_time, x_space], axis=-1)\n\n# Forward pass\noutput = conv(x, c=1.0)\nprint(output.shape)  # (8, 14, 14, 65) - dimensions preserved!\n</code></pre> <p>When to Use LorentzConv2D</p> <p>Choose LorentzConv2D when:</p> <ul> <li>Speed and memory efficiency are priorities</li> <li>Working with resource-constrained environments</li> <li>Acceptable accuracy trade-off for 2.5x speedup</li> </ul> <p>Choose HypConv2DHyperboloid when:</p> <ul> <li>Maximum accuracy is required</li> <li>Willing to accept slower training and dimensional growth</li> </ul>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.LorentzConv2D","title":"hyperbolix.nn_layers.LorentzConv2D","text":"<pre><code>LorentzConv2D(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int | tuple[int, int],\n    *,\n    rngs: Rngs,\n    stride: int | tuple[int, int] = 1,\n    padding: str = \"SAME\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Lorentz 2D Convolutional Layer using the Hyperbolic Layer (HL) approach.</p> <p>This layer applies convolution to the space-like components of Lorentzian vectors and reconstructs the time-like component to maintain the manifold constraint. This is equivalent to an HRC (Hyperbolic Regularization Component) wrapper around a standard Conv2D.</p> <p>Computation steps:     1) Extract space-like components x_s from input x = [x_t, x_s]^T     2) Apply Euclidean convolution: y_s = Conv2D(x_s)     3) Reconstruct time component: y_t = sqrt(||y_s||^2 + 1/c)     4) Return y = [y_t, y_s]^T</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels (ambient dimension, including time component)</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels (ambient dimension, including time component)</p> required <code>kernel_size</code> <code>int or tuple[int, int]</code> <p>Size of the convolutional kernel</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators for parameter initialization</p> required <code>stride</code> <code>int or tuple[int, int]</code> <p>Stride of the convolution (default: 1)</p> <code>1</code> <code>padding</code> <code>str or int or tuple</code> <p>Padding mode: 'SAME', 'VALID', or explicit padding (default: 'SAME')</p> <code>'SAME'</code> Notes <p>This implementation follows the Hyperbolic Layer (HL) approach from \"Fully Hyperbolic Convolutional Neural Networks for Computer Vision\".</p> <p>The layer operates only on space-like components, making it more computationally efficient than the HCat-based approach (HypConv2DHyperboloid), though it doesn't perform true hyperbolic convolution. Instead, it applies Euclidean operations to spatial components and reconstructs the time component.</p> See Also <p>hypformer.hrc : Core HRC function this layer is based on HypConv2DHyperboloid : Full hyperbolic convolution using HCat concatenation</p> References <p>He, Neil, Menglin Yang, and Rex Ying. \"Lorentzian residual neural networks.\" Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 1. 2025.</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int | tuple[int, int],\n    *,\n    rngs: nnx.Rngs,\n    stride: int | tuple[int, int] = 1,\n    padding: str = \"SAME\",\n):\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n\n    # Create Euclidean conv layer for space components only\n    # in_channels - 1: skip time component at index 0\n    # out_channels - 1: time will be reconstructed from constraint\n    self.conv = nnx.Conv(\n        in_features=in_channels - 1,\n        out_features=out_channels - 1,\n        kernel_size=kernel_size,\n        strides=stride,\n        padding=padding,\n        rngs=rngs,\n    )\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.LorentzConv2D.__call__","title":"__call__","text":"<pre><code>__call__(\n    x: Float[Array, \"batch height width in_channels\"],\n    c: float = 1.0,\n) -&gt; Float[\n    Array, \"batch out_height out_width out_channels\"\n]\n</code></pre> <p>Forward pass through the Lorentz convolutional layer.</p> <p>This layer is a specific instance of the Hyperbolic Regularization Component (HRC) where the regularization function f_r is a 2D convolution. The HRC pattern: 1. Extracts space components 2. Applies Euclidean convolution 3. Reconstructs time component using Lorentz constraint</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (batch, height, width, in_channels)</code> <p>Input feature map where x[..., 0] is time component and x[..., 1:] are space components on the Lorentz manifold</p> required <code>c</code> <code>float</code> <p>Manifold curvature parameter (default: 1.0)</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Array of shape (batch, out_height, out_width, out_channels)</code> <p>Output feature map on the Lorentz manifold</p> Notes <p>This implementation uses the HRC function from hypformer.py, demonstrating that LorentzConv2D (from LResNet) and HRC (from Hypformer) are mathematically equivalent approaches to adapting Euclidean operations for hyperbolic geometry.</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_conv.py</code> <pre><code>def __call__(\n    self,\n    x: Float[Array, \"batch height width in_channels\"],\n    c: float = 1.0,\n) -&gt; Float[Array, \"batch out_height out_width out_channels\"]:\n    \"\"\"\n    Forward pass through the Lorentz convolutional layer.\n\n    This layer is a specific instance of the Hyperbolic Regularization Component (HRC)\n    where the regularization function f_r is a 2D convolution. The HRC pattern:\n    1. Extracts space components\n    2. Applies Euclidean convolution\n    3. Reconstructs time component using Lorentz constraint\n\n    Parameters\n    ----------\n    x : Array of shape (batch, height, width, in_channels)\n        Input feature map where x[..., 0] is time component and\n        x[..., 1:] are space components on the Lorentz manifold\n    c : float\n        Manifold curvature parameter (default: 1.0)\n\n    Returns\n    -------\n    out : Array of shape (batch, out_height, out_width, out_channels)\n        Output feature map on the Lorentz manifold\n\n    Notes\n    -----\n    This implementation uses the HRC function from hypformer.py, demonstrating that\n    LorentzConv2D (from LResNet) and HRC (from Hypformer) are mathematically equivalent\n    approaches to adapting Euclidean operations for hyperbolic geometry.\n    \"\"\"\n\n    # Define convolution as the HRC regularization function f_r\n    def conv_fn(x_space):\n        return self.conv(x_space)\n\n    # Apply HRC with curvature-preserving transformation (c_in = c_out = c)\n    return hrc(x, conv_fn, c_in=c, c_out=c, eps=1e-8)\n</code></pre>"},{"location":"api-reference/nn-layers/#hypformer-components","title":"Hypformer Components","text":"<p>The Hyperbolic Transformation Component (HTC) and Hyperbolic Regularization Component (HRC) from the Hypformer paper provide general-purpose wrappers for adapting Euclidean operations to hyperbolic geometry with curvature-change support.</p>"},{"location":"api-reference/nn-layers/#core-functions","title":"Core Functions","text":""},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.hrc","title":"hyperbolix.nn_layers.hrc","text":"<pre><code>hrc(\n    x: Float[Array, \"... dim_plus_1\"],\n    f_r: Callable[[Float[Array, ...]], Float[Array, ...]],\n    c_in: float,\n    c_out: float,\n    eps: float = 1e-07,\n) -&gt; Float[Array, \"... out_dim_plus_1\"]\n</code></pre> <p>Hyperbolic Regularization Component.</p> <p>Applies a Euclidean regularization/activation function f_r to the spatial components of hyperboloid points, then maps the result to the hyperboloid with curvature c_out.</p> <p>Mathematical formula:     space = sqrt(c_in/c_out) * f_r(x[..., 1:])     time  = sqrt(||space||^2 + 1/c_out)     output = [time, space]</p> <p>When c_in = c_out = c, this reduces to:     output = [sqrt(||f_r(x_s)||^2 + 1/c), f_r(x_s)] which is the pattern used by curvature-preserving hyperboloid activations.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (..., dim+1)</code> <p>Input point(s) on the hyperboloid manifold with curvature c_in. The first element is the time-like component, remaining are spatial.</p> required <code>f_r</code> <code>Callable</code> <p>Euclidean function to apply to spatial components. Can be any activation, normalization, dropout, etc. Takes spatial components and returns transformed spatial components (may change dimension).</p> required <code>c_in</code> <code>float</code> <p>Input curvature parameter (must be positive, c &gt; 0).</p> required <code>c_out</code> <code>float</code> <p>Output curvature parameter (must be positive, c &gt; 0).</p> required <code>eps</code> <code>float</code> <p>Small value for numerical stability (default: 1e-7).</p> <code>1e-07</code> <p>Returns:</p> Name Type Description <code>y</code> <code>Array of shape (..., out_dim+1)</code> <p>Output point(s) on the hyperboloid manifold with curvature c_out.</p> Notes <ul> <li>f_r operates only on spatial components x[..., 1:], not the time component</li> <li>The time component is reconstructed using the hyperboloid constraint:   -x\u2080\u00b2 + ||x_rest||\u00b2 = -1/c_out</li> <li>This avoids expensive exp/log maps while maintaining mathematical correctness</li> <li>The spatial scaling factor sqrt(c_in/c_out) ensures proper curvature transformation</li> </ul> See Also <p>htc : Hyperbolic Transformation Component for full-point operations.</p> References <p>Hypformer paper (citation to be added)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from hyperbolix.nn_layers.hyperboloid_core import hrc\n&gt;&gt;&gt; from hyperbolix.manifolds import hyperboloid\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a point on the hyperboloid\n&gt;&gt;&gt; x = jnp.array([1.05, 0.1, -0.2, 0.15])\n&gt;&gt;&gt; x = hyperboloid.proj(x, c=1.0)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Apply HRC with ReLU (curvature-preserving)\n&gt;&gt;&gt; y = hrc(x, jax.nn.relu, c_in=1.0, c_out=1.0)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Apply HRC with curvature change\n&gt;&gt;&gt; y = hrc(x, jax.nn.relu, c_in=1.0, c_out=2.0)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Custom activation\n&gt;&gt;&gt; def custom_act(z):\n...     return jax.nn.gelu(z) * 0.5\n&gt;&gt;&gt; y = hrc(x, custom_act, c_in=1.0, c_out=0.5)\n</code></pre> Source code in <code>hyperbolix/nn_layers/hyperboloid_core.py</code> <pre><code>def hrc(\n    x: Float[Array, \"... dim_plus_1\"],\n    f_r: Callable[[Float[Array, \"...\"]], Float[Array, \"...\"]],\n    c_in: float,\n    c_out: float,\n    eps: float = 1e-7,\n) -&gt; Float[Array, \"... out_dim_plus_1\"]:\n    \"\"\"Hyperbolic Regularization Component.\n\n    Applies a Euclidean regularization/activation function f_r to the spatial\n    components of hyperboloid points, then maps the result to the hyperboloid\n    with curvature c_out.\n\n    Mathematical formula:\n        space = sqrt(c_in/c_out) * f_r(x[..., 1:])\n        time  = sqrt(||space||^2 + 1/c_out)\n        output = [time, space]\n\n    When c_in = c_out = c, this reduces to:\n        output = [sqrt(||f_r(x_s)||^2 + 1/c), f_r(x_s)]\n    which is the pattern used by curvature-preserving hyperboloid activations.\n\n    Parameters\n    ----------\n    x : Array of shape (..., dim+1)\n        Input point(s) on the hyperboloid manifold with curvature c_in.\n        The first element is the time-like component, remaining are spatial.\n    f_r : Callable\n        Euclidean function to apply to spatial components. Can be any activation,\n        normalization, dropout, etc. Takes spatial components and returns\n        transformed spatial components (may change dimension).\n    c_in : float\n        Input curvature parameter (must be positive, c &gt; 0).\n    c_out : float\n        Output curvature parameter (must be positive, c &gt; 0).\n    eps : float, optional\n        Small value for numerical stability (default: 1e-7).\n\n    Returns\n    -------\n    y : Array of shape (..., out_dim+1)\n        Output point(s) on the hyperboloid manifold with curvature c_out.\n\n    Notes\n    -----\n    - f_r operates only on spatial components x[..., 1:], not the time component\n    - The time component is reconstructed using the hyperboloid constraint:\n      -x\u2080\u00b2 + ||x_rest||\u00b2 = -1/c_out\n    - This avoids expensive exp/log maps while maintaining mathematical correctness\n    - The spatial scaling factor sqrt(c_in/c_out) ensures proper curvature transformation\n\n    See Also\n    --------\n    htc : Hyperbolic Transformation Component for full-point operations.\n\n    References\n    ----------\n    Hypformer paper (citation to be added)\n\n    Examples\n    --------\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt; from hyperbolix.nn_layers.hyperboloid_core import hrc\n    &gt;&gt;&gt; from hyperbolix.manifolds import hyperboloid\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Create a point on the hyperboloid\n    &gt;&gt;&gt; x = jnp.array([1.05, 0.1, -0.2, 0.15])\n    &gt;&gt;&gt; x = hyperboloid.proj(x, c=1.0)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Apply HRC with ReLU (curvature-preserving)\n    &gt;&gt;&gt; y = hrc(x, jax.nn.relu, c_in=1.0, c_out=1.0)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Apply HRC with curvature change\n    &gt;&gt;&gt; y = hrc(x, jax.nn.relu, c_in=1.0, c_out=2.0)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Custom activation\n    &gt;&gt;&gt; def custom_act(z):\n    ...     return jax.nn.gelu(z) * 0.5\n    &gt;&gt;&gt; y = hrc(x, custom_act, c_in=1.0, c_out=0.5)\n    \"\"\"\n    # Extract spatial components\n    x_space = x[..., 1:]\n\n    # Apply Euclidean function to spatial components\n    out_space = f_r(x_space)\n\n    # Scale spatial components for curvature transformation\n    scale = jnp.sqrt(c_in / c_out)\n    scaled_space = scale * out_space\n\n    # Compute norm squared of scaled spatial components\n    norm_sq = jnp.sum(scaled_space**2, axis=-1)\n\n    # Reconstruct time component using hyperboloid constraint\n    # Constraint: -x\u2080\u00b2 + ||x_rest||\u00b2 = -1/c_out\n    # =&gt; x\u2080 = sqrt(||x_rest||\u00b2 + 1/c_out)\n    x0_sq = norm_sq + 1.0 / c_out\n    x0 = jnp.sqrt(jnp.maximum(x0_sq, eps))\n\n    # Concatenate time and spatial components\n    return jnp.concatenate([x0[..., None], scaled_space], axis=-1)\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.htc","title":"hyperbolix.nn_layers.htc","text":"<pre><code>htc(\n    x: Float[Array, \"... in_dim_plus_1\"],\n    f_t: Callable[[Float[Array, ...]], Float[Array, ...]],\n    c_in: float,\n    c_out: float,\n    eps: float = 1e-07,\n) -&gt; Float[Array, \"... out_dim_plus_1\"]\n</code></pre> <p>Hyperbolic Transformation Component.</p> <p>Applies a Euclidean linear transformation f_t to the full hyperboloid point (including time component), then maps the result to the hyperboloid with curvature c_out.</p> <p>Mathematical formula:     space = sqrt(c_in/c_out) * f_t(x)     time  = sqrt(||space||^2 + 1/c_out)     output = [time, space]</p> <p>where f_t takes the full (dim+1)-dimensional input and produces the output spatial components.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (..., in_dim+1)</code> <p>Input point(s) on the hyperboloid manifold with curvature c_in. All components (time and spatial) are passed to f_t.</p> required <code>f_t</code> <code>Callable</code> <p>Euclidean linear transformation applied to the full input. Takes (in_dim+1)-dimensional input and produces out_dim-dimensional output (which becomes the spatial components of the output).</p> required <code>c_in</code> <code>float</code> <p>Input curvature parameter (must be positive, c &gt; 0).</p> required <code>c_out</code> <code>float</code> <p>Output curvature parameter (must be positive, c &gt; 0).</p> required <code>eps</code> <code>float</code> <p>Small value for numerical stability (default: 1e-7).</p> <code>1e-07</code> <p>Returns:</p> Name Type Description <code>y</code> <code>Array of shape (..., out_dim+1)</code> <p>Output point(s) on the hyperboloid manifold with curvature c_out.</p> Notes <ul> <li>Unlike HRC, f_t operates on the full point including the time component</li> <li>f_t's output dimension determines the output spatial dimension</li> <li>This is typically used for learnable linear transformations</li> <li>The spatial scaling factor sqrt(c_in/c_out) ensures proper curvature transformation</li> </ul> See Also <p>hrc : Hyperbolic Regularization Component for spatial-only operations. HTCLinear : Module wrapper for htc with learnable linear transformation.</p> References <p>Hypformer paper (citation to be added)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from hyperbolix.nn_layers.hyperboloid_core import htc\n&gt;&gt;&gt; from hyperbolix.manifolds import hyperboloid\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a point on the hyperboloid\n&gt;&gt;&gt; x = jnp.array([1.05, 0.1, -0.2, 0.15])\n&gt;&gt;&gt; x = hyperboloid.proj(x, c=1.0)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Define a linear transformation\n&gt;&gt;&gt; W = jax.random.normal(jax.random.PRNGKey(0), (3, 4))\n&gt;&gt;&gt; def linear(z):\n...     return z @ W.T\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Apply HTC\n&gt;&gt;&gt; y = htc(x, linear, c_in=1.0, c_out=2.0)\n&gt;&gt;&gt; y.shape\n(4,)  # (3 spatial + 1 time)\n</code></pre> Source code in <code>hyperbolix/nn_layers/hyperboloid_core.py</code> <pre><code>def htc(\n    x: Float[Array, \"... in_dim_plus_1\"],\n    f_t: Callable[[Float[Array, \"...\"]], Float[Array, \"...\"]],\n    c_in: float,\n    c_out: float,\n    eps: float = 1e-7,\n) -&gt; Float[Array, \"... out_dim_plus_1\"]:\n    \"\"\"Hyperbolic Transformation Component.\n\n    Applies a Euclidean linear transformation f_t to the full hyperboloid point\n    (including time component), then maps the result to the hyperboloid with\n    curvature c_out.\n\n    Mathematical formula:\n        space = sqrt(c_in/c_out) * f_t(x)\n        time  = sqrt(||space||^2 + 1/c_out)\n        output = [time, space]\n\n    where f_t takes the full (dim+1)-dimensional input and produces the output\n    spatial components.\n\n    Parameters\n    ----------\n    x : Array of shape (..., in_dim+1)\n        Input point(s) on the hyperboloid manifold with curvature c_in.\n        All components (time and spatial) are passed to f_t.\n    f_t : Callable\n        Euclidean linear transformation applied to the full input. Takes\n        (in_dim+1)-dimensional input and produces out_dim-dimensional output\n        (which becomes the spatial components of the output).\n    c_in : float\n        Input curvature parameter (must be positive, c &gt; 0).\n    c_out : float\n        Output curvature parameter (must be positive, c &gt; 0).\n    eps : float, optional\n        Small value for numerical stability (default: 1e-7).\n\n    Returns\n    -------\n    y : Array of shape (..., out_dim+1)\n        Output point(s) on the hyperboloid manifold with curvature c_out.\n\n    Notes\n    -----\n    - Unlike HRC, f_t operates on the full point including the time component\n    - f_t's output dimension determines the output spatial dimension\n    - This is typically used for learnable linear transformations\n    - The spatial scaling factor sqrt(c_in/c_out) ensures proper curvature transformation\n\n    See Also\n    --------\n    hrc : Hyperbolic Regularization Component for spatial-only operations.\n    HTCLinear : Module wrapper for htc with learnable linear transformation.\n\n    References\n    ----------\n    Hypformer paper (citation to be added)\n\n    Examples\n    --------\n    &gt;&gt;&gt; import jax\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt; from hyperbolix.nn_layers.hyperboloid_core import htc\n    &gt;&gt;&gt; from hyperbolix.manifolds import hyperboloid\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Create a point on the hyperboloid\n    &gt;&gt;&gt; x = jnp.array([1.05, 0.1, -0.2, 0.15])\n    &gt;&gt;&gt; x = hyperboloid.proj(x, c=1.0)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Define a linear transformation\n    &gt;&gt;&gt; W = jax.random.normal(jax.random.PRNGKey(0), (3, 4))\n    &gt;&gt;&gt; def linear(z):\n    ...     return z @ W.T\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Apply HTC\n    &gt;&gt;&gt; y = htc(x, linear, c_in=1.0, c_out=2.0)\n    &gt;&gt;&gt; y.shape\n    (4,)  # (3 spatial + 1 time)\n    \"\"\"\n    # Apply Euclidean transformation to full input\n    # f_t: (in_dim+1,) \u2192 (out_dim,)\n    out = f_t(x)\n\n    # Scale output for curvature transformation\n    scale = jnp.sqrt(c_in / c_out)\n    scaled_out = scale * out\n\n    # Compute norm squared of scaled output\n    norm_sq = jnp.sum(scaled_out**2, axis=-1)\n\n    # Reconstruct time component using hyperboloid constraint\n    x0_sq = norm_sq + 1.0 / c_out\n    x0 = jnp.sqrt(jnp.maximum(x0_sq, eps))\n\n    # Concatenate time and spatial components\n    return jnp.concatenate([x0[..., None], scaled_out], axis=-1)\n</code></pre>"},{"location":"api-reference/nn-layers/#htchrc-modules","title":"HTC/HRC Modules","text":""},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HTCLinear","title":"hyperbolix.nn_layers.HTCLinear","text":"<pre><code>HTCLinear(\n    in_features: int,\n    out_features: int,\n    *,\n    rngs: Rngs,\n    use_bias: bool = True,\n    init_bound: float = 0.02,\n    eps: float = 1e-07,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Hyperbolic Transformation Component with learnable linear transformation.</p> <p>This module wraps a Euclidean linear layer with the HTC operation, enabling learnable transformations between hyperboloid manifolds with different curvatures.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Input feature dimension (full hyperboloid dimension, including time component).</p> required <code>out_features</code> <code>int</code> <p>Output spatial dimension (time component is reconstructed automatically).</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators for parameter initialization.</p> required <code>use_bias</code> <code>bool</code> <p>Whether to include a bias term (default: True).</p> <code>True</code> <code>init_bound</code> <code>float</code> <p>Bound for uniform weight initialization. Weights are initialized from Uniform(-init_bound, init_bound). Small values keep initial outputs close to the hyperboloid origin for stable training (default: 0.02).</p> <code>0.02</code> <code>eps</code> <code>float</code> <p>Small value for numerical stability (default: 1e-7).</p> <code>1e-07</code> <p>Attributes:</p> Name Type Description <code>kernel</code> <code>Param</code> <p>Weight matrix of shape (in_features, out_features).</p> <code>bias</code> <code>Param or None</code> <p>Bias vector of shape (out_features,) if use_bias=True, else None.</p> <code>eps</code> <code>float</code> <p>Numerical stability parameter.</p> Notes <p>Weight Initialization:     This layer uses small uniform initialization U(-0.02, 0.02) by default,     matching the initialization used by FHNN/FHCNN layers. Standard deep learning     initializations (Xavier, Lecun) produce weights that are too large for     hyperbolic operations, causing gradient explosion and training instability.</p> See Also <p>hyperbolix.nn_layers.hyperboloid_core.htc : Core HTC function for functional transformations. HypLinearHyperboloidFHCNN : Alternative hyperbolic linear layer with sigmoid scaling.</p> References <p>Hypformer paper (citation to be added)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flax import nnx\n&gt;&gt;&gt; from hyperbolix.nn_layers import HTCLinear\n&gt;&gt;&gt; from hyperbolix.manifolds import hyperboloid\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create layer\n&gt;&gt;&gt; layer = HTCLinear(in_features=5, out_features=8, rngs=nnx.Rngs(0))\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Forward pass\n&gt;&gt;&gt; x = jnp.ones((32, 5))  # batch of 32 points\n&gt;&gt;&gt; x = jax.vmap(hyperboloid.proj, in_axes=(0, None))(x, 1.0)\n&gt;&gt;&gt; y = layer(x, c_in=1.0, c_out=2.0)\n&gt;&gt;&gt; y.shape\n(32, 9)  # 8 spatial + 1 time\n</code></pre> Source code in <code>hyperbolix/nn_layers/hyperboloid_linear.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    out_features: int,\n    *,\n    rngs: nnx.Rngs,\n    use_bias: bool = True,\n    init_bound: float = 0.02,\n    eps: float = 1e-7,\n):\n    # Small uniform initialization for hyperbolic stability\n    # Standard initializations (Lecun, Xavier) are too large and cause gradient explosion\n    self.kernel = nnx.Param(\n        jax.random.uniform(rngs.params(), (in_features, out_features), minval=-init_bound, maxval=init_bound)\n    )\n    if use_bias:\n        self.bias = nnx.Param(jnp.zeros((out_features,)))\n    else:\n        self.bias = None\n    self.eps = eps\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HTCLinear.__call__","title":"__call__","text":"<pre><code>__call__(\n    x: Float[Array, \"batch in_features\"],\n    c_in: float = 1.0,\n    c_out: float = 1.0,\n) -&gt; Float[Array, \"batch out_features_plus_1\"]\n</code></pre> <p>Apply HTC linear transformation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (batch, in_features)</code> <p>Input points on hyperboloid with curvature c_in.</p> required <code>c_in</code> <code>float</code> <p>Input curvature (default: 1.0).</p> <code>1.0</code> <code>c_out</code> <code>float</code> <p>Output curvature (default: 1.0).</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>y</code> <code>Array of shape (batch, out_features+1)</code> <p>Output points on hyperboloid with curvature c_out.</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_linear.py</code> <pre><code>def __call__(\n    self,\n    x: Float[Array, \"batch in_features\"],\n    c_in: float = 1.0,\n    c_out: float = 1.0,\n) -&gt; Float[Array, \"batch out_features_plus_1\"]:\n    \"\"\"Apply HTC linear transformation.\n\n    Parameters\n    ----------\n    x : Array of shape (batch, in_features)\n        Input points on hyperboloid with curvature c_in.\n    c_in : float, optional\n        Input curvature (default: 1.0).\n    c_out : float, optional\n        Output curvature (default: 1.0).\n\n    Returns\n    -------\n    y : Array of shape (batch, out_features+1)\n        Output points on hyperboloid with curvature c_out.\n    \"\"\"\n\n    def linear_fn(z):\n        out = z @ self.kernel[...]\n        if self.bias is not None:\n            out = out + self.bias[...]\n        return out\n\n    return htc(x, linear_fn, c_in, c_out, self.eps)\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HRCBatchNorm","title":"hyperbolix.nn_layers.HRCBatchNorm","text":"<pre><code>HRCBatchNorm(\n    num_features: int,\n    *,\n    rngs: Rngs,\n    momentum: float = 0.99,\n    epsilon: float = 1e-05,\n    eps: float = 1e-07,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Hyperbolic Regularization Component with batch normalization.</p> <p>Applies batch normalization to spatial components of hyperboloid points, then reconstructs the time component for the output curvature.</p> <p>Parameters:</p> Name Type Description Default <code>num_features</code> <code>int</code> <p>Number of spatial features to normalize.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators for parameter initialization.</p> required <code>momentum</code> <code>float</code> <p>Momentum for running statistics (default: 0.99).</p> <code>0.99</code> <code>epsilon</code> <code>float</code> <p>Small value for numerical stability in batch norm (default: 1e-5).</p> <code>1e-05</code> <code>eps</code> <code>float</code> <p>Small value for numerical stability in HRC (default: 1e-7).</p> <code>1e-07</code> <p>Attributes:</p> Name Type Description <code>bn</code> <code>BatchNorm</code> <p>Flax batch normalization.</p> <code>eps</code> <code>float</code> <p>Numerical stability parameter for HRC.</p> Notes <p>Training vs Evaluation Mode:     During training (use_running_average=False), batch norm computes statistics     from the current batch and updates running averages. During evaluation     (use_running_average=True), it uses the accumulated running statistics.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flax import nnx\n&gt;&gt;&gt; from hyperbolix.nn_layers import HRCBatchNorm\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create batch norm layer\n&gt;&gt;&gt; bn = HRCBatchNorm(num_features=64, rngs=nnx.Rngs(0))\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Training mode\n&gt;&gt;&gt; y_train = bn(x, c_in=1.0, c_out=2.0, use_running_average=False)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Evaluation mode\n&gt;&gt;&gt; y_eval = bn(x, c_in=1.0, c_out=2.0, use_running_average=True)\n</code></pre> Source code in <code>hyperbolix/nn_layers/hyperboloid_regularization.py</code> <pre><code>def __init__(\n    self,\n    num_features: int,\n    *,\n    rngs: nnx.Rngs,\n    momentum: float = 0.99,\n    epsilon: float = 1e-5,\n    eps: float = 1e-7,\n):\n    self.bn = nnx.BatchNorm(\n        num_features,\n        momentum=momentum,\n        epsilon=epsilon,\n        rngs=rngs,\n    )\n    self.eps = eps\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HRCBatchNorm.__call__","title":"__call__","text":"<pre><code>__call__(\n    x: Float[Array, \"batch dim_plus_1\"],\n    c_in: float = 1.0,\n    c_out: float = 1.0,\n    use_running_average: bool | None = None,\n) -&gt; Float[Array, \"batch dim_plus_1\"]\n</code></pre> <p>Apply HRC batch normalization.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (batch, dim+1)</code> <p>Input points on hyperboloid with curvature c_in.</p> required <code>c_in</code> <code>float</code> <p>Input curvature (default: 1.0).</p> <code>1.0</code> <code>c_out</code> <code>float</code> <p>Output curvature (default: 1.0).</p> <code>1.0</code> <code>use_running_average</code> <code>bool or None</code> <p>If True, use running statistics (eval mode). If False, use batch statistics (train mode). If None, use the default set during initialization.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>y</code> <code>Array of shape (batch, dim+1)</code> <p>Output points on hyperboloid with curvature c_out.</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_regularization.py</code> <pre><code>def __call__(\n    self,\n    x: Float[Array, \"batch dim_plus_1\"],\n    c_in: float = 1.0,\n    c_out: float = 1.0,\n    use_running_average: bool | None = None,\n) -&gt; Float[Array, \"batch dim_plus_1\"]:\n    \"\"\"Apply HRC batch normalization.\n\n    Parameters\n    ----------\n    x : Array of shape (batch, dim+1)\n        Input points on hyperboloid with curvature c_in.\n    c_in : float, optional\n        Input curvature (default: 1.0).\n    c_out : float, optional\n        Output curvature (default: 1.0).\n    use_running_average : bool or None, optional\n        If True, use running statistics (eval mode).\n        If False, use batch statistics (train mode).\n        If None, use the default set during initialization.\n\n    Returns\n    -------\n    y : Array of shape (batch, dim+1)\n        Output points on hyperboloid with curvature c_out.\n    \"\"\"\n\n    def bn_fn(z):\n        return self.bn(z, use_running_average=use_running_average)\n\n    return hrc(x, bn_fn, c_in, c_out, self.eps)\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HRCLayerNorm","title":"hyperbolix.nn_layers.HRCLayerNorm","text":"<pre><code>HRCLayerNorm(\n    num_features: int,\n    *,\n    rngs: Rngs,\n    epsilon: float = 1e-05,\n    eps: float = 1e-07,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Hyperbolic Regularization Component with layer normalization.</p> <p>Applies layer normalization to spatial components of hyperboloid points, then reconstructs the time component for the output curvature.</p> <p>Parameters:</p> Name Type Description Default <code>num_features</code> <code>int</code> <p>Number of spatial features to normalize.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators for parameter initialization.</p> required <code>epsilon</code> <code>float</code> <p>Small value for numerical stability in layer norm (default: 1e-5).</p> <code>1e-05</code> <code>eps</code> <code>float</code> <p>Small value for numerical stability in HRC (default: 1e-7).</p> <code>1e-07</code> <p>Attributes:</p> Name Type Description <code>ln</code> <code>LayerNorm</code> <p>Flax layer normalization.</p> <code>eps</code> <code>float</code> <p>Numerical stability parameter for HRC.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flax import nnx\n&gt;&gt;&gt; from hyperbolix.nn_layers import HRCLayerNorm\n&gt;&gt;&gt;\n&gt;&gt;&gt; ln = HRCLayerNorm(num_features=64, rngs=nnx.Rngs(0))\n&gt;&gt;&gt; y = ln(x, c_in=1.0, c_out=2.0)\n</code></pre> Source code in <code>hyperbolix/nn_layers/hyperboloid_regularization.py</code> <pre><code>def __init__(\n    self,\n    num_features: int,\n    *,\n    rngs: nnx.Rngs,\n    epsilon: float = 1e-5,\n    eps: float = 1e-7,\n):\n    self.ln = nnx.LayerNorm(num_features, epsilon=epsilon, rngs=rngs)\n    self.eps = eps\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HRCLayerNorm.__call__","title":"__call__","text":"<pre><code>__call__(\n    x: Float[Array, \"batch dim_plus_1\"],\n    c_in: float = 1.0,\n    c_out: float = 1.0,\n) -&gt; Float[Array, \"batch dim_plus_1\"]\n</code></pre> <p>Apply HRC layer normalization.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (batch, dim+1)</code> <p>Input points on hyperboloid with curvature c_in.</p> required <code>c_in</code> <code>float</code> <p>Input curvature (default: 1.0).</p> <code>1.0</code> <code>c_out</code> <code>float</code> <p>Output curvature (default: 1.0).</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>y</code> <code>Array of shape (batch, dim+1)</code> <p>Output points on hyperboloid with curvature c_out.</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_regularization.py</code> <pre><code>def __call__(\n    self,\n    x: Float[Array, \"batch dim_plus_1\"],\n    c_in: float = 1.0,\n    c_out: float = 1.0,\n) -&gt; Float[Array, \"batch dim_plus_1\"]:\n    \"\"\"Apply HRC layer normalization.\n\n    Parameters\n    ----------\n    x : Array of shape (batch, dim+1)\n        Input points on hyperboloid with curvature c_in.\n    c_in : float, optional\n        Input curvature (default: 1.0).\n    c_out : float, optional\n        Output curvature (default: 1.0).\n\n    Returns\n    -------\n    y : Array of shape (batch, dim+1)\n        Output points on hyperboloid with curvature c_out.\n    \"\"\"\n    return hrc(x, self.ln, c_in, c_out, self.eps)\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HRCDropout","title":"hyperbolix.nn_layers.HRCDropout","text":"<pre><code>HRCDropout(rate: float, *, rngs: Rngs, eps: float = 1e-07)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Hyperbolic Regularization Component with dropout.</p> <p>Applies dropout to spatial components of hyperboloid points, then reconstructs the time component for the output curvature.</p> <p>Parameters:</p> Name Type Description Default <code>rate</code> <code>float</code> <p>Dropout probability (fraction of units to drop).</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators for dropout.</p> required <code>eps</code> <code>float</code> <p>Small value for numerical stability (default: 1e-7).</p> <code>1e-07</code> <p>Attributes:</p> Name Type Description <code>dropout</code> <code>Dropout</code> <p>Flax dropout layer.</p> <code>eps</code> <code>float</code> <p>Numerical stability parameter.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flax import nnx\n&gt;&gt;&gt; from hyperbolix.nn_layers import HRCDropout\n&gt;&gt;&gt;\n&gt;&gt;&gt; dropout = HRCDropout(rate=0.1, rngs=nnx.Rngs(dropout=42))\n&gt;&gt;&gt; y = dropout(x, c_in=1.0, c_out=1.0, deterministic=False)\n</code></pre> Source code in <code>hyperbolix/nn_layers/hyperboloid_regularization.py</code> <pre><code>def __init__(self, rate: float, *, rngs: nnx.Rngs, eps: float = 1e-7):\n    self.dropout = nnx.Dropout(rate, rngs=rngs)\n    self.eps = eps\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HRCDropout.__call__","title":"__call__","text":"<pre><code>__call__(\n    x: Float[Array, \"batch dim_plus_1\"],\n    c_in: float = 1.0,\n    c_out: float = 1.0,\n    deterministic: bool = False,\n) -&gt; Float[Array, \"batch dim_plus_1\"]\n</code></pre> <p>Apply HRC dropout.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (batch, dim+1)</code> <p>Input points on hyperboloid with curvature c_in.</p> required <code>c_in</code> <code>float</code> <p>Input curvature (default: 1.0).</p> <code>1.0</code> <code>c_out</code> <code>float</code> <p>Output curvature (default: 1.0).</p> <code>1.0</code> <code>deterministic</code> <code>bool</code> <p>If True, no dropout is applied (for evaluation mode).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>y</code> <code>Array of shape (batch, dim+1)</code> <p>Output points on hyperboloid with curvature c_out.</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_regularization.py</code> <pre><code>def __call__(\n    self,\n    x: Float[Array, \"batch dim_plus_1\"],\n    c_in: float = 1.0,\n    c_out: float = 1.0,\n    deterministic: bool = False,\n) -&gt; Float[Array, \"batch dim_plus_1\"]:\n    \"\"\"Apply HRC dropout.\n\n    Parameters\n    ----------\n    x : Array of shape (batch, dim+1)\n        Input points on hyperboloid with curvature c_in.\n    c_in : float, optional\n        Input curvature (default: 1.0).\n    c_out : float, optional\n        Output curvature (default: 1.0).\n    deterministic : bool, optional\n        If True, no dropout is applied (for evaluation mode).\n\n    Returns\n    -------\n    y : Array of shape (batch, dim+1)\n        Output points on hyperboloid with curvature c_out.\n    \"\"\"\n\n    def drop_fn(z):\n        return self.dropout(z, deterministic=deterministic)\n\n    return hrc(x, drop_fn, c_in, c_out, self.eps)\n</code></pre>"},{"location":"api-reference/nn-layers/#hypformer-example","title":"Hypformer Example","text":"<pre><code>from hyperbolix.nn_layers import HTCLinear, HRCBatchNorm, hrc_relu\nfrom hyperbolix.manifolds import hyperboloid\nfrom flax import nnx\nimport jax.numpy as jnp\n\nclass HypformerBlock(nnx.Module):\n    \"\"\"Example using HTC/HRC components with curvature change.\"\"\"\n\n    def __init__(self, in_dim, out_dim, rngs):\n        self.linear = HTCLinear(\n            in_features=in_dim,\n            out_features=out_dim,\n            rngs=rngs\n        )\n        self.bn = HRCBatchNorm(num_features=out_dim, rngs=rngs)\n\n    def __call__(self, x, c_in=1.0, c_out=2.0, use_running_average=False):\n        # Linear transformation with curvature change\n        x = self.linear(x, c_in=c_in, c_out=c_out)\n\n        # Batch normalization (curvature-preserving)\n        x = self.bn(x, c_in=c_out, c_out=c_out,\n                    use_running_average=use_running_average)\n\n        # Activation (curvature-preserving)\n        x = hrc_relu(x, c_in=c_out, c_out=c_out)\n\n        return x\n\n# Create and use block\nblock = HypformerBlock(in_dim=33, out_dim=64, rngs=nnx.Rngs(0))\n\n# Input on hyperboloid with curvature 1.0\nx = jax.random.normal(nnx.Rngs(1).params(), (32, 33))\nx_proj = jax.vmap(hyperboloid.proj, in_axes=(0, None))(x, 1.0)\n\n# Transform to curvature 2.0\noutput = block(x_proj, c_in=1.0, c_out=2.0)\nprint(output.shape)  # (32, 65) - 64 spatial + 1 time\n</code></pre> <p>HTC vs HRC</p> <p>HRC (Hyperbolic Regularization Component):</p> <ul> <li>Applies Euclidean function <code>f_r</code> to space components only</li> <li>Use for: activations, normalization, dropout, convolutions</li> <li>Formula: <code>space = f_r(x_s)</code>, <code>time = sqrt(||space||^2 + 1/c_out)</code></li> </ul> <p>HTC (Hyperbolic Transformation Component):</p> <ul> <li>Applies Euclidean function <code>f_t</code> to full point (time + space)</li> <li>Use for: learnable linear transformations</li> <li>Formula: <code>space = f_t(x)</code>, <code>time = sqrt(||space||^2 + 1/c_out)</code></li> </ul> <p>Both support curvature changes (<code>c_in \u2192 c_out</code>) for flexible network design.</p>"},{"location":"api-reference/nn-layers/#regression-layers","title":"Regression Layers","text":"<p>Single-layer classifiers with Riemannian geometry.</p>"},{"location":"api-reference/nn-layers/#poincare-regression","title":"Poincar\u00e9 Regression","text":""},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HypRegressionPoincare","title":"hyperbolix.nn_layers.HypRegressionPoincare","text":"<pre><code>HypRegressionPoincare(\n    manifold_module: Any,\n    in_dim: int,\n    out_dim: int,\n    *,\n    rngs: Rngs,\n    input_space: str = \"manifold\",\n    clamping_factor: float = 1.0,\n    smoothing_factor: float = 50.0,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Hyperbolic Neural Networks multinomial linear regression layer (Poincar\u00e9 ball model).</p> <p>Computation steps:     0) Project the input tensor onto the manifold (optional)     1) Compute the multinomial linear regression score(s)</p> <p>Parameters:</p> Name Type Description Default <code>manifold_module</code> <code>module</code> <p>The PoincareBall manifold module</p> required <code>in_dim</code> <code>int</code> <p>Dimension of the input space</p> required <code>out_dim</code> <code>int</code> <p>Dimension of the output space</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators for parameter initialization</p> required <code>input_space</code> <code>str</code> <p>Type of the input tensor, either 'tangent' or 'manifold' (default: 'manifold'). Note: This is a static configuration - changing it after initialization requires recompilation.</p> <code>'manifold'</code> <code>clamping_factor</code> <code>float</code> <p>Clamping factor for the multinomial linear regression output (default: 1.0)</p> <code>1.0</code> <code>smoothing_factor</code> <code>float</code> <p>Smoothing factor for the multinomial linear regression output (default: 50.0)</p> <code>50.0</code> Notes <p>JIT Compatibility:     This layer is designed to work with nnx.jit. Configuration parameters (input_space,     clamping_factor, smoothing_factor) are treated as static and will be baked into the compiled function.</p> References <p>Ganea Octavian, Gary B\u00e9cigneul, and Thomas Hofmann. \"Hyperbolic neural networks.\"     Advances in neural information processing systems 31 (2018).</p> Source code in <code>hyperbolix/nn_layers/poincare_regression.py</code> <pre><code>def __init__(\n    self,\n    manifold_module: Any,\n    in_dim: int,\n    out_dim: int,\n    *,\n    rngs: nnx.Rngs,\n    input_space: str = \"manifold\",\n    clamping_factor: float = 1.0,\n    smoothing_factor: float = 50.0,\n):\n    if input_space not in [\"tangent\", \"manifold\"]:\n        raise ValueError(f\"input_space must be either 'tangent' or 'manifold', got '{input_space}'\")\n\n    # Static configuration (treated as compile-time constants for JIT)\n    self.manifold = manifold_module\n    self.in_dim = in_dim\n    self.out_dim = out_dim\n    self.input_space = input_space\n    self.clamping_factor = clamping_factor\n    self.smoothing_factor = smoothing_factor\n\n    # Trainable parameters\n    # Tangent space weight (Euclidean)\n    self.weight = nnx.Param(jax.random.normal(rngs.params(), (out_dim, in_dim)))\n    # Manifold bias (initialized to small random values)\n    # Mark as manifold parameter for Riemannian optimization\n    self.bias = mark_manifold_param(\n        nnx.Param(jax.random.normal(rngs.params(), (out_dim, in_dim)) * 0.01),\n        manifold_type=\"poincare\",\n        curvature=1.0,  # Default curvature, will be overridden by c parameter in forward pass\n    )\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HypRegressionPoincare.__call__","title":"__call__","text":"<pre><code>__call__(\n    x: Float[Array, \"batch in_dim\"], c: float = 1.0\n) -&gt; Float[Array, \"batch out_dim\"]\n</code></pre> <p>Forward pass through the hyperbolic regression layer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (batch, in_dim)</code> <p>Input tensor where the hyperbolic_axis is last</p> required <code>c</code> <code>float</code> <p>Manifold curvature (default: 1.0)</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>res</code> <code>Array of shape (batch, out_dim)</code> <p>Multinomial linear regression scores</p> Source code in <code>hyperbolix/nn_layers/poincare_regression.py</code> <pre><code>def __call__(\n    self,\n    x: Float[Array, \"batch in_dim\"],\n    c: float = 1.0,\n) -&gt; Float[Array, \"batch out_dim\"]:\n    \"\"\"\n    Forward pass through the hyperbolic regression layer.\n\n    Parameters\n    ----------\n    x : Array of shape (batch, in_dim)\n        Input tensor where the hyperbolic_axis is last\n    c : float\n        Manifold curvature (default: 1.0)\n\n    Returns\n    -------\n    res : Array of shape (batch, out_dim)\n        Multinomial linear regression scores\n    \"\"\"\n    # Map to manifold if needed (static branch - JIT friendly)\n    if self.input_space == \"tangent\":\n        x = jax.vmap(self.manifold.expmap_0, in_axes=(0, None), out_axes=0)(x, c)\n\n    # Project bias to manifold (vmap over out_dim dimension)\n    bias = jax.vmap(self.manifold.proj, in_axes=(0, None), out_axes=0)(self.bias, c)\n\n    # Map self.weight from the tangent space at the origin to the tangent space at self.bias\n    # vmap over out_dim dimension\n    pt_weight = jax.vmap(self.manifold.ptransp_0, in_axes=(0, 0, None), out_axes=0)(self.weight, bias, c)\n\n    # Compute the multinomial linear regression score(s)\n    res = self._compute_mlr(x, pt_weight, bias, c)\n    return res\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HypRegressionPoincarePP","title":"hyperbolix.nn_layers.HypRegressionPoincarePP","text":"<pre><code>HypRegressionPoincarePP(\n    manifold_module: Any,\n    in_dim: int,\n    out_dim: int,\n    *,\n    rngs: Rngs,\n    input_space: str = \"manifold\",\n    clamping_factor: float = 1.0,\n    smoothing_factor: float = 50.0,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Hyperbolic Neural Networks ++ multinomial linear regression layer (Poincar\u00e9 ball model).</p> <p>Computation steps:     0) Project the input tensor onto the manifold (optional)     1) Compute the multinomial linear regression score(s)</p> <p>Parameters:</p> Name Type Description Default <code>manifold_module</code> <code>module</code> <p>The PoincareBall manifold module</p> required <code>in_dim</code> <code>int</code> <p>Dimension of the input space</p> required <code>out_dim</code> <code>int</code> <p>Dimension of the output space</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators for parameter initialization</p> required <code>input_space</code> <code>str</code> <p>Type of the input tensor, either 'tangent' or 'manifold' (default: 'manifold'). Note: This is a static configuration - changing it after initialization requires recompilation.</p> <code>'manifold'</code> <code>clamping_factor</code> <code>float</code> <p>Clamping factor for the multinomial linear regression output (default: 1.0)</p> <code>1.0</code> <code>smoothing_factor</code> <code>float</code> <p>Smoothing factor for the multinomial linear regression output (default: 50.0)</p> <code>50.0</code> Notes <p>JIT Compatibility:     This layer is designed to work with nnx.jit. Configuration parameters (input_space,     clamping_factor, smoothing_factor) are treated as static and will be baked into the compiled function.</p> References <p>Shimizu Ryohei, Yusuke Mukuta, and Tatsuya Harada. \"Hyperbolic neural networks++.\"     arXiv preprint arXiv:2006.08210 (2020).</p> Source code in <code>hyperbolix/nn_layers/poincare_regression.py</code> <pre><code>def __init__(\n    self,\n    manifold_module: Any,\n    in_dim: int,\n    out_dim: int,\n    *,\n    rngs: nnx.Rngs,\n    input_space: str = \"manifold\",\n    clamping_factor: float = 1.0,\n    smoothing_factor: float = 50.0,\n):\n    if input_space not in [\"tangent\", \"manifold\"]:\n        raise ValueError(f\"input_space must be either 'tangent' or 'manifold', got '{input_space}'\")\n\n    # Static configuration (treated as compile-time constants for JIT)\n    self.manifold = manifold_module\n    self.in_dim = in_dim\n    self.out_dim = out_dim\n    self.input_space = input_space\n    self.clamping_factor = clamping_factor\n    self.smoothing_factor = smoothing_factor\n\n    # Trainable parameters\n    # Tangent space weight\n    self.weight = nnx.Param(jax.random.normal(rngs.params(), (out_dim, in_dim)))\n    # Scalar bias (initialized to small random values)\n    self.bias = nnx.Param(jax.random.normal(rngs.params(), (out_dim, 1)) * 0.01)\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HypRegressionPoincarePP.__call__","title":"__call__","text":"<pre><code>__call__(\n    x: Float[Array, \"batch in_dim\"], c: float = 1.0\n) -&gt; Float[Array, \"batch out_dim\"]\n</code></pre> <p>Forward pass through the HNN++ hyperbolic regression layer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (batch, in_dim)</code> <p>Input tensor where the hyperbolic_axis is last</p> required <code>c</code> <code>float</code> <p>Manifold curvature (default: 1.0)</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>res</code> <code>Array of shape (batch, out_dim)</code> <p>Multinomial linear regression scores</p> Source code in <code>hyperbolix/nn_layers/poincare_regression.py</code> <pre><code>def __call__(\n    self,\n    x: Float[Array, \"batch in_dim\"],\n    c: float = 1.0,\n) -&gt; Float[Array, \"batch out_dim\"]:\n    \"\"\"\n    Forward pass through the HNN++ hyperbolic regression layer.\n\n    Parameters\n    ----------\n    x : Array of shape (batch, in_dim)\n        Input tensor where the hyperbolic_axis is last\n    c : float\n        Manifold curvature (default: 1.0)\n\n    Returns\n    -------\n    res : Array of shape (batch, out_dim)\n        Multinomial linear regression scores\n    \"\"\"\n    # Map to manifold if needed (static branch - JIT friendly)\n    if self.input_space == \"tangent\":\n        x = jax.vmap(self.manifold.expmap_0, in_axes=(0, None), out_axes=0)(x, c)\n\n    # Compute multinomial linear regression\n    res = compute_mlr_poincare_pp(\n        x,\n        self.weight[...],\n        self.bias[...],\n        c,\n        self.clamping_factor,\n        self.smoothing_factor,\n    )\n\n    return res\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperboloid-regression","title":"Hyperboloid Regression","text":""},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HypRegressionHyperboloid","title":"hyperbolix.nn_layers.HypRegressionHyperboloid","text":"<pre><code>HypRegressionHyperboloid(\n    manifold_module: Any,\n    in_dim: int,\n    out_dim: int,\n    *,\n    rngs: Rngs,\n    input_space: str = \"manifold\",\n    clamping_factor: float = 1.0,\n    smoothing_factor: float = 50.0,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Fully Hyperbolic Convolutional Neural Networks multinomial linear regression layer (Hyperboloid model).</p> <p>Computation steps:     0) Project the input tensor onto the manifold (optional)     1) Compute the multinomial linear regression score(s)</p> <p>Parameters:</p> Name Type Description Default <code>manifold_module</code> <code>module</code> <p>The Hyperboloid manifold module</p> required <code>in_dim</code> <code>int</code> <p>Dimension of the input space</p> required <code>out_dim</code> <code>int</code> <p>Dimension of the output space</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators for parameter initialization</p> required <code>input_space</code> <code>str</code> <p>Type of the input tensor, either 'tangent' or 'manifold' (default: 'manifold'). Note: This is a static configuration - changing it after initialization requires recompilation.</p> <code>'manifold'</code> <code>clamping_factor</code> <code>float</code> <p>Clamping factor for the multinomial linear regression output (default: 1.0)</p> <code>1.0</code> <code>smoothing_factor</code> <code>float</code> <p>Smoothing factor for the multinomial linear regression output (default: 50.0)</p> <code>50.0</code> Notes <p>JIT Compatibility:     This layer is designed to work with nnx.jit. Configuration parameters (input_space,     clamping_factor, smoothing_factor) are treated as static and will be baked into the compiled function.</p> References <p>Ahmad Bdeir, Kristian Schwethelm, and Niels Landwehr. \"Fully hyperbolic convolutional neural networks for computer vision.\"     arXiv preprint arXiv:2303.15919 (2023).</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_regression.py</code> <pre><code>def __init__(\n    self,\n    manifold_module: Any,\n    in_dim: int,\n    out_dim: int,\n    *,\n    rngs: nnx.Rngs,\n    input_space: str = \"manifold\",\n    clamping_factor: float = 1.0,\n    smoothing_factor: float = 50.0,\n):\n    if input_space not in [\"tangent\", \"manifold\"]:\n        raise ValueError(f\"input_space must be either 'tangent' or 'manifold', got '{input_space}'\")\n\n    # Static configuration (treated as compile-time constants for JIT)\n    self.manifold = manifold_module\n    self.in_dim = in_dim\n    self.out_dim = out_dim\n    self.input_space = input_space\n    self.clamping_factor = clamping_factor\n    self.smoothing_factor = smoothing_factor\n\n    # Trainable parameters\n    # weight lies in the tangent space of the Hyperboloid origin, so the time coordinate along axis is zero\n    self.weight = nnx.Param(jax.random.normal(rngs.params(), (out_dim, in_dim - 1)))\n    # Scalar bias (initialized to small random values)\n    self.bias = nnx.Param(jax.random.normal(rngs.params(), (out_dim, 1)) * 0.01)\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HypRegressionHyperboloid.__call__","title":"__call__","text":"<pre><code>__call__(\n    x: Float[Array, \"batch in_dim\"], c: float = 1.0\n) -&gt; Float[Array, \"batch out_dim\"]\n</code></pre> <p>Forward pass through the hyperbolic regression layer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (batch, in_dim)</code> <p>Input tensor where the hyperbolic_axis is last</p> required <code>c</code> <code>float</code> <p>Manifold curvature (default: 1.0)</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>res</code> <code>Array of shape (batch, out_dim)</code> <p>Multinomial linear regression scores</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_regression.py</code> <pre><code>def __call__(\n    self,\n    x: Float[Array, \"batch in_dim\"],\n    c: float = 1.0,\n) -&gt; Float[Array, \"batch out_dim\"]:\n    \"\"\"\n    Forward pass through the hyperbolic regression layer.\n\n    Parameters\n    ----------\n    x : Array of shape (batch, in_dim)\n        Input tensor where the hyperbolic_axis is last\n    c : float\n        Manifold curvature (default: 1.0)\n\n    Returns\n    -------\n    res : Array of shape (batch, out_dim)\n        Multinomial linear regression scores\n    \"\"\"\n    # Map to manifold if needed (static branch - JIT friendly)\n    if self.input_space == \"tangent\":\n        x = jax.vmap(self.manifold.expmap_0, in_axes=(0, None), out_axes=0)(x, c)\n\n    # Compute multinomial linear regression\n    res = compute_mlr_hyperboloid(\n        x,\n        self.weight[...],\n        self.bias[...],\n        c,\n        self.clamping_factor,\n        self.smoothing_factor,\n    )\n\n    return res\n</code></pre>"},{"location":"api-reference/nn-layers/#reinforcement-learning","title":"Reinforcement Learning","text":""},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HypRegressionPoincareHDRL","title":"hyperbolix.nn_layers.HypRegressionPoincareHDRL","text":"<pre><code>HypRegressionPoincareHDRL(\n    manifold_module: Any,\n    in_dim: int,\n    out_dim: int,\n    *,\n    rngs: Rngs,\n    input_space: str = \"manifold\",\n    version: str = \"standard\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Hyperbolic Deep Reinforcement Learning multinomial linear regression layer (Poincar\u00e9 ball model).</p> <p>Computation steps:     0) Project the input tensor onto the manifold (optional)     1) Compute the multinomial linear regression score(s)</p> <p>Parameters:</p> Name Type Description Default <code>manifold_module</code> <code>module</code> <p>The PoincareBall manifold module</p> required <code>in_dim</code> <code>int</code> <p>Dimension of the input space</p> required <code>out_dim</code> <code>int</code> <p>Dimension of the output space</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators for parameter initialization</p> required <code>input_space</code> <code>str</code> <p>Type of the input tensor, either 'tangent' or 'manifold' (default: 'manifold'). Note: This is a static configuration - changing it after initialization requires recompilation.</p> <code>'manifold'</code> <code>version</code> <code>str</code> <p>Algorithm version to use: 'standard' or 'rs' (default: 'standard'). - 'standard': scaled multinomial linear regression score function - 'rs': multinomial linear regression with parallel transported a Note: This is a static configuration - changing it after initialization requires recompilation.</p> <code>'standard'</code> Notes <p>JIT Compatibility:     This layer is designed to work with nnx.jit. Configuration parameters (input_space, version)     are treated as static and will be baked into the compiled function.</p> References <p>Edoardo Cetin, Benjamin Chamberlain, Michael Bronstein and Jonathan J Hunt. \"Hyperbolic deep reinforcement learning.\"     arXiv (2022). Max Kochurov, Rasul Karimov and Serge Kozlukov. \"Geoopt: Riemannian Optimization in PyTorch.\"     arXiv (2020).</p> Source code in <code>hyperbolix/nn_layers/poincare_rl.py</code> <pre><code>def __init__(\n    self,\n    manifold_module: Any,\n    in_dim: int,\n    out_dim: int,\n    *,\n    rngs: nnx.Rngs,\n    input_space: str = \"manifold\",\n    version: str = \"standard\",\n):\n    if input_space not in [\"tangent\", \"manifold\"]:\n        raise ValueError(f\"input_space must be either 'tangent' or 'manifold', got '{input_space}'\")\n    if version not in [\"standard\", \"rs\"]:\n        raise ValueError(f\"version must be either 'standard' or 'rs', got '{version}'\")\n\n    # Static configuration (treated as compile-time constants for JIT)\n    self.manifold = manifold_module\n    self.in_dim = in_dim\n    self.out_dim = out_dim\n    self.input_space = input_space\n    self.version = version\n\n    # Trainable parameters\n    # Tangent space weight\n    self.weight = nnx.Param(jax.random.normal(rngs.params(), (out_dim, in_dim)))\n    # Manifold bias (initialized to small random values)\n    # FIXME: Not using ManifoldParameter\n    self.bias = nnx.Param(jax.random.normal(rngs.params(), (out_dim, in_dim)) * 0.01)\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.HypRegressionPoincareHDRL.__call__","title":"__call__","text":"<pre><code>__call__(\n    x: Float[Array, \"batch in_dim\"], c: float = 1.0\n) -&gt; Float[Array, \"batch out_dim\"]\n</code></pre> <p>Forward pass through the HDRL hyperbolic regression layer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (batch, in_dim)</code> <p>Input tensor where the hyperbolic_axis is last</p> required <code>c</code> <code>float</code> <p>Manifold curvature (default: 1.0)</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>res</code> <code>Array of shape (batch, out_dim)</code> <p>Multinomial linear regression scores</p> Source code in <code>hyperbolix/nn_layers/poincare_rl.py</code> <pre><code>def __call__(\n    self,\n    x: Float[Array, \"batch in_dim\"],\n    c: float = 1.0,\n) -&gt; Float[Array, \"batch out_dim\"]:\n    \"\"\"\n    Forward pass through the HDRL hyperbolic regression layer.\n\n    Parameters\n    ----------\n    x : Array of shape (batch, in_dim)\n        Input tensor where the hyperbolic_axis is last\n    c : float\n        Manifold curvature (default: 1.0)\n\n    Returns\n    -------\n    res : Array of shape (batch, out_dim)\n        Multinomial linear regression scores\n    \"\"\"\n    # Map to manifold if needed (static branch - JIT friendly)\n    if self.input_space == \"tangent\":\n        x = jax.vmap(self.manifold.expmap_0, in_axes=(0, None), out_axes=0)(x, c)\n\n    # Project bias to manifold (vmap over out_dim dimension)\n    bias = jax.vmap(self.manifold.proj, in_axes=(0, None), out_axes=0)(self.bias[...], c)\n\n    # Compute MLR scores\n    res = self._compute_mlr(x, self.weight[...], bias, c)\n    return res\n</code></pre>"},{"location":"api-reference/nn-layers/#regression-example","title":"Regression Example","text":"<pre><code>from hyperbolix.nn_layers import HypRegressionPoincare\nfrom hyperbolix.manifolds import poincare\nfrom flax import nnx\n\n# Multi-class classification (10 classes)\nregressor = HypRegressionPoincare(\n    manifold_module=poincare,\n    in_dim=32,\n    out_dim=10,\n    rngs=nnx.Rngs(0)\n)\n\n# Input: hyperbolic embeddings\nx = jax.random.normal(nnx.Rngs(1).params(), (64, 32)) * 0.3\nx_proj = jax.vmap(poincare.proj, in_axes=(0, None))(x, 1.0)\n\n# Forward pass returns logits\nlogits = regressor(x_proj, c=1.0)\nprint(logits.shape)  # (64, 10)\n\n# Use with softmax for classification\nprobs = jax.nn.softmax(logits, axis=-1)\n</code></pre>"},{"location":"api-reference/nn-layers/#activation-functions","title":"Activation Functions","text":"<p>Hyperbolic activation functions that preserve manifold constraints. All activations follow the HRC pattern: apply function to space components, then reconstruct time.</p>"},{"location":"api-reference/nn-layers/#curvature-preserving-activations","title":"Curvature-Preserving Activations","text":""},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.hyp_relu","title":"hyperbolix.nn_layers.hyp_relu","text":"<pre><code>hyp_relu(\n    x: Float[Array, \"... dim_plus_1\"], c: float\n) -&gt; Float[Array, \"... dim_plus_1\"]\n</code></pre> <p>Apply ReLU activation to space components of hyperboloid point(s).</p> <p>Curvature-preserving wrapper around hrc_relu(x, c_in=c, c_out=c).</p> <p>This function applies the ReLU activation function to the spatial components of hyperboloid points and reconstructs valid manifold points using the hyperboloid constraint.</p> <p>Mathematical formula:     y = [sqrt(||ReLU(x_s)||^2 + 1/c), ReLU(x_s)]</p> <p>where x_s are the spatial components x[..., 1:].</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (..., dim+1)</code> <p>Input point(s) on the hyperboloid manifold in ambient space, where ... represents arbitrary batch dimensions. The last dimension contains the time component (x[..., 0]) and spatial components (x[..., 1:]).</p> required <code>c</code> <code>float</code> <p>Curvature parameter, must be positive (c &gt; 0).</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>Array of shape (..., dim+1)</code> <p>Output point(s) on the hyperboloid manifold, same shape as input.</p> Notes <ul> <li>This function applies ReLU only to spatial components, not the time component</li> <li>The time component is reconstructed using the hyperboloid constraint:   -x\u2080\u00b2 + ||x_rest||\u00b2 = -1/c</li> <li>This approach avoids frequent exp/log maps for better numerical stability</li> <li>Works on arrays of any shape, similar to jax.nn.relu</li> <li>For curvature-changing transformations, use <code>hrc_relu</code> which supports   different input/output curvatures</li> </ul> References <p>Ahmad Bdeir, Kristian Schwethelm, and Niels Landwehr. \"Fully hyperbolic convolutional neural networks for computer vision.\" arXiv preprint arXiv:2303.15919 (2023).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from hyperbolix.nn_layers import hyp_relu\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Single point\n&gt;&gt;&gt; x = jnp.array([1.05, 0.1, -0.2, 0.15])\n&gt;&gt;&gt; y = hyp_relu(x, c=1.0)\n&gt;&gt;&gt; y.shape\n(4,)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Batch of points\n&gt;&gt;&gt; x_batch = jnp.ones((8, 5))  # 8 points in 5-dim ambient space\n&gt;&gt;&gt; y_batch = hyp_relu(x_batch, c=1.0)\n&gt;&gt;&gt; y_batch.shape\n(8, 5)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Multi-dimensional batch (e.g., feature maps)\n&gt;&gt;&gt; x_feature = jnp.ones((4, 16, 16, 10))  # 4 images, 16x16 spatial, 10-dim\n&gt;&gt;&gt; y_feature = hyp_relu(x_feature, c=1.0)\n&gt;&gt;&gt; y_feature.shape\n(4, 16, 16, 10)\n</code></pre> Source code in <code>hyperbolix/nn_layers/hyperboloid_activations.py</code> <pre><code>def hyp_relu(x: Float[Array, \"... dim_plus_1\"], c: float) -&gt; Float[Array, \"... dim_plus_1\"]:\n    \"\"\"Apply ReLU activation to space components of hyperboloid point(s).\n\n    Curvature-preserving wrapper around hrc_relu(x, c_in=c, c_out=c).\n\n    This function applies the ReLU activation function to the spatial components\n    of hyperboloid points and reconstructs valid manifold points using the\n    hyperboloid constraint.\n\n    Mathematical formula:\n        y = [sqrt(||ReLU(x_s)||^2 + 1/c), ReLU(x_s)]\n\n    where x_s are the spatial components x[..., 1:].\n\n    Parameters\n    ----------\n    x : Array of shape (..., dim+1)\n        Input point(s) on the hyperboloid manifold in ambient space, where\n        ... represents arbitrary batch dimensions. The last dimension contains\n        the time component (x[..., 0]) and spatial components (x[..., 1:]).\n    c : float\n        Curvature parameter, must be positive (c &gt; 0).\n\n    Returns\n    -------\n    y : Array of shape (..., dim+1)\n        Output point(s) on the hyperboloid manifold, same shape as input.\n\n    Notes\n    -----\n    - This function applies ReLU only to spatial components, not the time component\n    - The time component is reconstructed using the hyperboloid constraint:\n      -x\u2080\u00b2 + ||x_rest||\u00b2 = -1/c\n    - This approach avoids frequent exp/log maps for better numerical stability\n    - Works on arrays of any shape, similar to jax.nn.relu\n    - For curvature-changing transformations, use `hrc_relu` which supports\n      different input/output curvatures\n\n    References\n    ----------\n    Ahmad Bdeir, Kristian Schwethelm, and Niels Landwehr. \"Fully hyperbolic\n    convolutional neural networks for computer vision.\" arXiv preprint\n    arXiv:2303.15919 (2023).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt; from hyperbolix.nn_layers import hyp_relu\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Single point\n    &gt;&gt;&gt; x = jnp.array([1.05, 0.1, -0.2, 0.15])\n    &gt;&gt;&gt; y = hyp_relu(x, c=1.0)\n    &gt;&gt;&gt; y.shape\n    (4,)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Batch of points\n    &gt;&gt;&gt; x_batch = jnp.ones((8, 5))  # 8 points in 5-dim ambient space\n    &gt;&gt;&gt; y_batch = hyp_relu(x_batch, c=1.0)\n    &gt;&gt;&gt; y_batch.shape\n    (8, 5)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Multi-dimensional batch (e.g., feature maps)\n    &gt;&gt;&gt; x_feature = jnp.ones((4, 16, 16, 10))  # 4 images, 16x16 spatial, 10-dim\n    &gt;&gt;&gt; y_feature = hyp_relu(x_feature, c=1.0)\n    &gt;&gt;&gt; y_feature.shape\n    (4, 16, 16, 10)\n    \"\"\"\n    return hrc_relu(x, c_in=c, c_out=c)\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.hyp_leaky_relu","title":"hyperbolix.nn_layers.hyp_leaky_relu","text":"<pre><code>hyp_leaky_relu(\n    x: Float[Array, \"... dim_plus_1\"],\n    c: float,\n    negative_slope: float = 0.01,\n) -&gt; Float[Array, \"... dim_plus_1\"]\n</code></pre> <p>Apply LeakyReLU activation to space components of hyperboloid point(s).</p> <p>Curvature-preserving wrapper around hrc_leaky_relu(x, c_in=c, c_out=c, negative_slope).</p> <p>This function applies the LeakyReLU activation function to the spatial components of hyperboloid points and reconstructs valid manifold points using the hyperboloid constraint.</p> <p>Mathematical formula:     y = [sqrt(||LeakyReLU(x_s)||^2 + 1/c), LeakyReLU(x_s)]</p> <p>where x_s are the spatial components x[..., 1:], and LeakyReLU(x) = x if x &gt; 0 else negative_slope * x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (..., dim+1)</code> <p>Input point(s) on the hyperboloid manifold in ambient space, where ... represents arbitrary batch dimensions. The last dimension contains the time component (x[..., 0]) and spatial components (x[..., 1:]).</p> required <code>c</code> <code>float</code> <p>Curvature parameter, must be positive (c &gt; 0).</p> required <code>negative_slope</code> <code>float</code> <p>Negative slope coefficient for LeakyReLU (default: 0.01).</p> <code>0.01</code> <p>Returns:</p> Name Type Description <code>y</code> <code>Array of shape (..., dim+1)</code> <p>Output point(s) on the hyperboloid manifold, same shape as input.</p> Notes <ul> <li>This function applies LeakyReLU only to spatial components</li> <li>The time component is reconstructed using the hyperboloid constraint</li> <li>LeakyReLU allows small negative values (scaled by negative_slope) which   can help gradient flow compared to standard ReLU</li> <li>Works on arrays of any shape, similar to jax.nn.leaky_relu</li> <li>For curvature-changing transformations, use <code>hrc_leaky_relu</code> which   supports different input/output curvatures</li> </ul> References <p>Ahmad Bdeir, Kristian Schwethelm, and Niels Landwehr. \"Fully hyperbolic convolutional neural networks for computer vision.\" arXiv preprint arXiv:2303.15919 (2023).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from hyperbolix.nn_layers import hyp_leaky_relu\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Single point with default negative_slope\n&gt;&gt;&gt; x = jnp.array([1.05, 0.1, -0.2, 0.15])\n&gt;&gt;&gt; y = hyp_leaky_relu(x, c=1.0)\n&gt;&gt;&gt; y.shape\n(4,)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Custom negative_slope\n&gt;&gt;&gt; y = hyp_leaky_relu(x, c=1.0, negative_slope=0.1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Batch of points\n&gt;&gt;&gt; x_batch = jnp.ones((8, 5))\n&gt;&gt;&gt; y_batch = hyp_leaky_relu(x_batch, c=1.0, negative_slope=0.01)\n&gt;&gt;&gt; y_batch.shape\n(8, 5)\n</code></pre> Source code in <code>hyperbolix/nn_layers/hyperboloid_activations.py</code> <pre><code>def hyp_leaky_relu(\n    x: Float[Array, \"... dim_plus_1\"], c: float, negative_slope: float = 0.01\n) -&gt; Float[Array, \"... dim_plus_1\"]:\n    \"\"\"Apply LeakyReLU activation to space components of hyperboloid point(s).\n\n    Curvature-preserving wrapper around hrc_leaky_relu(x, c_in=c, c_out=c, negative_slope).\n\n    This function applies the LeakyReLU activation function to the spatial\n    components of hyperboloid points and reconstructs valid manifold points\n    using the hyperboloid constraint.\n\n    Mathematical formula:\n        y = [sqrt(||LeakyReLU(x_s)||^2 + 1/c), LeakyReLU(x_s)]\n\n    where x_s are the spatial components x[..., 1:], and\n    LeakyReLU(x) = x if x &gt; 0 else negative_slope * x.\n\n    Parameters\n    ----------\n    x : Array of shape (..., dim+1)\n        Input point(s) on the hyperboloid manifold in ambient space, where\n        ... represents arbitrary batch dimensions. The last dimension contains\n        the time component (x[..., 0]) and spatial components (x[..., 1:]).\n    c : float\n        Curvature parameter, must be positive (c &gt; 0).\n    negative_slope : float, optional\n        Negative slope coefficient for LeakyReLU (default: 0.01).\n\n    Returns\n    -------\n    y : Array of shape (..., dim+1)\n        Output point(s) on the hyperboloid manifold, same shape as input.\n\n    Notes\n    -----\n    - This function applies LeakyReLU only to spatial components\n    - The time component is reconstructed using the hyperboloid constraint\n    - LeakyReLU allows small negative values (scaled by negative_slope) which\n      can help gradient flow compared to standard ReLU\n    - Works on arrays of any shape, similar to jax.nn.leaky_relu\n    - For curvature-changing transformations, use `hrc_leaky_relu` which\n      supports different input/output curvatures\n\n    References\n    ----------\n    Ahmad Bdeir, Kristian Schwethelm, and Niels Landwehr. \"Fully hyperbolic\n    convolutional neural networks for computer vision.\" arXiv preprint\n    arXiv:2303.15919 (2023).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt; from hyperbolix.nn_layers import hyp_leaky_relu\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Single point with default negative_slope\n    &gt;&gt;&gt; x = jnp.array([1.05, 0.1, -0.2, 0.15])\n    &gt;&gt;&gt; y = hyp_leaky_relu(x, c=1.0)\n    &gt;&gt;&gt; y.shape\n    (4,)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Custom negative_slope\n    &gt;&gt;&gt; y = hyp_leaky_relu(x, c=1.0, negative_slope=0.1)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Batch of points\n    &gt;&gt;&gt; x_batch = jnp.ones((8, 5))\n    &gt;&gt;&gt; y_batch = hyp_leaky_relu(x_batch, c=1.0, negative_slope=0.01)\n    &gt;&gt;&gt; y_batch.shape\n    (8, 5)\n    \"\"\"\n    return hrc_leaky_relu(x, c_in=c, c_out=c, negative_slope=negative_slope)\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.hyp_tanh","title":"hyperbolix.nn_layers.hyp_tanh","text":"<pre><code>hyp_tanh(\n    x: Float[Array, \"... dim_plus_1\"], c: float\n) -&gt; Float[Array, \"... dim_plus_1\"]\n</code></pre> <p>Apply tanh activation to space components of hyperboloid point(s).</p> <p>Curvature-preserving wrapper around hrc_tanh(x, c_in=c, c_out=c).</p> <p>This function applies the hyperbolic tangent activation function to the spatial components of hyperboloid points and reconstructs valid manifold points using the hyperboloid constraint.</p> <p>Mathematical formula:     y = [sqrt(||tanh(x_s)||^2 + 1/c), tanh(x_s)]</p> <p>where x_s are the spatial components x[..., 1:].</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (..., dim+1)</code> <p>Input point(s) on the hyperboloid manifold in ambient space, where ... represents arbitrary batch dimensions. The last dimension contains the time component (x[..., 0]) and spatial components (x[..., 1:]).</p> required <code>c</code> <code>float</code> <p>Curvature parameter, must be positive (c &gt; 0).</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>Array of shape (..., dim+1)</code> <p>Output point(s) on the hyperboloid manifold, same shape as input.</p> Notes <ul> <li>This function applies tanh only to spatial components</li> <li>The time component is reconstructed using the hyperboloid constraint</li> <li>Tanh naturally bounds outputs in [-1, 1], which can help with stability</li> <li>Works on arrays of any shape, similar to jax.nn.tanh</li> <li>For curvature-changing transformations, use <code>hrc_tanh</code> which supports   different input/output curvatures</li> </ul> References <p>Ahmad Bdeir, Kristian Schwethelm, and Niels Landwehr. \"Fully hyperbolic convolutional neural networks for computer vision.\" arXiv preprint arXiv:2303.15919 (2023).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from hyperbolix.nn_layers import hyp_tanh\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Single point\n&gt;&gt;&gt; x = jnp.array([1.05, 0.1, -0.2, 0.15])\n&gt;&gt;&gt; y = hyp_tanh(x, c=1.0)\n&gt;&gt;&gt; y.shape\n(4,)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Batch of points\n&gt;&gt;&gt; x_batch = jnp.ones((8, 5))\n&gt;&gt;&gt; y_batch = hyp_tanh(x_batch, c=1.0)\n&gt;&gt;&gt; y_batch.shape\n(8, 5)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Verify spatial components are bounded\n&gt;&gt;&gt; import jax\n&gt;&gt;&gt; assert jnp.all(jnp.abs(y_batch[..., 1:]) &lt;= 1.0)\n</code></pre> Source code in <code>hyperbolix/nn_layers/hyperboloid_activations.py</code> <pre><code>def hyp_tanh(x: Float[Array, \"... dim_plus_1\"], c: float) -&gt; Float[Array, \"... dim_plus_1\"]:\n    \"\"\"Apply tanh activation to space components of hyperboloid point(s).\n\n    Curvature-preserving wrapper around hrc_tanh(x, c_in=c, c_out=c).\n\n    This function applies the hyperbolic tangent activation function to the\n    spatial components of hyperboloid points and reconstructs valid manifold\n    points using the hyperboloid constraint.\n\n    Mathematical formula:\n        y = [sqrt(||tanh(x_s)||^2 + 1/c), tanh(x_s)]\n\n    where x_s are the spatial components x[..., 1:].\n\n    Parameters\n    ----------\n    x : Array of shape (..., dim+1)\n        Input point(s) on the hyperboloid manifold in ambient space, where\n        ... represents arbitrary batch dimensions. The last dimension contains\n        the time component (x[..., 0]) and spatial components (x[..., 1:]).\n    c : float\n        Curvature parameter, must be positive (c &gt; 0).\n\n    Returns\n    -------\n    y : Array of shape (..., dim+1)\n        Output point(s) on the hyperboloid manifold, same shape as input.\n\n    Notes\n    -----\n    - This function applies tanh only to spatial components\n    - The time component is reconstructed using the hyperboloid constraint\n    - Tanh naturally bounds outputs in [-1, 1], which can help with stability\n    - Works on arrays of any shape, similar to jax.nn.tanh\n    - For curvature-changing transformations, use `hrc_tanh` which supports\n      different input/output curvatures\n\n    References\n    ----------\n    Ahmad Bdeir, Kristian Schwethelm, and Niels Landwehr. \"Fully hyperbolic\n    convolutional neural networks for computer vision.\" arXiv preprint\n    arXiv:2303.15919 (2023).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt; from hyperbolix.nn_layers import hyp_tanh\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Single point\n    &gt;&gt;&gt; x = jnp.array([1.05, 0.1, -0.2, 0.15])\n    &gt;&gt;&gt; y = hyp_tanh(x, c=1.0)\n    &gt;&gt;&gt; y.shape\n    (4,)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Batch of points\n    &gt;&gt;&gt; x_batch = jnp.ones((8, 5))\n    &gt;&gt;&gt; y_batch = hyp_tanh(x_batch, c=1.0)\n    &gt;&gt;&gt; y_batch.shape\n    (8, 5)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Verify spatial components are bounded\n    &gt;&gt;&gt; import jax\n    &gt;&gt;&gt; assert jnp.all(jnp.abs(y_batch[..., 1:]) &lt;= 1.0)\n    \"\"\"\n    return hrc_tanh(x, c_in=c, c_out=c)\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.hyp_swish","title":"hyperbolix.nn_layers.hyp_swish","text":"<pre><code>hyp_swish(\n    x: Float[Array, \"... dim_plus_1\"], c: float\n) -&gt; Float[Array, \"... dim_plus_1\"]\n</code></pre> <p>Apply Swish/SiLU activation to space components of hyperboloid point(s).</p> <p>Curvature-preserving wrapper around hrc_swish(x, c_in=c, c_out=c).</p> <p>This function applies the Swish (also known as SiLU) activation function to the spatial components of hyperboloid points and reconstructs valid manifold points using the hyperboloid constraint.</p> <p>Swish is defined as: swish(x) = x * sigmoid(x)</p> <p>Mathematical formula:     y = [sqrt(||swish(x_s)||^2 + 1/c), swish(x_s)]</p> <p>where x_s are the spatial components x[..., 1:].</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (..., dim+1)</code> <p>Input point(s) on the hyperboloid manifold in ambient space, where ... represents arbitrary batch dimensions. The last dimension contains the time component (x[..., 0]) and spatial components (x[..., 1:]).</p> required <code>c</code> <code>float</code> <p>Curvature parameter, must be positive (c &gt; 0).</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>Array of shape (..., dim+1)</code> <p>Output point(s) on the hyperboloid manifold, same shape as input.</p> Notes <ul> <li>This function applies Swish only to spatial components</li> <li>The time component is reconstructed using the hyperboloid constraint</li> <li>Swish is smooth and non-monotonic, often performing well in deep networks</li> <li>Works on arrays of any shape, similar to jax.nn.swish</li> <li>For curvature-changing transformations, use <code>hrc_swish</code> which supports   different input/output curvatures</li> </ul> References <p>Ahmad Bdeir, Kristian Schwethelm, and Niels Landwehr. \"Fully hyperbolic convolutional neural networks for computer vision.\" arXiv preprint arXiv:2303.15919 (2023).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from hyperbolix.nn_layers import hyp_swish\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Single point\n&gt;&gt;&gt; x = jnp.array([1.05, 0.1, -0.2, 0.15])\n&gt;&gt;&gt; y = hyp_swish(x, c=1.0)\n&gt;&gt;&gt; y.shape\n(4,)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Batch of points\n&gt;&gt;&gt; x_batch = jnp.ones((8, 5))\n&gt;&gt;&gt; y_batch = hyp_swish(x_batch, c=1.0)\n&gt;&gt;&gt; y_batch.shape\n(8, 5)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Multi-dimensional batch\n&gt;&gt;&gt; x_feature = jnp.ones((4, 16, 16, 10))\n&gt;&gt;&gt; y_feature = hyp_swish(x_feature, c=1.0)\n&gt;&gt;&gt; y_feature.shape\n(4, 16, 16, 10)\n</code></pre> Source code in <code>hyperbolix/nn_layers/hyperboloid_activations.py</code> <pre><code>def hyp_swish(x: Float[Array, \"... dim_plus_1\"], c: float) -&gt; Float[Array, \"... dim_plus_1\"]:\n    \"\"\"Apply Swish/SiLU activation to space components of hyperboloid point(s).\n\n    Curvature-preserving wrapper around hrc_swish(x, c_in=c, c_out=c).\n\n    This function applies the Swish (also known as SiLU) activation function\n    to the spatial components of hyperboloid points and reconstructs valid\n    manifold points using the hyperboloid constraint.\n\n    Swish is defined as: swish(x) = x * sigmoid(x)\n\n    Mathematical formula:\n        y = [sqrt(||swish(x_s)||^2 + 1/c), swish(x_s)]\n\n    where x_s are the spatial components x[..., 1:].\n\n    Parameters\n    ----------\n    x : Array of shape (..., dim+1)\n        Input point(s) on the hyperboloid manifold in ambient space, where\n        ... represents arbitrary batch dimensions. The last dimension contains\n        the time component (x[..., 0]) and spatial components (x[..., 1:]).\n    c : float\n        Curvature parameter, must be positive (c &gt; 0).\n\n    Returns\n    -------\n    y : Array of shape (..., dim+1)\n        Output point(s) on the hyperboloid manifold, same shape as input.\n\n    Notes\n    -----\n    - This function applies Swish only to spatial components\n    - The time component is reconstructed using the hyperboloid constraint\n    - Swish is smooth and non-monotonic, often performing well in deep networks\n    - Works on arrays of any shape, similar to jax.nn.swish\n    - For curvature-changing transformations, use `hrc_swish` which supports\n      different input/output curvatures\n\n    References\n    ----------\n    Ahmad Bdeir, Kristian Schwethelm, and Niels Landwehr. \"Fully hyperbolic\n    convolutional neural networks for computer vision.\" arXiv preprint\n    arXiv:2303.15919 (2023).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt; from hyperbolix.nn_layers import hyp_swish\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Single point\n    &gt;&gt;&gt; x = jnp.array([1.05, 0.1, -0.2, 0.15])\n    &gt;&gt;&gt; y = hyp_swish(x, c=1.0)\n    &gt;&gt;&gt; y.shape\n    (4,)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Batch of points\n    &gt;&gt;&gt; x_batch = jnp.ones((8, 5))\n    &gt;&gt;&gt; y_batch = hyp_swish(x_batch, c=1.0)\n    &gt;&gt;&gt; y_batch.shape\n    (8, 5)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Multi-dimensional batch\n    &gt;&gt;&gt; x_feature = jnp.ones((4, 16, 16, 10))\n    &gt;&gt;&gt; y_feature = hyp_swish(x_feature, c=1.0)\n    &gt;&gt;&gt; y_feature.shape\n    (4, 16, 16, 10)\n    \"\"\"\n    return hrc_swish(x, c_in=c, c_out=c)\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.hyp_gelu","title":"hyperbolix.nn_layers.hyp_gelu","text":"<pre><code>hyp_gelu(\n    x: Float[Array, \"... dim_plus_1\"], c: float\n) -&gt; Float[Array, \"... dim_plus_1\"]\n</code></pre> <p>Apply GELU activation to space components of hyperboloid point(s).</p> <p>Curvature-preserving wrapper around hrc_gelu(x, c_in=c, c_out=c).</p> <p>This function applies the Gaussian Error Linear Unit (GELU) activation function to the spatial components of hyperboloid points and reconstructs valid manifold points using the hyperboloid constraint.</p> <p>Mathematical formula:     y = [sqrt(||GELU(x_s)||^2 + 1/c), GELU(x_s)]</p> <p>where x_s are the spatial components x[..., 1:].</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (..., dim+1)</code> <p>Input point(s) on the hyperboloid manifold in ambient space, where ... represents arbitrary batch dimensions. The last dimension contains the time component (x[..., 0]) and spatial components (x[..., 1:]).</p> required <code>c</code> <code>float</code> <p>Curvature parameter, must be positive (c &gt; 0).</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>Array of shape (..., dim+1)</code> <p>Output point(s) on the hyperboloid manifold, same shape as input.</p> Notes <ul> <li>This function applies GELU only to spatial components</li> <li>The time component is reconstructed using the hyperboloid constraint</li> <li>GELU is smooth and commonly used in transformer architectures</li> <li>Works on arrays of any shape, similar to jax.nn.gelu</li> <li>For curvature-changing transformations, use <code>hrc_gelu</code> which supports   different input/output curvatures</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from hyperbolix.nn_layers import hyp_gelu\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Single point\n&gt;&gt;&gt; x = jnp.array([1.05, 0.1, -0.2, 0.15])\n&gt;&gt;&gt; y = hyp_gelu(x, c=1.0)\n&gt;&gt;&gt; y.shape\n(4,)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Batch of points\n&gt;&gt;&gt; x_batch = jnp.ones((8, 5))\n&gt;&gt;&gt; y_batch = hyp_gelu(x_batch, c=1.0)\n&gt;&gt;&gt; y_batch.shape\n(8, 5)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Multi-dimensional batch\n&gt;&gt;&gt; x_feature = jnp.ones((4, 16, 16, 10))\n&gt;&gt;&gt; y_feature = hyp_gelu(x_feature, c=1.0)\n&gt;&gt;&gt; y_feature.shape\n(4, 16, 16, 10)\n</code></pre> Source code in <code>hyperbolix/nn_layers/hyperboloid_activations.py</code> <pre><code>def hyp_gelu(x: Float[Array, \"... dim_plus_1\"], c: float) -&gt; Float[Array, \"... dim_plus_1\"]:\n    \"\"\"Apply GELU activation to space components of hyperboloid point(s).\n\n    Curvature-preserving wrapper around hrc_gelu(x, c_in=c, c_out=c).\n\n    This function applies the Gaussian Error Linear Unit (GELU) activation function\n    to the spatial components of hyperboloid points and reconstructs valid manifold\n    points using the hyperboloid constraint.\n\n    Mathematical formula:\n        y = [sqrt(||GELU(x_s)||^2 + 1/c), GELU(x_s)]\n\n    where x_s are the spatial components x[..., 1:].\n\n    Parameters\n    ----------\n    x : Array of shape (..., dim+1)\n        Input point(s) on the hyperboloid manifold in ambient space, where\n        ... represents arbitrary batch dimensions. The last dimension contains\n        the time component (x[..., 0]) and spatial components (x[..., 1:]).\n    c : float\n        Curvature parameter, must be positive (c &gt; 0).\n\n    Returns\n    -------\n    y : Array of shape (..., dim+1)\n        Output point(s) on the hyperboloid manifold, same shape as input.\n\n    Notes\n    -----\n    - This function applies GELU only to spatial components\n    - The time component is reconstructed using the hyperboloid constraint\n    - GELU is smooth and commonly used in transformer architectures\n    - Works on arrays of any shape, similar to jax.nn.gelu\n    - For curvature-changing transformations, use `hrc_gelu` which supports\n      different input/output curvatures\n\n    Examples\n    --------\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt; from hyperbolix.nn_layers import hyp_gelu\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Single point\n    &gt;&gt;&gt; x = jnp.array([1.05, 0.1, -0.2, 0.15])\n    &gt;&gt;&gt; y = hyp_gelu(x, c=1.0)\n    &gt;&gt;&gt; y.shape\n    (4,)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Batch of points\n    &gt;&gt;&gt; x_batch = jnp.ones((8, 5))\n    &gt;&gt;&gt; y_batch = hyp_gelu(x_batch, c=1.0)\n    &gt;&gt;&gt; y_batch.shape\n    (8, 5)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Multi-dimensional batch\n    &gt;&gt;&gt; x_feature = jnp.ones((4, 16, 16, 10))\n    &gt;&gt;&gt; y_feature = hyp_gelu(x_feature, c=1.0)\n    &gt;&gt;&gt; y_feature.shape\n    (4, 16, 16, 10)\n    \"\"\"\n    return hrc_gelu(x, c_in=c, c_out=c)\n</code></pre>"},{"location":"api-reference/nn-layers/#curvature-changing-activations-hrc-based","title":"Curvature-Changing Activations (HRC-based)","text":"<p>For advanced use cases requiring curvature transformations:</p>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.hrc_relu","title":"hyperbolix.nn_layers.hrc_relu","text":"<pre><code>hrc_relu(\n    x: Float[Array, \"... dim_plus_1\"],\n    c_in: float,\n    c_out: float,\n    eps: float = 1e-07,\n) -&gt; Float[Array, \"... dim_plus_1\"]\n</code></pre> <p>HRC with ReLU activation.</p> <p>Equivalent to hrc(x, jax.nn.relu, c_in, c_out, eps). When c_in = c_out = c, this is equivalent to hyp_relu(x, c).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (..., dim+1)</code> <p>Input point(s) on the hyperboloid manifold with curvature c_in.</p> required <code>c_in</code> <code>float</code> <p>Input curvature parameter (must be positive).</p> required <code>c_out</code> <code>float</code> <p>Output curvature parameter (must be positive).</p> required <code>eps</code> <code>float</code> <p>Small value for numerical stability (default: 1e-7).</p> <code>1e-07</code> <p>Returns:</p> Name Type Description <code>y</code> <code>Array of shape (..., dim+1)</code> <p>Output point(s) on the hyperboloid manifold with curvature c_out.</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_activations.py</code> <pre><code>def hrc_relu(\n    x: Float[Array, \"... dim_plus_1\"],\n    c_in: float,\n    c_out: float,\n    eps: float = 1e-7,\n) -&gt; Float[Array, \"... dim_plus_1\"]:\n    \"\"\"HRC with ReLU activation.\n\n    Equivalent to hrc(x, jax.nn.relu, c_in, c_out, eps).\n    When c_in = c_out = c, this is equivalent to hyp_relu(x, c).\n\n    Parameters\n    ----------\n    x : Array of shape (..., dim+1)\n        Input point(s) on the hyperboloid manifold with curvature c_in.\n    c_in : float\n        Input curvature parameter (must be positive).\n    c_out : float\n        Output curvature parameter (must be positive).\n    eps : float, optional\n        Small value for numerical stability (default: 1e-7).\n\n    Returns\n    -------\n    y : Array of shape (..., dim+1)\n        Output point(s) on the hyperboloid manifold with curvature c_out.\n    \"\"\"\n    return hrc(x, jax.nn.relu, c_in, c_out, eps)\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.hrc_leaky_relu","title":"hyperbolix.nn_layers.hrc_leaky_relu","text":"<pre><code>hrc_leaky_relu(\n    x: Float[Array, \"... dim_plus_1\"],\n    c_in: float,\n    c_out: float,\n    negative_slope: float = 0.01,\n    eps: float = 1e-07,\n) -&gt; Float[Array, \"... dim_plus_1\"]\n</code></pre> <p>HRC with LeakyReLU activation.</p> <p>When c_in = c_out = c, this is equivalent to hyp_leaky_relu(x, c, negative_slope).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (..., dim+1)</code> <p>Input point(s) on the hyperboloid manifold with curvature c_in.</p> required <code>c_in</code> <code>float</code> <p>Input curvature parameter (must be positive).</p> required <code>c_out</code> <code>float</code> <p>Output curvature parameter (must be positive).</p> required <code>negative_slope</code> <code>float</code> <p>Negative slope coefficient for LeakyReLU (default: 0.01).</p> <code>0.01</code> <code>eps</code> <code>float</code> <p>Small value for numerical stability (default: 1e-7).</p> <code>1e-07</code> <p>Returns:</p> Name Type Description <code>y</code> <code>Array of shape (..., dim+1)</code> <p>Output point(s) on the hyperboloid manifold with curvature c_out.</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_activations.py</code> <pre><code>def hrc_leaky_relu(\n    x: Float[Array, \"... dim_plus_1\"],\n    c_in: float,\n    c_out: float,\n    negative_slope: float = 0.01,\n    eps: float = 1e-7,\n) -&gt; Float[Array, \"... dim_plus_1\"]:\n    \"\"\"HRC with LeakyReLU activation.\n\n    When c_in = c_out = c, this is equivalent to hyp_leaky_relu(x, c, negative_slope).\n\n    Parameters\n    ----------\n    x : Array of shape (..., dim+1)\n        Input point(s) on the hyperboloid manifold with curvature c_in.\n    c_in : float\n        Input curvature parameter (must be positive).\n    c_out : float\n        Output curvature parameter (must be positive).\n    negative_slope : float, optional\n        Negative slope coefficient for LeakyReLU (default: 0.01).\n    eps : float, optional\n        Small value for numerical stability (default: 1e-7).\n\n    Returns\n    -------\n    y : Array of shape (..., dim+1)\n        Output point(s) on the hyperboloid manifold with curvature c_out.\n    \"\"\"\n\n    def f_r(z):\n        return jax.nn.leaky_relu(z, negative_slope)\n\n    return hrc(x, f_r, c_in, c_out, eps)\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.hrc_tanh","title":"hyperbolix.nn_layers.hrc_tanh","text":"<pre><code>hrc_tanh(\n    x: Float[Array, \"... dim_plus_1\"],\n    c_in: float,\n    c_out: float,\n    eps: float = 1e-07,\n) -&gt; Float[Array, \"... dim_plus_1\"]\n</code></pre> <p>HRC with tanh activation.</p> <p>When c_in = c_out = c, this is equivalent to hyp_tanh(x, c).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (..., dim+1)</code> <p>Input point(s) on the hyperboloid manifold with curvature c_in.</p> required <code>c_in</code> <code>float</code> <p>Input curvature parameter (must be positive).</p> required <code>c_out</code> <code>float</code> <p>Output curvature parameter (must be positive).</p> required <code>eps</code> <code>float</code> <p>Small value for numerical stability (default: 1e-7).</p> <code>1e-07</code> <p>Returns:</p> Name Type Description <code>y</code> <code>Array of shape (..., dim+1)</code> <p>Output point(s) on the hyperboloid manifold with curvature c_out.</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_activations.py</code> <pre><code>def hrc_tanh(\n    x: Float[Array, \"... dim_plus_1\"],\n    c_in: float,\n    c_out: float,\n    eps: float = 1e-7,\n) -&gt; Float[Array, \"... dim_plus_1\"]:\n    \"\"\"HRC with tanh activation.\n\n    When c_in = c_out = c, this is equivalent to hyp_tanh(x, c).\n\n    Parameters\n    ----------\n    x : Array of shape (..., dim+1)\n        Input point(s) on the hyperboloid manifold with curvature c_in.\n    c_in : float\n        Input curvature parameter (must be positive).\n    c_out : float\n        Output curvature parameter (must be positive).\n    eps : float, optional\n        Small value for numerical stability (default: 1e-7).\n\n    Returns\n    -------\n    y : Array of shape (..., dim+1)\n        Output point(s) on the hyperboloid manifold with curvature c_out.\n    \"\"\"\n    return hrc(x, jnp.tanh, c_in, c_out, eps)\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.hrc_swish","title":"hyperbolix.nn_layers.hrc_swish","text":"<pre><code>hrc_swish(\n    x: Float[Array, \"... dim_plus_1\"],\n    c_in: float,\n    c_out: float,\n    eps: float = 1e-07,\n) -&gt; Float[Array, \"... dim_plus_1\"]\n</code></pre> <p>HRC with Swish/SiLU activation.</p> <p>When c_in = c_out = c, this is equivalent to hyp_swish(x, c).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (..., dim+1)</code> <p>Input point(s) on the hyperboloid manifold with curvature c_in.</p> required <code>c_in</code> <code>float</code> <p>Input curvature parameter (must be positive).</p> required <code>c_out</code> <code>float</code> <p>Output curvature parameter (must be positive).</p> required <code>eps</code> <code>float</code> <p>Small value for numerical stability (default: 1e-7).</p> <code>1e-07</code> <p>Returns:</p> Name Type Description <code>y</code> <code>Array of shape (..., dim+1)</code> <p>Output point(s) on the hyperboloid manifold with curvature c_out.</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_activations.py</code> <pre><code>def hrc_swish(\n    x: Float[Array, \"... dim_plus_1\"],\n    c_in: float,\n    c_out: float,\n    eps: float = 1e-7,\n) -&gt; Float[Array, \"... dim_plus_1\"]:\n    \"\"\"HRC with Swish/SiLU activation.\n\n    When c_in = c_out = c, this is equivalent to hyp_swish(x, c).\n\n    Parameters\n    ----------\n    x : Array of shape (..., dim+1)\n        Input point(s) on the hyperboloid manifold with curvature c_in.\n    c_in : float\n        Input curvature parameter (must be positive).\n    c_out : float\n        Output curvature parameter (must be positive).\n    eps : float, optional\n        Small value for numerical stability (default: 1e-7).\n\n    Returns\n    -------\n    y : Array of shape (..., dim+1)\n        Output point(s) on the hyperboloid manifold with curvature c_out.\n    \"\"\"\n    return hrc(x, jax.nn.swish, c_in, c_out, eps)\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.hrc_gelu","title":"hyperbolix.nn_layers.hrc_gelu","text":"<pre><code>hrc_gelu(\n    x: Float[Array, \"... dim_plus_1\"],\n    c_in: float,\n    c_out: float,\n    eps: float = 1e-07,\n) -&gt; Float[Array, \"... dim_plus_1\"]\n</code></pre> <p>HRC with GELU activation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array of shape (..., dim+1)</code> <p>Input point(s) on the hyperboloid manifold with curvature c_in.</p> required <code>c_in</code> <code>float</code> <p>Input curvature parameter (must be positive).</p> required <code>c_out</code> <code>float</code> <p>Output curvature parameter (must be positive).</p> required <code>eps</code> <code>float</code> <p>Small value for numerical stability (default: 1e-7).</p> <code>1e-07</code> <p>Returns:</p> Name Type Description <code>y</code> <code>Array of shape (..., dim+1)</code> <p>Output point(s) on the hyperboloid manifold with curvature c_out.</p> Source code in <code>hyperbolix/nn_layers/hyperboloid_activations.py</code> <pre><code>def hrc_gelu(\n    x: Float[Array, \"... dim_plus_1\"],\n    c_in: float,\n    c_out: float,\n    eps: float = 1e-7,\n) -&gt; Float[Array, \"... dim_plus_1\"]:\n    \"\"\"HRC with GELU activation.\n\n    Parameters\n    ----------\n    x : Array of shape (..., dim+1)\n        Input point(s) on the hyperboloid manifold with curvature c_in.\n    c_in : float\n        Input curvature parameter (must be positive).\n    c_out : float\n        Output curvature parameter (must be positive).\n    eps : float, optional\n        Small value for numerical stability (default: 1e-7).\n\n    Returns\n    -------\n    y : Array of shape (..., dim+1)\n        Output point(s) on the hyperboloid manifold with curvature c_out.\n    \"\"\"\n    return hrc(x, jax.nn.gelu, c_in, c_out, eps)\n</code></pre>"},{"location":"api-reference/nn-layers/#activation-examples","title":"Activation Examples","text":"<p>Curvature-Preserving Activation:</p> <pre><code>from hyperbolix.nn_layers import hyp_relu, hyp_gelu\nfrom hyperbolix.manifolds import hyperboloid\nimport jax.numpy as jnp\n\n# Points on hyperboloid (ambient coordinates)\nx = jax.random.normal(jax.random.PRNGKey(0), (10, 5))\nx_ambient = jnp.concatenate([\n    jnp.sqrt(jnp.sum(x**2, axis=-1, keepdims=True) + 1.0),\n    x\n], axis=-1)\n\n# Apply hyperbolic ReLU (curvature preserving)\noutput = hyp_relu(x_ambient, c=1.0)\nprint(output.shape)  # (10, 6) - same shape\n\n# Verify manifold constraint\nconstraint = -output[:, 0]**2 + jnp.sum(output[:, 1:]**2, axis=-1)\nprint(jnp.allclose(constraint, -1.0, atol=1e-5))  # True\n\n# Use GELU instead\noutput_gelu = hyp_gelu(x_ambient, c=1.0)\n</code></pre> <p>Curvature-Changing Activation:</p> <pre><code>from hyperbolix.nn_layers import hrc_relu\n\n# Transform from curvature 1.0 to curvature 2.0\noutput = hrc_relu(x_ambient, c_in=1.0, c_out=2.0)\n\n# Verify new manifold constraint (c=2.0)\nconstraint = -output[:, 0]**2 + jnp.sum(output[:, 1:]**2, axis=-1)\nprint(jnp.allclose(constraint, -1.0/2.0, atol=1e-5))  # True\n</code></pre> <p>How Activations Work</p> <p>Hyperbolic activations follow the HRC pattern:</p> <ol> <li>Extract space components <code>x_s = x[..., 1:]</code></li> <li>Apply activation to space: <code>y_s = activation(x_s)</code></li> <li>Scale for curvature change: <code>y_s = sqrt(c_in/c_out) * y_s</code></li> <li>Reconstruct time: <code>y_t = sqrt(||y_s||^2 + 1/c_out)</code></li> </ol> <p>This avoids expensive exp/log maps while preserving geometry and enabling flexible curvature transformations.</p>"},{"location":"api-reference/nn-layers/#helper-functions","title":"Helper Functions","text":"<p>Utility functions for manifold operations in neural networks.</p>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.helpers","title":"hyperbolix.nn_layers.helpers","text":"<p>Helper functions for hyperbolic neural network layers.</p>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.helpers.compute_mlr_poincare_pp","title":"compute_mlr_poincare_pp","text":"<pre><code>compute_mlr_poincare_pp(\n    x: Float[Array, \"batch in_dim\"],\n    z: Float[Array, \"out_dim in_dim\"],\n    r: Float[Array, \"out_dim 1\"],\n    c: float,\n    clamping_factor: float,\n    smoothing_factor: float,\n    min_enorm: float = 1e-15,\n) -&gt; Float[Array, \"batch out_dim\"]\n</code></pre> <p>Compute 'Hyperbolic Neural Networks ++' multinomial linear regression.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array(batch, in_dim)</code> <p>PoincareBall point(s)</p> required <code>z</code> <code>Array(out_dim, in_dim)</code> <p>Hyperplane tangent normal(s) lying in the tangent space at the origin</p> required <code>r</code> <code>Array(out_dim, 1)</code> <p>Hyperplane PoincareBall translation(s) defined by the scalar r and z</p> required <code>c</code> <code>float</code> <p>Manifold curvature</p> required <code>clamping_factor</code> <code>float</code> <p>Clamping value for the output</p> required <code>smoothing_factor</code> <code>float</code> <p>Smoothing factor for the output</p> required <code>min_enorm</code> <code>float</code> <p>Minimum norm to avoid division by zero (default: 1e-15)</p> <code>1e-15</code> <p>Returns:</p> Name Type Description <code>res</code> <code>Array(batch, out_dim)</code> <p>The multinomial linear regression score(s) of x with respect to the linear model(s) defined by z and r.</p> References <p>Shimizu Ryohei, Yusuke Mukuta, and Tatsuya Harada. \"Hyperbolic neural networks++.\"     arXiv preprint arXiv:2006.08210 (2020).</p> Source code in <code>hyperbolix/nn_layers/helpers.py</code> <pre><code>def compute_mlr_poincare_pp(\n    x: Float[Array, \"batch in_dim\"],\n    z: Float[Array, \"out_dim in_dim\"],\n    r: Float[Array, \"out_dim 1\"],\n    c: float,\n    clamping_factor: float,\n    smoothing_factor: float,\n    min_enorm: float = 1e-15,\n) -&gt; Float[Array, \"batch out_dim\"]:\n    \"\"\"\n    Compute 'Hyperbolic Neural Networks ++' multinomial linear regression.\n\n    Parameters\n    ----------\n    x : Array (batch, in_dim)\n        PoincareBall point(s)\n    z : Array (out_dim, in_dim)\n        Hyperplane tangent normal(s) lying in the tangent space at the origin\n    r : Array (out_dim, 1)\n        Hyperplane PoincareBall translation(s) defined by the scalar r and z\n    c : float\n        Manifold curvature\n    clamping_factor : float\n        Clamping value for the output\n    smoothing_factor : float\n        Smoothing factor for the output\n    min_enorm : float\n        Minimum norm to avoid division by zero (default: 1e-15)\n\n    Returns\n    -------\n    res : Array (batch, out_dim)\n        The multinomial linear regression score(s) of x with respect to the linear model(s) defined by z and r.\n\n    References\n    ----------\n    Shimizu Ryohei, Yusuke Mukuta, and Tatsuya Harada. \"Hyperbolic neural networks++.\"\n        arXiv preprint arXiv:2006.08210 (2020).\n    \"\"\"\n    sqrt_c = jnp.sqrt(c)\n    sqrt_c2r = 2 * sqrt_c * r.T  # (1, out_dim)\n    z_norm = jnp.linalg.norm(z, ord=2, axis=-1, keepdims=True).clip(min=min_enorm)  # (out_dim, 1)\n\n    # Compute conformal factor (lambda_x)\n    lambda_x = safe_conformal_factor(x, c)  # (batch, 1)\n\n    z_unitx = jnp.einsum(\"bi,oi-&gt;bo\", x, z / z_norm)  # (batch, out_dim)\n    asinh_arg = (1 - lambda_x) * sinh(sqrt_c2r) + sqrt_c * lambda_x * cosh(sqrt_c2r) * z_unitx  # (batch, out_dim)\n\n    # Improve performance by smoothly clamping the input of asinh() to approximately the range of ...\n    # ... [-16*clamping_factor, 16*clamping_factor] for float32\n    # ... [-36*clamping_factor, 36*clamping_factor] for float64\n    eps = jnp.finfo(jnp.float32).eps if x.dtype == jnp.float32 else jnp.finfo(jnp.float64).eps\n    clamp = clamping_factor * float(math.log(2 / eps))\n    asinh_arg = smooth_clamp(asinh_arg, -clamp, clamp, smoothing_factor)  # (batch, out_dim)\n    signed_dist2hyp = asinh(asinh_arg) / sqrt_c  # (batch, out_dim)\n    res = 2 * z_norm.T * signed_dist2hyp  # (batch, out_dim)\n    return res\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.helpers.compute_mlr_hyperboloid","title":"compute_mlr_hyperboloid","text":"<pre><code>compute_mlr_hyperboloid(\n    x: Float[Array, \"batch in_dim\"],\n    z: Float[Array, \"out_dim in_dim_minus_1\"],\n    r: Float[Array, \"out_dim 1\"],\n    c: float,\n    clamping_factor: float,\n    smoothing_factor: float,\n    min_enorm: float = 1e-15,\n) -&gt; Float[Array, \"batch out_dim\"]\n</code></pre> <p>Compute 'Fully Hyperbolic Convolutional Neural Networks' multinomial linear regression.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array(batch, in_dim)</code> <p>Hyperboloid point(s)</p> required <code>z</code> <code>Array(out_dim, in_dim - 1)</code> <p>Hyperplane tangent normal(s) in tangent space at origin (time coordinate omitted)</p> required <code>r</code> <code>Array(out_dim, 1)</code> <p>Hyperplane Hyperboloid translation(s) defined by the scalar r and z</p> required <code>c</code> <code>float</code> <p>Manifold curvature</p> required <code>clamping_factor</code> <code>float</code> <p>Clamping value for the output</p> required <code>smoothing_factor</code> <code>float</code> <p>Smoothing factor for the output</p> required <code>min_enorm</code> <code>float</code> <p>Minimum norm to avoid division by zero (default: 1e-15)</p> <code>1e-15</code> <p>Returns:</p> Name Type Description <code>res</code> <code>Array(batch, out_dim)</code> <p>The multinomial linear regression score(s) of x with respect to the linear model(s) defined by z and r.</p> References <p>Ahmad Bdeir, Kristian Schwethelm, and Niels Landwehr. \"Fully hyperbolic convolutional neural networks for computer vision.\"     arXiv preprint arXiv:2303.15919 (2023).</p> Source code in <code>hyperbolix/nn_layers/helpers.py</code> <pre><code>def compute_mlr_hyperboloid(\n    x: Float[Array, \"batch in_dim\"],\n    z: Float[Array, \"out_dim in_dim_minus_1\"],\n    r: Float[Array, \"out_dim 1\"],\n    c: float,\n    clamping_factor: float,\n    smoothing_factor: float,\n    min_enorm: float = 1e-15,\n) -&gt; Float[Array, \"batch out_dim\"]:\n    \"\"\"\n    Compute 'Fully Hyperbolic Convolutional Neural Networks' multinomial linear regression.\n\n    Parameters\n    ----------\n    x : Array (batch, in_dim)\n        Hyperboloid point(s)\n    z : Array (out_dim, in_dim-1)\n        Hyperplane tangent normal(s) in tangent space at origin (time coordinate omitted)\n    r : Array (out_dim, 1)\n        Hyperplane Hyperboloid translation(s) defined by the scalar r and z\n    c : float\n        Manifold curvature\n    clamping_factor : float\n        Clamping value for the output\n    smoothing_factor : float\n        Smoothing factor for the output\n    min_enorm : float\n        Minimum norm to avoid division by zero (default: 1e-15)\n\n    Returns\n    -------\n    res : Array (batch, out_dim)\n        The multinomial linear regression score(s) of x with respect to the linear model(s) defined by z and r.\n\n    References\n    ----------\n    Ahmad Bdeir, Kristian Schwethelm, and Niels Landwehr. \"Fully hyperbolic convolutional neural networks for computer vision.\"\n        arXiv preprint arXiv:2303.15919 (2023).\n    \"\"\"\n    sqrt_c = jnp.sqrt(c)\n    sqrt_cr = sqrt_c * r.T  # (1, out_dim)\n    z_norm = jnp.linalg.norm(z, ord=2, axis=-1, keepdims=True).clip(min=min_enorm).T  # (1, out_dim)\n    x0 = x[:, 0:1]  # (batch, 1) - time coordinate\n    x_rem = x[:, 1:]  # (batch, in_dim-1) - space coordinates\n    zx_rem = jnp.einsum(\"bi,oi-&gt;bo\", x_rem, z)  # (batch, out_dim)\n    alpha = -x0 * sinh(sqrt_cr) * z_norm + cosh(sqrt_cr) * zx_rem  # (batch, out_dim)\n    asinh_arg = sqrt_c * alpha / z_norm  # (batch, out_dim)\n\n    # Improve performance by smoothly clamping the input of asinh() to approximately the range of ...\n    # ... [-16*clamping_factor, 16*clamping_factor] for float32\n    # ... [-36*clamping_factor, 36*clamping_factor] for float64\n    eps = jnp.finfo(jnp.float32).eps if x.dtype == jnp.float32 else jnp.finfo(jnp.float64).eps\n    clamp = clamping_factor * float(math.log(2 / eps))\n    asinh_arg = smooth_clamp(asinh_arg, -clamp, clamp, smoothing_factor)  # (batch, out_dim)\n    signed_dist2hyp = asinh(asinh_arg) / sqrt_c  # (batch, out_dim)\n    res = z_norm * signed_dist2hyp  # (batch, out_dim)\n    return res\n</code></pre>"},{"location":"api-reference/nn-layers/#hyperbolix.nn_layers.helpers.safe_conformal_factor","title":"safe_conformal_factor","text":"<pre><code>safe_conformal_factor(\n    x: Float[Array, ...], c: float\n) -&gt; Float[Array, ...]\n</code></pre> <p>Numerically stable conformal factor \u03bb(x) = 2 / (1 - c||x||\u00b2).</p> <p>Mirrors the manifold implementation to avoid division by values near zero when points approach the Poincar\u00e9 ball boundary.</p> <p>Args:     x: Poincar\u00e9 ball point(s), shape (..., dim)     c: Manifold curvature</p> <p>Returns:     Conformal factor, shape (..., 1)</p> Source code in <code>hyperbolix/nn_layers/helpers.py</code> <pre><code>def safe_conformal_factor(\n    x: Float[Array, \"...\"],\n    c: float,\n) -&gt; Float[Array, \"...\"]:\n    \"\"\"Numerically stable conformal factor \u03bb(x) = 2 / (1 - c||x||\u00b2).\n\n    Mirrors the manifold implementation to avoid division by values near zero when\n    points approach the Poincar\u00e9 ball boundary.\n\n    Args:\n        x: Poincar\u00e9 ball point(s), shape (..., dim)\n        c: Manifold curvature\n\n    Returns:\n        Conformal factor, shape (..., 1)\n    \"\"\"\n    dtype = x.dtype\n    c_arr = jnp.asarray(c, dtype=dtype)\n    sqrt_c = jnp.sqrt(c_arr)\n    max_norm_eps = jnp.asarray(_get_max_norm_eps(x), dtype=dtype)\n    x_norm_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n    denom_min = 2 * sqrt_c * max_norm_eps - c_arr * max_norm_eps**2\n    denom = jnp.maximum(jnp.asarray(1.0, dtype=dtype) - c_arr * x_norm_sq, denom_min)\n    return 2.0 / denom\n</code></pre>"},{"location":"api-reference/nn-layers/#building-models","title":"Building Models","text":"<p>Example of a complete hyperbolic neural network:</p> <pre><code>from flax import nnx\nfrom hyperbolix.nn_layers import HypLinearPoincare, hyp_relu\nfrom hyperbolix.manifolds import poincare\nimport jax\nimport jax.numpy as jnp\n\nclass HyperbolicNN(nnx.Module):\n    def __init__(self, rngs):\n        self.layer1 = HypLinearPoincare(\n            manifold_module=poincare,\n            in_dim=784,  # MNIST flattened\n            out_dim=256,\n            rngs=rngs\n        )\n        self.layer2 = HypLinearPoincare(\n            manifold_module=poincare,\n            in_dim=256,\n            out_dim=128,\n            rngs=rngs\n        )\n        self.layer3 = HypLinearPoincare(\n            manifold_module=poincare,\n            in_dim=128,\n            out_dim=10,\n            rngs=rngs\n        )\n\n    def __call__(self, x, c=1.0):\n        # x: (batch, 784) on Poincar\u00e9 ball\n        x = self.layer1(x, c)\n        x = jax.vmap(lambda xi: hyp_relu(xi, c))(x)\n\n        x = self.layer2(x, c)\n        x = jax.vmap(lambda xi: hyp_relu(xi, c))(x)\n\n        x = self.layer3(x, c)\n        return x\n\n# Create and use model\nmodel = HyperbolicNN(rngs=nnx.Rngs(0))\n\n# Input data (projected to Poincar\u00e9 ball)\nx = jax.random.normal(nnx.Rngs(1).params(), (32, 784)) * 0.1\nx_proj = jax.vmap(poincare.proj, in_axes=(0, None))(x, 1.0)\n\noutput = model(x_proj, c=1.0)\nprint(output.shape)  # (32, 10)\n</code></pre>"},{"location":"api-reference/nn-layers/#references","title":"References","text":"<p>The neural network layers implement methods from:</p> <ul> <li>Ganea et al. (2018): \"Hyperbolic Neural Networks\" - Poincar\u00e9 linear layers and activations</li> <li>Shimizu et al. (2020): \"Hyperbolic Neural Networks++\" - Enhanced Poincar\u00e9 operations</li> <li>Bdeir et al. (2023): \"Fully Hyperbolic Convolutional Neural Networks for Computer Vision\" - HCat-based convolutions (<code>HypConv2DHyperboloid</code>)</li> <li>Chen et al. (2022): \"Fully Hyperbolic Neural Networks\" - FHCNN linear layers</li> <li>LResNet (2023): \"Lorentzian ResNet\" - HRC-based convolutions (<code>LorentzConv2D</code>)</li> <li>Hypformer: \"Hyperbolic Transformers\" - HTC/HRC components with curvature-change support</li> </ul>"},{"location":"api-reference/nn-layers/#key-theoretical-connections","title":"Key Theoretical Connections","text":"<ul> <li>HL (Hyperbolic Layer) from LResNet \u2261 HRC (Hyperbolic Regularization Component) from Hypformer</li> <li>Both apply Euclidean operations to spatial components and reconstruct time using the Lorentz constraint</li> <li><code>LorentzConv2D</code> is a specific instance of <code>hrc()</code> where <code>f_r</code> is a 2D convolution</li> </ul> <p>See also:</p> <ul> <li>Manifolds API: Underlying geometric operations</li> <li>Optimizers API: Training with Riemannian optimization</li> <li>Training Workflows: Complete training examples</li> </ul>"},{"location":"api-reference/optimizers/","title":"Optimizers API","text":"<p>Riemannian optimization algorithms for training neural networks with hyperbolic parameters.</p>"},{"location":"api-reference/optimizers/#overview","title":"Overview","text":"<p>Hyperbolix provides two Riemannian optimizers that extend standard Euclidean optimizers to manifold-valued parameters:</p> <ul> <li>Riemannian SGD (RSGD): Stochastic gradient descent with momentum</li> <li>Riemannian Adam (RAdam): Adaptive learning rates with moment transport</li> </ul> <p>Both optimizers:</p> <ul> <li>Follow the standard Optax <code>GradientTransformation</code> interface</li> <li>Automatically detect manifold parameters via metadata</li> <li>Support mixed Euclidean/Riemannian parameter optimization</li> <li>Are compatible with <code>nnx.Optimizer</code> wrapper</li> </ul>"},{"location":"api-reference/optimizers/#riemannian-sgd","title":"Riemannian SGD","text":""},{"location":"api-reference/optimizers/#hyperbolix.optim.riemannian_sgd","title":"hyperbolix.optim.riemannian_sgd","text":"<p>Riemannian SGD optimizer for JAX/Flax NNX.</p> <p>This module implements Riemannian Stochastic Gradient Descent (RSGD) as a standard Optax GradientTransformation. It automatically detects manifold parameters via metadata and applies appropriate Riemannian operations, while treating Euclidean parameters with standard SGD.</p> <p>The optimizer supports: - Momentum with parallel transport - Both exponential map (exact) and retraction (first-order approximation) - Mixed Euclidean/Riemannian parameter optimization</p> <p>Algorithm (for manifold parameters):     1. Convert Euclidean gradient to Riemannian gradient: grad = manifold.egrad2rgrad(grad, param, c)     2. Update momentum: m = momentum * m + grad     3. Move on manifold: new_param = manifold.expmap(-lr * m, param, c)  # or retraction     4. Transport momentum: m = manifold.ptransp(m, param, new_param, c)</p> <p>For Euclidean parameters, standard SGD is applied.</p> References <p>B\u00e9cigneul, Gary, and Octavian-Eugen Ganea. \"Riemannian adaptive optimization methods.\"     arXiv preprint arXiv:1810.00760 (2018).</p>"},{"location":"api-reference/optimizers/#hyperbolix.optim.riemannian_sgd.RSGDState","title":"RSGDState","text":"<p>               Bases: <code>NamedTuple</code></p> <p>State for Riemannian SGD optimizer.</p> <p>Attributes:</p> Name Type Description <code>momentum</code> <code>Any</code> <p>Pytree of momentum terms, same structure as parameters</p> <code>count</code> <code>Array</code> <p>Step count for schedule handling</p>"},{"location":"api-reference/optimizers/#hyperbolix.optim.riemannian_sgd.riemannian_sgd","title":"riemannian_sgd","text":"<pre><code>riemannian_sgd(\n    learning_rate: float | Schedule,\n    momentum: float = 0.0,\n    use_expmap: bool = True,\n) -&gt; optax.GradientTransformation\n</code></pre> <p>Create a Riemannian SGD optimizer as an Optax GradientTransformation.</p> <p>This optimizer automatically detects manifold parameters via metadata and applies Riemannian operations (egrad2rgrad, expmap/retraction, parallel transport), while treating Euclidean parameters with standard SGD.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float or Schedule</code> <p>Learning rate (static or scheduled)</p> required <code>momentum</code> <code>float</code> <p>Momentum coefficient (0 for no momentum, typically 0.9)</p> <code>0.0</code> <code>use_expmap</code> <code>bool</code> <p>If True, use exponential map (exact geodesic). If False, use retraction (first-order approximation, faster).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>optimizer</code> <code>GradientTransformation</code> <p>An Optax GradientTransformation that can be used with nnx.Optimizer</p> Example <p>import jax from flax import nnx from hyperbolix.optim import riemannian_sgd from hyperbolix.nn_layers import HypLinearPoincare from hyperbolix.manifolds import poincare</p> Notes <ul> <li>Compatible with Optax combinators (optax.chain, schedules, etc.)</li> <li>Momentum is parallel transported for manifold parameters</li> <li>Works seamlessly with nnx.Optimizer wrapper</li> <li>Parameters stay on manifold after updates (via expmap/retraction + projection)</li> </ul> Source code in <code>hyperbolix/optim/riemannian_sgd.py</code> <pre><code>def riemannian_sgd(\n    learning_rate: float | optax.Schedule,\n    momentum: float = 0.0,\n    use_expmap: bool = True,\n) -&gt; optax.GradientTransformation:\n    \"\"\"Create a Riemannian SGD optimizer as an Optax GradientTransformation.\n\n    This optimizer automatically detects manifold parameters via metadata and\n    applies Riemannian operations (egrad2rgrad, expmap/retraction, parallel transport),\n    while treating Euclidean parameters with standard SGD.\n\n    Parameters\n    ----------\n    learning_rate : float or optax.Schedule\n        Learning rate (static or scheduled)\n    momentum : float, default=0.0\n        Momentum coefficient (0 for no momentum, typically 0.9)\n    use_expmap : bool, default=True\n        If True, use exponential map (exact geodesic).\n        If False, use retraction (first-order approximation, faster).\n\n    Returns\n    -------\n    optimizer : optax.GradientTransformation\n        An Optax GradientTransformation that can be used with nnx.Optimizer\n\n    Example\n    -------\n    &gt;&gt;&gt; import jax\n    &gt;&gt;&gt; from flax import nnx\n    &gt;&gt;&gt; from hyperbolix.optim import riemannian_sgd\n    &gt;&gt;&gt; from hyperbolix.nn_layers import HypLinearPoincare\n    &gt;&gt;&gt; from hyperbolix.manifolds import poincare\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Create model with manifold parameters\n    &gt;&gt;&gt; layer = HypLinearPoincare(poincare, 10, 5, rngs=nnx.Rngs(0))\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Create Riemannian optimizer\n    &gt;&gt;&gt; tx = riemannian_sgd(learning_rate=0.01, momentum=0.9, use_expmap=True)\n    &gt;&gt;&gt; optimizer = nnx.Optimizer(layer, tx, wrt=nnx.Param)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Training step\n    &gt;&gt;&gt; def loss_fn(model, x):\n    ...     y = model(x, c=1.0)\n    ...     return jnp.sum(y ** 2)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; x = jax.random.normal(jax.random.key(1), (32, 10))\n    &gt;&gt;&gt; grads = nnx.grad(loss_fn)(layer, x)\n    &gt;&gt;&gt; optimizer.update(grads)  # Automatically handles manifold parameters\n\n    Notes\n    -----\n    - Compatible with Optax combinators (optax.chain, schedules, etc.)\n    - Momentum is parallel transported for manifold parameters\n    - Works seamlessly with nnx.Optimizer wrapper\n    - Parameters stay on manifold after updates (via expmap/retraction + projection)\n    \"\"\"\n\n    def init_fn(params: Any) -&gt; RSGDState:\n        \"\"\"Initialize optimizer state.\n\n        Parameters\n        ----------\n        params : Any\n            Pytree of parameters\n\n        Returns\n        -------\n        state : RSGDState\n            Initial optimizer state with zero momentum\n        \"\"\"\n        momentum = tree_util.tree_map(lambda p: jnp.zeros_like(p), params)\n        count = jnp.zeros([], jnp.int32)\n        return RSGDState(momentum=momentum, count=count)\n\n    def update_fn(\n        updates: Any,\n        state: RSGDState,\n        params: Any | None = None,\n    ) -&gt; tuple[Any, RSGDState]:\n        \"\"\"Apply Riemannian SGD update.\n\n        Parameters\n        ----------\n        updates : Any\n            Pytree of gradients (typically from nnx.grad)\n        state : RSGDState\n            Current optimizer state\n        params : Any, optional\n            Pytree of parameters (required for Riemannian operations)\n\n        Returns\n        -------\n        updates : Any\n            Pytree of parameter updates (new_param - old_param)\n        new_state : RSGDState\n            Updated optimizer state\n\n        Raises\n        ------\n        ValueError\n            If params is None (required for Riemannian operations)\n        \"\"\"\n        if params is None:\n            raise ValueError(\"Riemannian SGD requires params to be provided in update step\")\n\n        # Increment step count\n        count_inc = state.count + 1\n\n        # Get learning rate (handle both static value and schedule)\n        if callable(learning_rate):\n            lr_value = learning_rate(count_inc)\n        else:\n            lr_value = cast(float, learning_rate)\n        lr = jnp.asarray(lr_value)\n\n        # We need to traverse the params pytree to access Variable metadata\n        # while simultaneously mapping over the gradients and momentum\n        def update_single_leaf(grad_value, mom_value, param_variable):\n            \"\"\"Update a single leaf parameter.\n\n            Note: param_variable may be an nnx.Variable or a plain array depending on\n            how nnx.Optimizer structures the pytree.\n            \"\"\"\n            # Check if this parameter has manifold metadata\n            manifold_info = None\n            param_value = grad_value  # Default: use grad structure\n\n            if hasattr(param_variable, \"_var_metadata\"):\n                # param_variable is an nnx.Variable with potential metadata\n                manifold_info = get_manifold_info(param_variable)\n                param_value = param_variable[...] if isinstance(param_variable, nnx.Variable) else param_variable\n\n            if manifold_info is not None:\n                # Riemannian parameter update\n                manifold_module, c = manifold_info\n\n                # 1. Convert Euclidean gradient to Riemannian gradient\n                rgrad = manifold_module.egrad2rgrad(grad_value, param_value, c)\n\n                # 2. Update momentum\n                new_momentum = momentum * mom_value + rgrad\n\n                # 3. Move on manifold using exponential map or retraction\n                lr_cast = lr.astype(new_momentum.dtype)\n                direction = -lr_cast * new_momentum\n                if use_expmap:\n                    new_param_value = manifold_module.expmap(direction, param_value, c)\n                else:\n                    new_param_value = manifold_module.retraction(direction, param_value, c)\n\n                # 4. Parallel transport momentum to new location\n                if momentum &gt; 0.0:\n                    transported_momentum = manifold_module.ptransp(new_momentum, param_value, new_param_value, c)\n                else:\n                    transported_momentum = new_momentum\n\n                # Return the update (new - old) and transported momentum\n                param_update = new_param_value - param_value\n                return (param_update, transported_momentum)\n\n            else:\n                # Euclidean parameter update (standard SGD with momentum)\n                # momentum update: m = momentum * m + grad\n                new_momentum = momentum * mom_value + grad_value\n\n                # parameter update: param = param - lr * m\n                lr_cast = lr.astype(new_momentum.dtype)\n                param_update = -lr_cast * new_momentum\n\n                return (param_update, new_momentum)\n\n        # Apply update to all parameters\n        # Use tree_map with is_leaf to handle nnx.Variable nodes correctly\n        results = tree_util.tree_map(\n            update_single_leaf,\n            updates,  # gradients (arrays)\n            state.momentum,  # old momentum (arrays)\n            params,  # parameters (Variables with metadata)\n            is_leaf=lambda x: isinstance(x, nnx.Variable),\n        )\n\n        # Separate the tuple results\n        # Each leaf in results is a tuple (param_update, new_momentum)\n        # Use is_leaf to treat tuples as leaves so we can extract their components\n        param_updates = tree_util.tree_map(\n            lambda r: r[0],\n            results,\n            is_leaf=lambda x: isinstance(x, tuple),\n        )\n        new_momentum = tree_util.tree_map(\n            lambda r: r[1],\n            results,\n            is_leaf=lambda x: isinstance(x, tuple),\n        )\n\n        new_state = RSGDState(momentum=new_momentum, count=count_inc)\n\n        return param_updates, new_state\n\n    return optax.GradientTransformation(init_fn, cast(Any, update_fn))\n</code></pre>"},{"location":"api-reference/optimizers/#hyperbolix.optim.riemannian_sgd.riemannian_sgd--create-model-with-manifold-parameters","title":"Create model with manifold parameters","text":"<p>layer = HypLinearPoincare(poincare, 10, 5, rngs=nnx.Rngs(0))</p>"},{"location":"api-reference/optimizers/#hyperbolix.optim.riemannian_sgd.riemannian_sgd--create-riemannian-optimizer","title":"Create Riemannian optimizer","text":"<p>tx = riemannian_sgd(learning_rate=0.01, momentum=0.9, use_expmap=True) optimizer = nnx.Optimizer(layer, tx, wrt=nnx.Param)</p>"},{"location":"api-reference/optimizers/#hyperbolix.optim.riemannian_sgd.riemannian_sgd--training-step","title":"Training step","text":"<p>def loss_fn(model, x): ...     y = model(x, c=1.0) ...     return jnp.sum(y ** 2)</p> <p>x = jax.random.normal(jax.random.key(1), (32, 10)) grads = nnx.grad(loss_fn)(layer, x) optimizer.update(grads)  # Automatically handles manifold parameters</p>"},{"location":"api-reference/optimizers/#example","title":"Example","text":"<pre><code>from flax import nnx\nfrom hyperbolix.optim import riemannian_sgd\nfrom hyperbolix.nn_layers import HypLinearPoincare\nimport hyperbolix.manifolds.poincare as poincare\n\n# Create model with hyperbolic parameters\nmodel = HypLinearPoincare(\n    manifold_module=poincare,\n    in_dim=32,\n    out_dim=16,\n    rngs=nnx.Rngs(0)\n)\n\n# Create Riemannian SGD optimizer\noptimizer = nnx.Optimizer(\n    model,\n    riemannian_sgd(learning_rate=0.01, momentum=0.9)\n)\n\n# Training step\ndef train_step(model, optimizer, x, y):\n    def loss_fn(model):\n        pred = model(x, c=1.0)\n        return jnp.mean((pred - y) ** 2)\n\n    loss, grads = nnx.value_and_grad(loss_fn)(model)\n    optimizer.update(grads)\n\n    return loss\n</code></pre>"},{"location":"api-reference/optimizers/#riemannian-adam","title":"Riemannian Adam","text":""},{"location":"api-reference/optimizers/#hyperbolix.optim.riemannian_adam","title":"hyperbolix.optim.riemannian_adam","text":"<p>Riemannian Adam optimizer for JAX/Flax NNX.</p> <p>This module implements Riemannian Adam as a standard Optax GradientTransformation. It automatically detects manifold parameters via metadata and applies appropriate Riemannian operations (adaptive learning rates on manifolds), while treating Euclidean parameters with standard Adam.</p> <p>The optimizer supports: - Adaptive learning rates with first and second moment estimation - Parallel transport of first moments (second moments follow PyTorch scalar update) - Both exponential map (exact) and retraction (first-order approximation) - Mixed Euclidean/Riemannian parameter optimization</p> <p>Algorithm (for manifold parameters):     1. Convert Euclidean gradient to Riemannian gradient: grad = manifold.egrad2rgrad(grad, param, c)     2. Update first moment: m1 = beta1 * m1 + (1 - beta1) * grad     3. Update second moment: m2 = beta2 * m2 + (1 - beta2) * _param     4. Bias correction: m1_hat = m1 / (1 - beta1^t), m2_hat = m2 / (1 - beta2^t)     5. Compute direction: direction = m1_hat / (sqrt(m2_hat) + eps)     6. Move on manifold: new_param = manifold.expmap(-lr * direction, param, c)     7. Transport moments: m1 = manifold.ptransp(m1, param, new_param, c)                           # m2 accumulated via tangent inner product, no transport <p>For Euclidean parameters, standard Adam is applied.</p> References <p>B\u00e9cigneul, Gary, and Octavian-Eugen Ganea. \"Riemannian adaptive optimization methods.\"     arXiv preprint arXiv:1810.00760 (2018). Kingma, Diederik P., and Jimmy Ba. \"Adam: A method for stochastic optimization.\"     arXiv preprint arXiv:1412.6980 (2014).</p>"},{"location":"api-reference/optimizers/#hyperbolix.optim.riemannian_adam.RAdamState","title":"RAdamState","text":"<p>               Bases: <code>NamedTuple</code></p> <p>State for Riemannian Adam optimizer.</p> <p>Attributes:</p> Name Type Description <code>m1</code> <code>Any</code> <p>Pytree of first moment estimates (exponential moving average of gradients)</p> <code>m2</code> <code>Any</code> <p>Pytree of second moment estimates (exponential moving average of squared gradients)</p> <code>count</code> <code>Array</code> <p>Step count for bias correction</p>"},{"location":"api-reference/optimizers/#hyperbolix.optim.riemannian_adam.riemannian_adam","title":"riemannian_adam","text":"<pre><code>riemannian_adam(\n    learning_rate: float | Schedule,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-08,\n    use_expmap: bool = True,\n) -&gt; optax.GradientTransformation\n</code></pre> <p>Create a Riemannian Adam optimizer as an Optax GradientTransformation.</p> <p>This optimizer automatically detects manifold parameters via metadata and applies Riemannian operations with adaptive learning rates, while treating Euclidean parameters with standard Adam.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float or Schedule</code> <p>Learning rate (static or scheduled)</p> required <code>beta1</code> <code>float</code> <p>Exponential decay rate for first moment estimates</p> <code>0.9</code> <code>beta2</code> <code>float</code> <p>Exponential decay rate for second moment estimates</p> <code>0.999</code> <code>eps</code> <code>float</code> <p>Small constant for numerical stability</p> <code>1e-8</code> <code>use_expmap</code> <code>bool</code> <p>If True, use exponential map (exact geodesic). If False, use retraction (first-order approximation, faster).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>optimizer</code> <code>GradientTransformation</code> <p>An Optax GradientTransformation that can be used with nnx.Optimizer</p> Example <p>import jax from flax import nnx from hyperbolix.optim import riemannian_adam from hyperbolix.nn_layers import HypLinearPoincare from hyperbolix.manifolds import poincare</p> Notes <ul> <li>Compatible with Optax combinators (optax.chain, schedules, etc.)</li> <li>First moments are parallel transported for manifold parameters (matching PyTorch behaviour)</li> <li>Second moments follow Geoopt/PyTorch: accumulated as tangent inner products without transport</li> <li>Works seamlessly with nnx.Optimizer wrapper</li> <li>Parameters stay on manifold after updates (via expmap/retraction + projection)</li> </ul> Source code in <code>hyperbolix/optim/riemannian_adam.py</code> <pre><code>def riemannian_adam(\n    learning_rate: float | optax.Schedule,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n    use_expmap: bool = True,\n) -&gt; optax.GradientTransformation:\n    \"\"\"Create a Riemannian Adam optimizer as an Optax GradientTransformation.\n\n    This optimizer automatically detects manifold parameters via metadata and\n    applies Riemannian operations with adaptive learning rates, while treating\n    Euclidean parameters with standard Adam.\n\n    Parameters\n    ----------\n    learning_rate : float or optax.Schedule\n        Learning rate (static or scheduled)\n    beta1 : float, default=0.9\n        Exponential decay rate for first moment estimates\n    beta2 : float, default=0.999\n        Exponential decay rate for second moment estimates\n    eps : float, default=1e-8\n        Small constant for numerical stability\n    use_expmap : bool, default=True\n        If True, use exponential map (exact geodesic).\n        If False, use retraction (first-order approximation, faster).\n\n    Returns\n    -------\n    optimizer : optax.GradientTransformation\n        An Optax GradientTransformation that can be used with nnx.Optimizer\n\n    Example\n    -------\n    &gt;&gt;&gt; import jax\n    &gt;&gt;&gt; from flax import nnx\n    &gt;&gt;&gt; from hyperbolix.optim import riemannian_adam\n    &gt;&gt;&gt; from hyperbolix.nn_layers import HypLinearPoincare\n    &gt;&gt;&gt; from hyperbolix.manifolds import poincare\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Create model with manifold parameters\n    &gt;&gt;&gt; layer = HypLinearPoincare(poincare, 10, 5, rngs=nnx.Rngs(0))\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Create Riemannian Adam optimizer\n    &gt;&gt;&gt; tx = riemannian_adam(learning_rate=0.001, use_expmap=True)\n    &gt;&gt;&gt; optimizer = nnx.Optimizer(layer, tx, wrt=nnx.Param)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Training step\n    &gt;&gt;&gt; def loss_fn(model, x):\n    ...     y = model(x, c=1.0)\n    ...     return jnp.sum(y ** 2)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; x = jax.random.normal(jax.random.key(1), (32, 10))\n    &gt;&gt;&gt; grads = nnx.grad(loss_fn)(layer, x)\n    &gt;&gt;&gt; optimizer.update(grads)  # Automatically handles manifold parameters\n\n    Notes\n    -----\n    - Compatible with Optax combinators (optax.chain, schedules, etc.)\n    - First moments are parallel transported for manifold parameters (matching PyTorch behaviour)\n    - Second moments follow Geoopt/PyTorch: accumulated as tangent inner products without transport\n    - Works seamlessly with nnx.Optimizer wrapper\n    - Parameters stay on manifold after updates (via expmap/retraction + projection)\n    \"\"\"\n\n    def init_fn(params: Any) -&gt; RAdamState:\n        \"\"\"Initialize optimizer state.\n\n        Parameters\n        ----------\n        params : Any\n            Pytree of parameters\n\n        Returns\n        -------\n        state : RAdamState\n            Initial optimizer state with zero moments and count\n        \"\"\"\n        # Initialize moments as zeros with same structure as params\n        m1 = tree_util.tree_map(lambda p: jnp.zeros_like(p), params)\n        m2 = tree_util.tree_map(lambda p: jnp.zeros_like(p), params)\n        count = jnp.zeros([], jnp.int32)\n        return RAdamState(m1=m1, m2=m2, count=count)\n\n    def update_fn(\n        updates: Any,\n        state: RAdamState,\n        params: Any | None = None,\n    ) -&gt; tuple[Any, RAdamState]:\n        \"\"\"Apply Riemannian Adam update.\n\n        Parameters\n        ----------\n        updates : Any\n            Pytree of gradients (typically from nnx.grad)\n        state : RAdamState\n            Current optimizer state\n        params : Any, optional\n            Pytree of parameters (required for Riemannian operations)\n\n        Returns\n        -------\n        updates : Any\n            Pytree of parameter updates (new_param - old_param)\n        new_state : RAdamState\n            Updated optimizer state\n\n        Raises\n        ------\n        ValueError\n            If params is None (required for Riemannian operations)\n        \"\"\"\n        if params is None:\n            raise ValueError(\"Riemannian Adam requires params to be provided in update step\")\n\n        # Increment step count\n        count_inc = state.count + 1\n\n        # Get learning rate (handle both static value and schedule)\n        if callable(learning_rate):\n            lr_value = learning_rate(count_inc)\n        else:\n            lr_value = cast(float, learning_rate)\n        lr = jnp.asarray(lr_value)\n\n        # Bias correction terms\n        bias_correction1 = 1 - beta1**count_inc\n        bias_correction2 = 1 - beta2**count_inc\n\n        # Flatten the pytrees so we can process leaves in lock-step while preserving structure.\n        def is_variable_leaf(x):\n            return isinstance(x, nnx.Variable)\n\n        grad_leaves, treedef = tree_util.tree_flatten(updates, is_leaf=is_variable_leaf)\n        m1_leaves, treedef_m1 = tree_util.tree_flatten(state.m1, is_leaf=is_variable_leaf)\n        m2_leaves, treedef_m2 = tree_util.tree_flatten(state.m2, is_leaf=is_variable_leaf)\n        param_leaves, treedef_params = tree_util.tree_flatten(params, is_leaf=is_variable_leaf)\n\n        if not (treedef == treedef_m1 == treedef_m2 == treedef_params):\n            raise ValueError(\"Gradient, moment, and parameter pytrees must share the same structure.\")\n\n        param_update_leaves = []\n        new_m1_leaves = []\n        new_m2_leaves = []\n\n        for grad_value, m1_value, m2_value, param_variable in zip(\n            grad_leaves, m1_leaves, m2_leaves, param_leaves, strict=True\n        ):\n            # Default to treating parameters as Euclidean tensors\n            manifold_info = None\n            if hasattr(param_variable, \"_var_metadata\"):\n                manifold_info = get_manifold_info(param_variable)\n                param_value = param_variable[...] if isinstance(param_variable, nnx.Variable) else param_variable\n            else:\n                param_value = param_variable[...] if isinstance(param_variable, nnx.Variable) else param_variable\n\n            if manifold_info is not None:\n                manifold_module, c = manifold_info\n\n                rgrad = manifold_module.egrad2rgrad(grad_value, param_value, c)\n                new_m1 = beta1 * m1_value + (1 - beta1) * rgrad\n\n                rgrad_sq = manifold_module.tangent_inner(rgrad, rgrad, param_value, c)\n                rgrad_sq = jnp.asarray(rgrad_sq, dtype=rgrad.dtype)\n                rgrad_sq = jnp.broadcast_to(rgrad_sq, m2_value.shape)\n                new_m2 = beta2 * m2_value + (1 - beta2) * rgrad_sq\n\n                m1_hat = new_m1 / bias_correction1\n                m2_hat = new_m2 / bias_correction2\n                direction = m1_hat / (jnp.sqrt(m2_hat) + eps)\n\n                lr_cast = lr.astype(direction.dtype)\n                step = -lr_cast * direction\n                if use_expmap:\n                    new_param_value = manifold_module.expmap(step, param_value, c)\n                else:\n                    new_param_value = manifold_module.retraction(step, param_value, c)\n\n                transported_m1 = manifold_module.ptransp(new_m1, param_value, new_param_value, c)\n                transported_m2 = new_m2\n\n                param_update = new_param_value - param_value\n            else:\n                new_m1 = beta1 * m1_value + (1 - beta1) * grad_value\n                new_m2 = beta2 * m2_value + (1 - beta2) * (grad_value**2)\n\n                m1_hat = new_m1 / bias_correction1\n                m2_hat = new_m2 / bias_correction2\n\n                lr_cast = lr.astype(m1_hat.dtype)\n                param_update = -lr_cast * m1_hat / (jnp.sqrt(m2_hat) + eps)\n                transported_m1 = new_m1\n                transported_m2 = new_m2\n\n            param_update_leaves.append(param_update)\n            new_m1_leaves.append(transported_m1)\n            new_m2_leaves.append(transported_m2)\n\n        param_updates = tree_util.tree_unflatten(treedef, param_update_leaves)\n        new_m1 = tree_util.tree_unflatten(treedef, new_m1_leaves)\n        new_m2 = tree_util.tree_unflatten(treedef, new_m2_leaves)\n\n        new_state = RAdamState(m1=new_m1, m2=new_m2, count=count_inc)\n\n        return param_updates, new_state\n\n    return optax.GradientTransformation(init_fn, cast(Any, update_fn))\n</code></pre>"},{"location":"api-reference/optimizers/#hyperbolix.optim.riemannian_adam.riemannian_adam--create-model-with-manifold-parameters","title":"Create model with manifold parameters","text":"<p>layer = HypLinearPoincare(poincare, 10, 5, rngs=nnx.Rngs(0))</p>"},{"location":"api-reference/optimizers/#hyperbolix.optim.riemannian_adam.riemannian_adam--create-riemannian-adam-optimizer","title":"Create Riemannian Adam optimizer","text":"<p>tx = riemannian_adam(learning_rate=0.001, use_expmap=True) optimizer = nnx.Optimizer(layer, tx, wrt=nnx.Param)</p>"},{"location":"api-reference/optimizers/#hyperbolix.optim.riemannian_adam.riemannian_adam--training-step","title":"Training step","text":"<p>def loss_fn(model, x): ...     y = model(x, c=1.0) ...     return jnp.sum(y ** 2)</p> <p>x = jax.random.normal(jax.random.key(1), (32, 10)) grads = nnx.grad(loss_fn)(layer, x) optimizer.update(grads)  # Automatically handles manifold parameters</p>"},{"location":"api-reference/optimizers/#example_1","title":"Example","text":"<pre><code>from hyperbolix.optim import riemannian_adam\n\n# Create Riemannian Adam optimizer\noptimizer = nnx.Optimizer(\n    model,\n    riemannian_adam(\n        learning_rate=0.001,\n        b1=0.9,\n        b2=0.999,\n        eps=1e-8\n    )\n)\n\n# Use in training loop (same as RSGD example)\n</code></pre>"},{"location":"api-reference/optimizers/#manifold-metadata-system","title":"Manifold Metadata System","text":"<p>Hyperbolix uses Flax NNX's <code>Variable._var_metadata</code> system to tag manifold parameters. This enables automatic manifold detection during optimization.</p>"},{"location":"api-reference/optimizers/#how-it-works","title":"How It Works","text":"<pre><code>from hyperbolix.optim.manifold_metadata import PoincareMetadata\n\n# Layer automatically tags hyperbolic parameters\nclass HypLinearPoincare(nnx.Module):\n    def __init__(self, manifold_module, in_dim, out_dim, *, rngs):\n        self.manifold = manifold_module\n\n        # Weight is Euclidean\n        self.weight = nnx.Param(\n            nnx.initializers.xavier_uniform()(rngs.params(), (in_dim, out_dim))\n        )\n\n        # Bias lives on Poincar\u00e9 ball (tagged with metadata)\n        self.bias = nnx.Param(\n            jnp.zeros(out_dim),\n            metadata=PoincareMetadata()\n        )\n</code></pre> <p>The optimizer automatically:</p> <ol> <li>Detects parameters with manifold metadata</li> <li>Applies Riemannian gradient updates (expmap/retraction)</li> <li>Performs parallel transport for momentum/adaptive moments</li> <li>Falls back to Euclidean updates for unmarked parameters</li> </ol>"},{"location":"api-reference/optimizers/#available-metadata","title":"Available Metadata","text":""},{"location":"api-reference/optimizers/#hyperbolix.optim.manifold_metadata","title":"hyperbolix.optim.manifold_metadata","text":"<p>Manifold metadata utilities for Riemannian optimization.</p> <p>This module provides utilities to mark NNX parameters with manifold metadata and extract that metadata during optimization. The metadata system uses Flax NNX's built-in Variable._var_metadata attribute to store information about which manifold a parameter lives on, enabling automatic detection and appropriate handling during Riemannian optimization.</p> <p>Design rationale: - All trainable parameters remain nnx.Param (single variable type) - Metadata is stored as part of the Variable, making it serialization-friendly - String identifiers map to actual manifold modules via a registry - Supports both static and callable curvature parameters</p> <p>Example:     &gt;&gt;&gt; import jax.numpy as jnp     &gt;&gt;&gt; from flax import nnx     &gt;&gt;&gt; from hyperbolix.optim import mark_manifold_param, get_manifold_info     &gt;&gt;&gt;     &gt;&gt;&gt; # Create a parameter on the Poincar\u00e9 manifold     &gt;&gt;&gt; bias_init = jnp.zeros((10,))     &gt;&gt;&gt; bias = mark_manifold_param(     ...     nnx.Param(bias_init),     ...     manifold_type='poincare',     ...     curvature=1.0     ... )     &gt;&gt;&gt;     &gt;&gt;&gt; # Later, in optimizer: extract manifold info     &gt;&gt;&gt; manifold_info = get_manifold_info(bias)     &gt;&gt;&gt; if manifold_info is not None:     ...     manifold_module, c = manifold_info     ...     # Apply Riemannian operations...</p>"},{"location":"api-reference/optimizers/#expmap-vs-retraction","title":"Expmap vs Retraction","text":"<p>Both optimizers support two update modes:</p> <ul> <li>Exponential map (default): <code>expmap(x, -lr * grad)</code></li> <li>Exact geodesic following</li> <li>Numerically stable for large steps</li> <li> <p>Slightly slower</p> </li> <li> <p>Retraction: <code>proj(x - lr * grad)</code></p> </li> <li>First-order approximation</li> <li>Faster computation</li> <li>Can be less stable for large learning rates</li> </ul>"},{"location":"api-reference/optimizers/#choosing-update-mode","title":"Choosing Update Mode","text":"<pre><code># Use exponential map (default, recommended)\nopt = riemannian_adam(learning_rate=0.001)\n\n# For extremely performance-critical applications,\n# you can experiment with retraction-based updates\n# by modifying the optimizer implementation\n</code></pre> <p>In practice, exponential maps provide better stability and convergence, especially for hyperbolic neural networks.</p>"},{"location":"api-reference/optimizers/#mixed-optimization","title":"Mixed Optimization","text":"<p>The optimizers seamlessly handle models with both Euclidean and hyperbolic parameters:</p> <pre><code>class MixedModel(nnx.Module):\n    def __init__(self, rngs):\n        # Euclidean linear layer\n        self.fc1 = nnx.Linear(32, 64, rngs=rngs)\n\n        # Hyperbolic layer (bias has manifold metadata)\n        self.hyp = HypLinearPoincare(\n            manifold_module=poincare,\n            in_dim=64,\n            out_dim=16,\n            rngs=rngs\n        )\n\n        # Another Euclidean layer\n        self.fc2 = nnx.Linear(16, 10, rngs=rngs)\n\n# Optimizer handles all parameter types automatically\noptimizer = nnx.Optimizer(model, riemannian_adam(learning_rate=0.001))\n</code></pre> <p>The optimizer will:</p> <ul> <li>Apply standard Adam updates to <code>fc1</code> and <code>fc2</code> parameters</li> <li>Apply Riemannian Adam updates to <code>hyp.bias</code> (tagged with metadata)</li> <li>Apply Euclidean Adam updates to <code>hyp.weight</code> (no metadata)</li> </ul>"},{"location":"api-reference/optimizers/#performance-considerations","title":"Performance Considerations","text":"<p>JIT Compilation</p> <p>Both optimizers are JIT-compatible. For best performance:</p> <pre><code>@jax.jit\ndef train_step(model, optimizer, x, y):\n    def loss_fn(model):\n        return compute_loss(model, x, y)\n\n    loss, grads = nnx.value_and_grad(loss_fn)(model)\n    optimizer.update(grads)\n    return loss\n</code></pre> <p>Curvature as Static Argument</p> <p>If curvature <code>c</code> is constant during training, pass it as a static argument to enable better JIT optimization:</p> <pre><code>@jax.jit\ndef forward(model, x):\n    return model(x, c=1.0)  # c is traced, not ideal\n\n# Better: use partial application\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=(2,))\ndef forward(model, x, c):\n    return model(x, c=c)\n\noutput = forward(model, x, 1.0)  # c=1.0 is static\n</code></pre>"},{"location":"api-reference/optimizers/#references","title":"References","text":"<p>The Riemannian optimizers are based on:</p> <ul> <li>B\u00e9cigneul, G., &amp; Ganea, O. (2019). \"Riemannian Adaptive Optimization Methods.\" ICLR 2019.</li> <li>Bonnabel, S. (2013). \"Stochastic gradient descent on Riemannian manifolds.\" IEEE TAC.</li> </ul> <p>See the User Guide for detailed explanations and best practices.</p>"},{"location":"api-reference/utils/","title":"Utilities API","text":"<p>Utility functions for hyperbolic deep learning.</p>"},{"location":"api-reference/utils/#math-utilities","title":"Math Utilities","text":"<p>Numerically stable implementations of hyperbolic functions.</p>"},{"location":"api-reference/utils/#hyperbolix.utils.math_utils","title":"hyperbolix.utils.math_utils","text":"<p>Math utils functions for hyperbolic operations with numerically stable limits.</p> <p>Direct JAX port of PyTorch math_utils.py with type annotations using jaxtyping.</p>"},{"location":"api-reference/utils/#hyperbolix.utils.math_utils.cosh","title":"cosh","text":"<pre><code>cosh(x: Float[Array, ...]) -&gt; Float[Array, ...]\n</code></pre> <p>Hyperbolic cosine with overflow protection. Domain=(-inf, inf).</p> <p>Clamps input to safe ranges to prevent overflow based on dtype. Uses log(max) * 0.99 as safety margin.</p> <p>Args:     x: Input array of any shape</p> <p>Returns:     cosh(x) with overflow protection</p> Source code in <code>hyperbolix/utils/math_utils.py</code> <pre><code>@jax.jit\ndef cosh(x: Float[Array, \"...\"]) -&gt; Float[Array, \"...\"]:\n    \"\"\"Hyperbolic cosine with overflow protection. Domain=(-inf, inf).\n\n    Clamps input to safe ranges to prevent overflow based on dtype.\n    Uses log(max) * 0.99 as safety margin.\n\n    Args:\n        x: Input array of any shape\n\n    Returns:\n        cosh(x) with overflow protection\n    \"\"\"\n    # Safe limit based on dtype: cosh(x) \u2248 exp(x)/2 for large x, so x &lt; log(max)\n    clamp = jnp.log(jnp.finfo(x.dtype).max) * 0.99\n    x = smooth_clamp(x, -clamp, clamp)\n    return jnp.cosh(x)\n</code></pre>"},{"location":"api-reference/utils/#hyperbolix.utils.math_utils.sinh","title":"sinh","text":"<pre><code>sinh(x: Float[Array, ...]) -&gt; Float[Array, ...]\n</code></pre> <p>Hyperbolic sine with overflow protection. Domain=(-inf, inf).</p> <p>Clamps input to safe ranges to prevent overflow based on dtype. Uses log(max) * 0.99 as safety margin.</p> <p>Args:     x: Input array of any shape</p> <p>Returns:     sinh(x) with overflow protection</p> Source code in <code>hyperbolix/utils/math_utils.py</code> <pre><code>@jax.jit\ndef sinh(x: Float[Array, \"...\"]) -&gt; Float[Array, \"...\"]:\n    \"\"\"Hyperbolic sine with overflow protection. Domain=(-inf, inf).\n\n    Clamps input to safe ranges to prevent overflow based on dtype.\n    Uses log(max) * 0.99 as safety margin.\n\n    Args:\n        x: Input array of any shape\n\n    Returns:\n        sinh(x) with overflow protection\n    \"\"\"\n    # Safe limit based on dtype: sinh(x) \u2248 exp(x)/2 for large x, so x &lt; log(max)\n    clamp = jnp.log(jnp.finfo(x.dtype).max) * 0.99\n    x = smooth_clamp(x, -clamp, clamp)\n    return jnp.sinh(x)\n</code></pre>"},{"location":"api-reference/utils/#hyperbolix.utils.math_utils.acosh","title":"acosh","text":"<pre><code>acosh(x: Float[Array, ...]) -&gt; Float[Array, ...]\n</code></pre> <p>Inverse hyperbolic cosine with domain clamping. Domain=[1, inf).</p> <p>Args:     x: Input array of any shape</p> <p>Returns:     acosh(x) with domain protection (clamps x &gt;= 1.0)</p> Source code in <code>hyperbolix/utils/math_utils.py</code> <pre><code>@jax.jit\ndef acosh(x: Float[Array, \"...\"]) -&gt; Float[Array, \"...\"]:\n    \"\"\"Inverse hyperbolic cosine with domain clamping. Domain=[1, inf).\n\n    Args:\n        x: Input array of any shape\n\n    Returns:\n        acosh(x) with domain protection (clamps x &gt;= 1.0)\n    \"\"\"\n    x = jnp.clip(x, 1.0, None)\n    return jnp.acosh(x)\n</code></pre>"},{"location":"api-reference/utils/#hyperbolix.utils.math_utils.atanh","title":"atanh","text":"<pre><code>atanh(x: Float[Array, ...]) -&gt; Float[Array, ...]\n</code></pre> <p>Inverse hyperbolic tangent with domain clamping. Domain=(-1, 1).</p> <p>Clamps input away from \u00b11 to avoid singularities.</p> <p>Args:     x: Input array of any shape</p> <p>Returns:     atanh(x) with domain protection</p> Source code in <code>hyperbolix/utils/math_utils.py</code> <pre><code>@jax.jit\ndef atanh(x: Float[Array, \"...\"]) -&gt; Float[Array, \"...\"]:\n    \"\"\"Inverse hyperbolic tangent with domain clamping. Domain=(-1, 1).\n\n    Clamps input away from \u00b11 to avoid singularities.\n\n    Args:\n        x: Input array of any shape\n\n    Returns:\n        atanh(x) with domain protection\n    \"\"\"\n    eps = jnp.finfo(x.dtype).eps\n    x = jnp.clip(x, -1.0 + eps, 1.0 - eps)\n    return jnp.atanh(x)\n</code></pre>"},{"location":"api-reference/utils/#hyperbolix.utils.math_utils.smooth_clamp","title":"smooth_clamp","text":"<pre><code>smooth_clamp(\n    x: Float[Array, ...],\n    min_value: float,\n    max_value: float,\n    smoothing_factor: float = 50.0,\n) -&gt; Float[Array, ...]\n</code></pre> <p>Smoothly clamp array values to a range [min_value, max_value].</p> <p>Args:     x: Input array of any shape     min_value: Minimum value to clamp to     max_value: Maximum value to clamp to     smoothing_factor: Beta parameter for softplus (higher = sharper transition)</p> <p>Returns:     Array with values smoothly clamped to [min_value, max_value]</p> Source code in <code>hyperbolix/utils/math_utils.py</code> <pre><code>@functools.partial(jax.jit, static_argnames=[\"smoothing_factor\"])\ndef smooth_clamp(\n    x: Float[Array, \"...\"], min_value: float, max_value: float, smoothing_factor: float = 50.0\n) -&gt; Float[Array, \"...\"]:\n    \"\"\"Smoothly clamp array values to a range [min_value, max_value].\n\n    Args:\n        x: Input array of any shape\n        min_value: Minimum value to clamp to\n        max_value: Maximum value to clamp to\n        smoothing_factor: Beta parameter for softplus (higher = sharper transition)\n\n    Returns:\n        Array with values smoothly clamped to [min_value, max_value]\n    \"\"\"\n    x = smooth_clamp_max(x, max_value, smoothing_factor=smoothing_factor)\n    return smooth_clamp_min(x, min_value, smoothing_factor=smoothing_factor)\n</code></pre>"},{"location":"api-reference/utils/#usage-example","title":"Usage Example","text":"<pre><code>from hyperbolix.utils.math_utils import acosh, atanh\nimport jax.numpy as jnp\n\n# Numerically stable hyperbolic functions\nx = jnp.array([1.5, 2.0, 10.0])\ny = acosh(x)  # Handles edge cases near 1.0\n\n# Smooth clamping for stability\nz = jnp.array([0.99, 1.0, 1.01])\nz_clamped = smooth_clamp(z, min_val=0.0, max_val=1.0)\n</code></pre>"},{"location":"api-reference/utils/#helper-functions","title":"Helper Functions","text":"<p>Helper utilities for distance computation and delta-hyperbolicity analysis.</p>"},{"location":"api-reference/utils/#hyperbolix.utils.helpers","title":"hyperbolix.utils.helpers","text":"<p>Helper utilities for hyperbolic geometry computations.</p> <p>This module provides utilities for computing pairwise distances, delta-hyperbolicity metrics, and other geometric measures on hyperbolic manifolds.</p>"},{"location":"api-reference/utils/#hyperbolix.utils.helpers.compute_pairwise_distances","title":"compute_pairwise_distances","text":"<pre><code>compute_pairwise_distances(\n    points: Float[Array, \"n_points dim\"],\n    manifold_module,\n    c: Float[Array, \"\"] | float,\n    version_idx: int = 0,\n) -&gt; Float[Array, \"n_points n_points\"]\n</code></pre> <p>Compute pairwise geodesic distances between points on a manifold.</p> <p>This function computes the full distance matrix efficiently by leveraging JAX's vmap for vectorization. The computation is NOT chunked - the entire distance matrix is computed in a single pass using nested vmap operations.</p> <p>Memory Considerations:     For n points, this computes an n-by-n distance matrix in memory. For very     large point sets (&gt;5000-10000 points depending on available memory),     consider subsampling or implementing a chunked version. The current     implementation prioritizes simplicity and leverages XLA's automatic     memory optimizations.</p> <p>Args:     points: Points on the manifold, shape (n_points, dim)         For Hyperboloid: dim is ambient dimension (dim+1)         For PoincareBall: dim is intrinsic dimension     manifold_module: Manifold module (hyperboloid or poincare)     c: Curvature parameter (positive scalar)     version_idx: Distance version index (manifold-specific, default: 0)         For Hyperboloid:             0 = VERSION_DEFAULT (standard acosh with hard clipping)             1 = VERSION_SMOOTHENED (smoothened distance)         For PoincareBall:             0 = VERSION_MOBIUS_DIRECT (direct M\u00f6bius formula)             1 = VERSION_MOBIUS (via addition)             2 = VERSION_METRIC_TENSOR (metric tensor induced)             3 = VERSION_LORENTZIAN_PROXY (Lorentzian proxy)</p> <p>Returns:     Symmetric distance matrix of shape (n_points, n_points)</p> <p>Examples:     &gt;&gt;&gt; import jax.numpy as jnp     &gt;&gt;&gt; from hyperbolix.manifolds import hyperboloid     &gt;&gt;&gt; from hyperbolix.utils.helpers import compute_pairwise_distances     &gt;&gt;&gt;     &gt;&gt;&gt; # Generate random hyperboloid points     &gt;&gt;&gt; key = jax.random.PRNGKey(0)     &gt;&gt;&gt; points = jax.random.normal(key, (100, 11))     &gt;&gt;&gt; points = jax.vmap(hyperboloid.proj, in_axes=(0, None))(points, 1.0)     &gt;&gt;&gt;     &gt;&gt;&gt; # Compute pairwise distances     &gt;&gt;&gt; distmat = compute_pairwise_distances(     ...     points, hyperboloid, c=1.0, version_idx=hyperboloid.VERSION_DEFAULT     ... )     &gt;&gt;&gt; print(distmat.shape)  # (100, 100)</p> <p>Notes:     - The PyTorch reference implementation used explicit chunking for memory       management. This JAX version uses vmap and relies on XLA optimization.     - The distance matrix is symmetric: distmat[i, j] == distmat[j, i]     - Diagonal elements are zero: distmat[i, i] == 0     - For large datasets, consider subsampling before calling this function</p> Source code in <code>hyperbolix/utils/helpers.py</code> <pre><code>def compute_pairwise_distances(\n    points: Float[Array, \"n_points dim\"],\n    manifold_module,\n    c: Float[Array, \"\"] | float,\n    version_idx: int = 0,\n) -&gt; Float[Array, \"n_points n_points\"]:\n    \"\"\"Compute pairwise geodesic distances between points on a manifold.\n\n    This function computes the full distance matrix efficiently by leveraging\n    JAX's vmap for vectorization. The computation is NOT chunked - the entire\n    distance matrix is computed in a single pass using nested vmap operations.\n\n    Memory Considerations:\n        For n points, this computes an n-by-n distance matrix in memory. For very\n        large point sets (&gt;5000-10000 points depending on available memory),\n        consider subsampling or implementing a chunked version. The current\n        implementation prioritizes simplicity and leverages XLA's automatic\n        memory optimizations.\n\n    Args:\n        points: Points on the manifold, shape (n_points, dim)\n            For Hyperboloid: dim is ambient dimension (dim+1)\n            For PoincareBall: dim is intrinsic dimension\n        manifold_module: Manifold module (hyperboloid or poincare)\n        c: Curvature parameter (positive scalar)\n        version_idx: Distance version index (manifold-specific, default: 0)\n            For Hyperboloid:\n                0 = VERSION_DEFAULT (standard acosh with hard clipping)\n                1 = VERSION_SMOOTHENED (smoothened distance)\n            For PoincareBall:\n                0 = VERSION_MOBIUS_DIRECT (direct M\u00f6bius formula)\n                1 = VERSION_MOBIUS (via addition)\n                2 = VERSION_METRIC_TENSOR (metric tensor induced)\n                3 = VERSION_LORENTZIAN_PROXY (Lorentzian proxy)\n\n    Returns:\n        Symmetric distance matrix of shape (n_points, n_points)\n\n    Examples:\n        &gt;&gt;&gt; import jax.numpy as jnp\n        &gt;&gt;&gt; from hyperbolix.manifolds import hyperboloid\n        &gt;&gt;&gt; from hyperbolix.utils.helpers import compute_pairwise_distances\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Generate random hyperboloid points\n        &gt;&gt;&gt; key = jax.random.PRNGKey(0)\n        &gt;&gt;&gt; points = jax.random.normal(key, (100, 11))\n        &gt;&gt;&gt; points = jax.vmap(hyperboloid.proj, in_axes=(0, None))(points, 1.0)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compute pairwise distances\n        &gt;&gt;&gt; distmat = compute_pairwise_distances(\n        ...     points, hyperboloid, c=1.0, version_idx=hyperboloid.VERSION_DEFAULT\n        ... )\n        &gt;&gt;&gt; print(distmat.shape)  # (100, 100)\n\n    Notes:\n        - The PyTorch reference implementation used explicit chunking for memory\n          management. This JAX version uses vmap and relies on XLA optimization.\n        - The distance matrix is symmetric: distmat[i, j] == distmat[j, i]\n        - Diagonal elements are zero: distmat[i, i] == 0\n        - For large datasets, consider subsampling before calling this function\n    \"\"\"\n\n    # Create vectorized distance function: dist(x, y) -&gt; scalar\n    # We need to compute dist for all pairs (i, j)\n    def dist_fn(x, y):\n        return manifold_module.dist(x, y, c, version_idx)\n\n    # Use vmap to vectorize over both dimensions\n    # First vmap over y (columns), then over x (rows)\n    dist_col = jax.vmap(dist_fn, in_axes=(None, 0))  # Broadcasts x over all y\n    dist_matrix_fn = jax.vmap(dist_col, in_axes=(0, None))  # Broadcasts over all x\n\n    # Compute full distance matrix\n    distmat = dist_matrix_fn(points, points)\n\n    return distmat\n</code></pre>"},{"location":"api-reference/utils/#hyperbolix.utils.helpers.compute_hyperbolic_delta","title":"compute_hyperbolic_delta","text":"<pre><code>compute_hyperbolic_delta(\n    distmat: Float[Array, \"n_points n_points\"],\n    version: str = \"average\",\n) -&gt; Float[Array, \"\"]\n</code></pre> <p>Compute the delta-hyperbolicity value from a distance matrix.</p> <p>Delta-hyperbolicity is a metric space property that quantifies how \"tree-like\" or \"hyperbolic\" a metric space is. It is based on the Gromov 4-point condition.</p> <p>For any four points w, x, y, z in a metric space, define:     S1 = d(w,x) + d(y,z)     S2 = d(w,y) + d(x,z)     S3 = d(w,z) + d(x,y)</p> <p>The 4-point condition requires that the two largest of these sums differ by at most 2\u03b4. A space is \u03b4-hyperbolic if this holds for all quadruples.</p> <p>This implementation uses a reference point (the first point) to compute Gromov products efficiently:     (x|y)_w = [d(w,x) + d(w,y) - d(x,y)] / 2</p> <p>Args:     distmat: Symmetric distance matrix, shape (n_points, n_points)     version: Which delta statistic to compute (default: \"average\")         - \"average\": Mean of delta values over all point quadruples         - \"smallest\": Maximum delta (worst-case over all quadruples)</p> <p>Returns:     Delta-hyperbolicity value (scalar)</p> <p>References:     Gromov, M. (1987). \"Hyperbolic groups.\" Essays in group theory.     Chami, I., et al. (2021). \"HoroPCA: Hyperbolic dimensionality reduction         via horospherical projections.\" ICML 2021.</p> <p>Examples:     &gt;&gt;&gt; import jax.numpy as jnp     &gt;&gt;&gt; from hyperbolix.utils.helpers import compute_hyperbolic_delta     &gt;&gt;&gt;     &gt;&gt;&gt; # Create a distance matrix (should be symmetric)     &gt;&gt;&gt; distmat = jnp.array([     ...     [0.0, 1.0, 2.0, 3.0],     ...     [1.0, 0.0, 1.5, 2.5],     ...     [2.0, 1.5, 0.0, 1.0],     ...     [3.0, 2.5, 1.0, 0.0]     ... ])     &gt;&gt;&gt;     &gt;&gt;&gt; delta_avg = compute_hyperbolic_delta(distmat, version=\"average\")     &gt;&gt;&gt; delta_max = compute_hyperbolic_delta(distmat, version=\"smallest\")</p> <p>Notes:     - The result is scaled by 2 because we fix a reference point     - Lower delta values indicate more hyperbolic (tree-like) structure     - Euclidean spaces have unbounded delta; hyperbolic spaces have bounded delta</p> Source code in <code>hyperbolix/utils/helpers.py</code> <pre><code>def compute_hyperbolic_delta(distmat: Float[Array, \"n_points n_points\"], version: str = \"average\") -&gt; Float[Array, \"\"]:\n    \"\"\"Compute the delta-hyperbolicity value from a distance matrix.\n\n    Delta-hyperbolicity is a metric space property that quantifies how \"tree-like\"\n    or \"hyperbolic\" a metric space is. It is based on the Gromov 4-point condition.\n\n    For any four points w, x, y, z in a metric space, define:\n        S1 = d(w,x) + d(y,z)\n        S2 = d(w,y) + d(x,z)\n        S3 = d(w,z) + d(x,y)\n\n    The 4-point condition requires that the two largest of these sums differ by at\n    most 2\u03b4. A space is \u03b4-hyperbolic if this holds for all quadruples.\n\n    This implementation uses a reference point (the first point) to compute\n    Gromov products efficiently:\n        (x|y)_w = [d(w,x) + d(w,y) - d(x,y)] / 2\n\n    Args:\n        distmat: Symmetric distance matrix, shape (n_points, n_points)\n        version: Which delta statistic to compute (default: \"average\")\n            - \"average\": Mean of delta values over all point quadruples\n            - \"smallest\": Maximum delta (worst-case over all quadruples)\n\n    Returns:\n        Delta-hyperbolicity value (scalar)\n\n    References:\n        Gromov, M. (1987). \"Hyperbolic groups.\" Essays in group theory.\n        Chami, I., et al. (2021). \"HoroPCA: Hyperbolic dimensionality reduction\n            via horospherical projections.\" ICML 2021.\n\n    Examples:\n        &gt;&gt;&gt; import jax.numpy as jnp\n        &gt;&gt;&gt; from hyperbolix.utils.helpers import compute_hyperbolic_delta\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create a distance matrix (should be symmetric)\n        &gt;&gt;&gt; distmat = jnp.array([\n        ...     [0.0, 1.0, 2.0, 3.0],\n        ...     [1.0, 0.0, 1.5, 2.5],\n        ...     [2.0, 1.5, 0.0, 1.0],\n        ...     [3.0, 2.5, 1.0, 0.0]\n        ... ])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; delta_avg = compute_hyperbolic_delta(distmat, version=\"average\")\n        &gt;&gt;&gt; delta_max = compute_hyperbolic_delta(distmat, version=\"smallest\")\n\n    Notes:\n        - The result is scaled by 2 because we fix a reference point\n        - Lower delta values indicate more hyperbolic (tree-like) structure\n        - Euclidean spaces have unbounded delta; hyperbolic spaces have bounded delta\n    \"\"\"\n    # Set the first point as reference and compute pairwise Gromov products\n    # Gromov product: (x|y)_w = [d(w,x) + d(w,y) - d(x,y)] / 2\n    # We use point 0 as reference: (i|j)_0 = [d(0,i) + d(0,j) - d(i,j)] / 2\n\n    # distmat_i0[i, j] = d(i, 0) for all j (broadcast column 0 across columns)\n    distmat_i0 = jnp.tile(distmat[:, 0:1], (1, distmat.shape[0]))  # shape: (n, n)\n    # distmat_0j[i, j] = d(0, j) for all i (broadcast row 0 across rows)\n    distmat_0j = jnp.tile(distmat[0:1, :], (distmat.shape[0], 1))  # shape: (n, n)\n\n    # Compute Gromov product matrix: (i|j)_0 for all pairs (i, j)\n    gromov_prod_mat = (distmat_i0 + distmat_0j - distmat) / 2.0  # shape: (n, n)\n\n    # Compute the (max, min)-product of the Gromov product matrix with itself\n    # For each triple (i, j, k), we need: min((i|j)_0, (i|k)_0) over k, then max over j\n    # gromov_prod_mat[i, j] = (i|j)_0\n    # gromov_prod_mat[i, k] = (i|k)_0\n    # We want: max_j { max_k { min((i|j)_0, (i|k)_0) } }\n\n    # Expand dimensions for broadcasting:\n    # gromov_prod_mat.shape = (n, n)\n    # Reshape to compute min over pairs:\n    # gromov_prod_mat[:, None, :] has shape (n, 1, n) - this is (i, j, k) with j=1\n    # gromov_prod_mat[:, :, None] has shape (n, n, 1) - this is (i, j, k) with k=1\n    # Broadcasting: (n, 1, n) and (n, n, 1) -&gt; (n, n, n)\n    # Result[i, j, k] = min((i|j)_0, (i|k)_0)\n\n    min_products = jnp.minimum(\n        gromov_prod_mat[:, None, :],  # shape: (n, 1, n) - (i, 1, k)\n        gromov_prod_mat[:, :, None],  # shape: (n, n, 1) - (i, j, 1)\n    )  # shape: (n, n, n) - (i, j, k)\n\n    # Take maximum over the last dimension (k): max_k { min((i|j)_0, (i|k)_0) }\n    max_min_prod = jnp.max(min_products, axis=2)  # shape: (n, n) - (i, j)\n\n    # Compute delta for each pair: max_k{min((i|j)_0, (i|k)_0)} - (i|j)_0\n    delta_matrix = max_min_prod - gromov_prod_mat  # shape: (n, n)\n\n    # Compute the requested statistic\n    if version == \"average\":\n        delta = jnp.mean(delta_matrix)\n    else:  # \"smallest\" (which is actually the maximum delta value)\n        delta = jnp.max(delta_matrix)\n\n    # Rescale delta since a reference point was fixed\n    # The factor of 2 accounts for the fact that we fixed one point\n    res = 2.0 * delta\n\n    return res\n</code></pre>"},{"location":"api-reference/utils/#hyperbolix.utils.helpers.get_delta","title":"get_delta","text":"<pre><code>get_delta(\n    points: Float[Array, \"n_points dim\"],\n    manifold_module,\n    c: float,\n    version_idx: int = 0,\n    sample_size: int = 1500,\n    version: str = \"average\",\n    key: Key[Array, \"\"] | None = None,\n) -&gt; tuple[\n    Float[Array, \"\"], Float[Array, \"\"], Float[Array, \"\"]\n]\n</code></pre> <p>Compute delta-hyperbolicity and related metrics for a point set.</p> <p>This function subsamples points (if needed), computes the pairwise distance matrix, and then calculates the delta-hyperbolicity value along with the diameter and relative delta (delta normalized by diameter).</p> <p>Args:     points: Points on the manifold, shape (n_points, dim)     manifold_module: Manifold module (hyperboloid or poincare)     c: Curvature parameter (positive scalar)     version_idx: Distance version index (manifold-specific, default: 0)     sample_size: Maximum number of points to use for delta computation         (default: 1500). If n_points &gt; sample_size, randomly subsample.     version: Which delta statistic to compute (default: \"average\")         - \"average\": Mean of delta values         - \"smallest\": Maximum delta (worst-case)     key: JAX random key for subsampling (required if n_points &gt; sample_size)</p> <p>Returns:     Tuple of (delta, diameter, relative_delta):         - delta: Delta-hyperbolicity value         - diameter: Maximum pairwise distance in the point set         - relative_delta: delta / diameter (scale-invariant measure)</p> <p>Examples:     &gt;&gt;&gt; import jax     &gt;&gt;&gt; import jax.numpy as jnp     &gt;&gt;&gt; from hyperbolix.manifolds import hyperboloid     &gt;&gt;&gt; from hyperbolix.utils.helpers import get_delta     &gt;&gt;&gt;     &gt;&gt;&gt; # Generate random hyperboloid points     &gt;&gt;&gt; key = jax.random.PRNGKey(42)     &gt;&gt;&gt; points = jax.random.normal(key, (2000, 11))     &gt;&gt;&gt; points = jax.vmap(hyperboloid.proj, in_axes=(0, None))(points, 1.0)     &gt;&gt;&gt;     &gt;&gt;&gt; # Compute delta metrics     &gt;&gt;&gt; key, subkey = jax.random.split(key)     &gt;&gt;&gt; delta, diam, rel_delta = get_delta(     ...     points, hyperboloid, c=1.0, sample_size=1500, key=subkey     ... )     &gt;&gt;&gt; print(f\"Delta: {delta:.4f}, Diameter: {diam:.4f}, Relative: {rel_delta:.4f}\")</p> <p>Notes:     - Subsampling is done randomly without replacement     - For reproducibility, always provide the same random key     - The PyTorch version used torch.randperm; we use jax.random.permutation</p> Source code in <code>hyperbolix/utils/helpers.py</code> <pre><code>def get_delta(\n    points: Float[Array, \"n_points dim\"],\n    manifold_module,\n    c: float,\n    version_idx: int = 0,\n    sample_size: int = 1500,\n    version: str = \"average\",\n    key: Key[Array, \"\"] | None = None,\n) -&gt; tuple[Float[Array, \"\"], Float[Array, \"\"], Float[Array, \"\"]]:\n    \"\"\"Compute delta-hyperbolicity and related metrics for a point set.\n\n    This function subsamples points (if needed), computes the pairwise distance\n    matrix, and then calculates the delta-hyperbolicity value along with the\n    diameter and relative delta (delta normalized by diameter).\n\n    Args:\n        points: Points on the manifold, shape (n_points, dim)\n        manifold_module: Manifold module (hyperboloid or poincare)\n        c: Curvature parameter (positive scalar)\n        version_idx: Distance version index (manifold-specific, default: 0)\n        sample_size: Maximum number of points to use for delta computation\n            (default: 1500). If n_points &gt; sample_size, randomly subsample.\n        version: Which delta statistic to compute (default: \"average\")\n            - \"average\": Mean of delta values\n            - \"smallest\": Maximum delta (worst-case)\n        key: JAX random key for subsampling (required if n_points &gt; sample_size)\n\n    Returns:\n        Tuple of (delta, diameter, relative_delta):\n            - delta: Delta-hyperbolicity value\n            - diameter: Maximum pairwise distance in the point set\n            - relative_delta: delta / diameter (scale-invariant measure)\n\n    Examples:\n        &gt;&gt;&gt; import jax\n        &gt;&gt;&gt; import jax.numpy as jnp\n        &gt;&gt;&gt; from hyperbolix.manifolds import hyperboloid\n        &gt;&gt;&gt; from hyperbolix.utils.helpers import get_delta\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Generate random hyperboloid points\n        &gt;&gt;&gt; key = jax.random.PRNGKey(42)\n        &gt;&gt;&gt; points = jax.random.normal(key, (2000, 11))\n        &gt;&gt;&gt; points = jax.vmap(hyperboloid.proj, in_axes=(0, None))(points, 1.0)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compute delta metrics\n        &gt;&gt;&gt; key, subkey = jax.random.split(key)\n        &gt;&gt;&gt; delta, diam, rel_delta = get_delta(\n        ...     points, hyperboloid, c=1.0, sample_size=1500, key=subkey\n        ... )\n        &gt;&gt;&gt; print(f\"Delta: {delta:.4f}, Diameter: {diam:.4f}, Relative: {rel_delta:.4f}\")\n\n    Notes:\n        - Subsampling is done randomly without replacement\n        - For reproducibility, always provide the same random key\n        - The PyTorch version used torch.randperm; we use jax.random.permutation\n    \"\"\"\n    n_points = points.shape[0]\n\n    # Subsample points if necessary\n    if n_points &gt; sample_size:\n        if key is None:\n            raise ValueError(f\"Random key required for subsampling (n_points={n_points} &gt; sample_size={sample_size})\")\n        # Random permutation of indices\n        indices = jax.random.permutation(key, n_points)[:sample_size]\n        sub_points = points[indices]\n    else:\n        sub_points = points\n\n    # Compute pairwise distances\n    distmat = compute_pairwise_distances(sub_points, manifold_module, c, version_idx)\n\n    # Compute delta-hyperbolicity\n    delta = compute_hyperbolic_delta(distmat, version)\n\n    # Compute diameter (maximum distance)\n    diam = jnp.max(distmat)\n\n    # Compute relative delta (scale-invariant)\n    rel_delta = delta / diam\n\n    return delta, diam, rel_delta\n</code></pre>"},{"location":"api-reference/utils/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/utils/#pairwise-distances","title":"Pairwise Distances","text":"<pre><code>from hyperbolix.utils.helpers import compute_pairwise_distances\nfrom hyperbolix.manifolds import poincare\nimport jax.numpy as jnp\n\n# Set of points on Poincar\u00e9 ball\npoints = jnp.array([\n    [0.1, 0.2],\n    [0.3, -0.1],\n    [-0.2, 0.4],\n    [0.0, 0.0]\n])\n\n# Compute all pairwise distances\ndist_matrix = compute_pairwise_distances(\n    points,\n    manifold_module=poincare,\n    c=1.0,\n    version_idx=0\n)\n\n# Result: (4, 4) matrix of distances\nprint(dist_matrix.shape)  # (4, 4)\n</code></pre>"},{"location":"api-reference/utils/#delta-hyperbolicity","title":"Delta-Hyperbolicity","text":"<p>Measure how \"hyperbolic\" a dataset is using the Gromov delta metric:</p> <pre><code>from hyperbolix.utils.helpers import get_delta\nfrom hyperbolix.manifolds import poincare\nimport jax.numpy as jnp\n\n# Generate random points\nkey = jax.random.PRNGKey(0)\npoints = jax.random.normal(key, (100, 2)) * 0.3\n\n# Project to Poincar\u00e9 ball\npoints_proj = jax.vmap(poincare.proj, in_axes=(0, None))(\n    points, 1.0\n)\n\n# Compute delta-hyperbolicity\ndelta, diameter, rel_delta = get_delta(\n    points_proj,\n    manifold_module=poincare,\n    c=1.0,\n    sample_size=500,  # Number of 4-point samples\n    seed=42\n)\n\nprint(f\"Delta: {delta:.4f}\")\nprint(f\"Diameter: {diameter:.4f}\")\nprint(f\"Relative delta: {rel_delta:.4f}\")\n</code></pre> <p>The Gromov delta quantifies tree-likeness:</p> <ul> <li>\u03b4 \u2248 0: Perfect tree structure (hyperbolic)</li> <li>\u03b4 &gt; 0: Non-tree structure (less hyperbolic)</li> <li>\u03b4/diameter: Normalized measure (relative delta)</li> </ul>"},{"location":"api-reference/utils/#horopca","title":"HoroPCA","text":"<p>Horospherical Principal Component Analysis for dimensionality reduction on hyperbolic manifolds.</p>"},{"location":"api-reference/utils/#hyperbolix.utils.horo_pca","title":"hyperbolix.utils.horo_pca","text":"<p>Horospherical PCA for hyperbolic dimensionality reduction.</p> <p>This module provides HoroPCA, a method for dimensionality reduction in hyperbolic space using horospherical projections. It is a JAX/Flax port of the PyTorch implementation.</p> <p>References:     Ines Chami, et al. \"Horopca: Hyperbolic dimensionality reduction via horospherical projections.\"         International Conference on Machine Learning (2021).     Weize Chen, et al. \"Fully hyperbolic neural networks.\"         arXiv preprint arXiv:2105.14686 (2021).</p>"},{"location":"api-reference/utils/#hyperbolix.utils.horo_pca.HoroPCA","title":"HoroPCA","text":"<pre><code>HoroPCA(\n    n_components: int,\n    n_in_features: int,\n    manifold_name: str,\n    c: float,\n    lr: float = 0.001,\n    max_steps: int = 100,\n    *,\n    rngs: Rngs,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Horospherical PCA for hyperbolic dimensionality reduction.</p> <p>This class implements HoroPCA using JAX and Flax. It supports both Poincar\u00e9 ball and hyperboloid manifolds. The principal components are represented as ideal points in the hyperboloid model's null cone.</p> <p>Attributes:     n_components: Target dimensionality     n_in_features: Input dimensionality (for hyperboloid: dim+1, for Poincar\u00e9: dim)     manifold_name: Either \"poincare\" or \"hyperboloid\"     c: Curvature parameter (nnx.Variable)     lr: Learning rate for optimization     max_steps: Maximum optimization steps     Q: Principal component parameters (nnx.Param)     data_mean: Stored Fr\u00e9chet mean (nnx.Variable, optional)</p> <p>References:     Ines Chami, et al. \"Horopca: Hyperbolic dimensionality reduction via horospherical projections.\"         International Conference on Machine Learning (2021).</p> <p>Initialize HoroPCA module.</p> <p>Args:     n_components: Target dimensionality after reduction     n_in_features: Input dimensionality         - For Poincar\u00e9 ball: dim (intrinsic dimension)         - For Hyperboloid: dim+1 (ambient dimension)     manifold_name: Either \"poincare\" or \"hyperboloid\"     c: Initial curvature parameter (positive)     lr: Learning rate for gradient descent     max_steps: Maximum optimization iterations     rngs: Flax random number generator state</p> Source code in <code>hyperbolix/utils/horo_pca.py</code> <pre><code>def __init__(\n    self,\n    n_components: int,\n    n_in_features: int,\n    manifold_name: str,\n    c: float,\n    lr: float = 1e-3,\n    max_steps: int = 100,\n    *,\n    rngs: nnx.Rngs,\n):\n    \"\"\"Initialize HoroPCA module.\n\n    Args:\n        n_components: Target dimensionality after reduction\n        n_in_features: Input dimensionality\n            - For Poincar\u00e9 ball: dim (intrinsic dimension)\n            - For Hyperboloid: dim+1 (ambient dimension)\n        manifold_name: Either \"poincare\" or \"hyperboloid\"\n        c: Initial curvature parameter (positive)\n        lr: Learning rate for gradient descent\n        max_steps: Maximum optimization iterations\n        rngs: Flax random number generator state\n    \"\"\"\n    self.n_components = n_components\n    self.n_in_features = n_in_features\n    self.manifold_name = manifold_name\n    self.lr = lr\n    self.max_steps = max_steps\n\n    # Store curvature as a variable (can be made trainable if needed)\n    self.c = nnx.Variable(jnp.array(c, dtype=jnp.float32))\n\n    # Initialize data_mean as None (will be set during fit)\n    self.data_mean: nnx.Variable[Float[Array, \"1 dim_plus_1\"] | None] = nnx.Variable(None)\n    self.loss_history = nnx.Variable(jnp.zeros((0,), dtype=jnp.float32))\n\n    # Initialize principal components Q based on manifold type\n    if manifold_name == \"poincare\":\n        # For Poincar\u00e9: Q has shape (n_components, n_in_features)\n        # These represent directions in the Poincar\u00e9 ball that will be mapped to ideals\n        self.Q = nnx.Param(\n            jax.random.normal(\n                rngs.params(),\n                shape=(n_components, n_in_features),\n                dtype=jnp.float32,\n            )\n        )\n    elif manifold_name == \"hyperboloid\":\n        # For Hyperboloid: Q has shape (n_components, n_in_features - 1)\n        # These represent spatial components that will be augmented to ideals\n        self.Q = nnx.Param(\n            jax.random.normal(\n                rngs.params(),\n                shape=(n_components, n_in_features - 1),\n                dtype=jnp.float32,\n            )\n        )\n    else:\n        raise ValueError(f\"Unsupported manifold: {manifold_name}. Use 'poincare' or 'hyperboloid'.\")\n</code></pre>"},{"location":"api-reference/utils/#hyperbolix.utils.horo_pca.HoroPCA.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(\n    x: Float[Array, \"n_points dim_plus_1\"],\n) -&gt; Float[Array, \"\"]\n</code></pre> <p>Compute negative generalized variance as loss function.</p> <p>The loss is the negative mean of squared pairwise distances in the projected space. Maximizing variance encourages spreading out the projected points.</p> <p>Args:     x: Hyperboloid points of shape (n_points, dim+1)</p> <p>Returns:     Negative generalized variance (scalar)</p> Source code in <code>hyperbolix/utils/horo_pca.py</code> <pre><code>def compute_loss(self, x: Float[Array, \"n_points dim_plus_1\"]) -&gt; Float[Array, \"\"]:\n    \"\"\"Compute negative generalized variance as loss function.\n\n    The loss is the negative mean of squared pairwise distances in the projected\n    space. Maximizing variance encourages spreading out the projected points.\n\n    Args:\n        x: Hyperboloid points of shape (n_points, dim+1)\n\n    Returns:\n        Negative generalized variance (scalar)\n    \"\"\"\n    return _projected_variance_loss(self.Q[...], x, self.c[...])\n</code></pre>"},{"location":"api-reference/utils/#hyperbolix.utils.horo_pca.HoroPCA.fit","title":"fit","text":"<pre><code>fit(x: Float[Array, 'n_points n_in_features']) -&gt; None\n</code></pre> <p>Fit HoroPCA to the data by finding optimal principal components.</p> <p>This method: 1. Converts data to hyperboloid if needed 2. Computes Fr\u00e9chet mean 3. Centers data around the mean 4. Optimizes principal components to maximize projected variance</p> <p>Args:     x: Input points of shape (n_points, n_in_features)         - For Poincar\u00e9: shape is (n_points, dim)         - For Hyperboloid: shape is (n_points, dim+1)</p> Source code in <code>hyperbolix/utils/horo_pca.py</code> <pre><code>def fit(self, x: Float[Array, \"n_points n_in_features\"]) -&gt; None:\n    \"\"\"Fit HoroPCA to the data by finding optimal principal components.\n\n    This method:\n    1. Converts data to hyperboloid if needed\n    2. Computes Fr\u00e9chet mean\n    3. Centers data around the mean\n    4. Optimizes principal components to maximize projected variance\n\n    Args:\n        x: Input points of shape (n_points, n_in_features)\n            - For Poincar\u00e9: shape is (n_points, dim)\n            - For Hyperboloid: shape is (n_points, dim+1)\n    \"\"\"\n    c = self.c[...]\n\n    # Convert to hyperboloid if needed\n    if self.manifold_name == \"poincare\":\n        # Map each Poincar\u00e9 point to hyperboloid\n        def poincare_to_hyperboloid(p):\n            # First, convert single Poincar\u00e9 point to hyperboloid\n            # We need to use the poincare module's conversion logic\n            # For now, we'll compute it manually using the conformal model\n            p_sqnorm = jnp.dot(p, p)\n            denom = 1.0 - c * p_sqnorm\n            x0 = (1.0 + c * p_sqnorm) / denom\n            x_rest = 2 * jnp.sqrt(c) * p / denom\n            return jnp.concatenate([x0[jnp.newaxis], x_rest])\n\n        x = jax.vmap(poincare_to_hyperboloid)(x)\n\n    # Compute Fr\u00e9chet mean\n    self.data_mean.set_value(compute_frechet_mean(x, c))\n\n    # Center data around Fr\u00e9chet mean\n    x_centered = center_data(x, self.data_mean[...], c)\n\n    # Set up optimizer\n    optimizer = optax.adam(self.lr)\n    opt_state = optimizer.init(self.Q)\n\n    loss_and_grad = jax.jit(jax.value_and_grad(_projected_variance_loss))\n    loss_history: list[float] = []\n\n    for _ in range(self.max_steps):\n        loss, grads = loss_and_grad(self.Q[...], x_centered, c)\n        loss_history.append(float(loss))\n\n        grads = jnp.clip(grads, -1e5, 1e5)\n\n        # Update parameters\n        updates, opt_state = optimizer.update(grads, opt_state)\n        updated_Q = cast(Float[Array, \"n_components dim_or_dim_minus_1\"], optax.apply_updates(self.Q[...], updates))\n        self.Q[...] = updated_Q\n\n    if loss_history:\n        self.loss_history.set_value(jnp.asarray(loss_history, dtype=jnp.float32))\n    else:\n        self.loss_history.set_value(jnp.zeros((0,), dtype=jnp.float32))\n</code></pre>"},{"location":"api-reference/utils/#hyperbolix.utils.horo_pca.HoroPCA.transform","title":"transform","text":"<pre><code>transform(\n    x: Float[Array, \"n_points n_in_features\"],\n    recompute_mean: bool = False,\n) -&gt; Float[Array, \"n_points n_components\"]\n</code></pre> <p>Project points onto the learned lower-dimensional submanifold.</p> <p>Args:     x: Input points of shape (n_points, n_in_features)     recompute_mean: If True, recompute Fr\u00e9chet mean (default: False)</p> <p>Returns:     Projected points of shape (n_points, n_components) in Poincar\u00e9 ball</p> <p>Notes:     The output is in the lower-dimensional Poincar\u00e9 ball, represented     by coordinates in the orthonormalized principal component basis.</p> Source code in <code>hyperbolix/utils/horo_pca.py</code> <pre><code>def transform(\n    self,\n    x: Float[Array, \"n_points n_in_features\"],\n    recompute_mean: bool = False,\n) -&gt; Float[Array, \"n_points n_components\"]:\n    \"\"\"Project points onto the learned lower-dimensional submanifold.\n\n    Args:\n        x: Input points of shape (n_points, n_in_features)\n        recompute_mean: If True, recompute Fr\u00e9chet mean (default: False)\n\n    Returns:\n        Projected points of shape (n_points, n_components) in Poincar\u00e9 ball\n\n    Notes:\n        The output is in the lower-dimensional Poincar\u00e9 ball, represented\n        by coordinates in the orthonormalized principal component basis.\n    \"\"\"\n    c = self.c[...]\n\n    # Convert to hyperboloid if needed\n    if self.manifold_name == \"poincare\":\n\n        def poincare_to_hyperboloid(p):\n            p_sqnorm = jnp.dot(p, p)\n            denom = 1.0 - c * p_sqnorm\n            x0 = (1.0 + c * p_sqnorm) / denom\n            x_rest = 2 * jnp.sqrt(c) * p / denom\n            return jnp.concatenate([x0[jnp.newaxis], x_rest])\n\n        x = jax.vmap(poincare_to_hyperboloid)(x)\n\n    # Compute or recompute mean\n    if recompute_mean or self.data_mean[...] is None:\n        self.data_mean.set_value(compute_frechet_mean(x, c))\n\n    # Center data\n    x_centered = center_data(x, self.data_mean[...], c)\n\n    # Orthonormalize principal components\n    Q_ortho, _ = jnp.linalg.qr(self.Q[...].T, mode=\"reduced\")\n    Q_ortho = Q_ortho.T  # shape: (n_components, dim)\n\n    # Map to hyperboloid ideals\n    hyperboloid_ideals = self._to_hyperboloid_ideals(Q_ortho)\n\n    # Project onto submanifold\n    x_proj = self._horo_projection(x_centered, hyperboloid_ideals, c)\n\n    # Convert back to Poincar\u00e9 ball\n    def hyperboloid_to_poincare(h):\n        # h = [h0, h1, h2, ...]\n        # Poincar\u00e9 point = h_rest / (sqrt(c) * (h0 + 1/sqrt(c)))\n        sqrt_c = jnp.sqrt(c)\n        h_rest = h[1:]\n        denom = sqrt_c * (h[0] + 1.0 / sqrt_c)\n        return h_rest / denom\n\n    x_poincare = jax.vmap(hyperboloid_to_poincare)(x_proj)  # shape: (n_points, dim)\n\n    # Compute coordinates in lower-dimensional Poincar\u00e9 ball\n    res = x_poincare @ Q_ortho.T  # shape: (n_points, n_components)\n\n    return res\n</code></pre>"},{"location":"api-reference/utils/#hyperbolix.utils.horo_pca.compute_frechet_mean","title":"compute_frechet_mean","text":"<pre><code>compute_frechet_mean(\n    x: Float[Array, \"n_points dim_plus_1\"],\n    c: Float[Array, \"\"] | float,\n    max_iters: int = 5000,\n    tol: float = 5e-06,\n    lr_candidates: tuple[float, ...] = (\n        0.01,\n        0.02,\n        0.005,\n        0.04,\n        0.0025,\n    ),\n) -&gt; Float[Array, \"1 dim_plus_1\"]\n</code></pre> <p>Compute the Fr\u00e9chet mean of hyperboloid points using gradient descent.</p> <p>The Fr\u00e9chet mean minimizes the sum of squared geodesic distances to all points. We iterate through multiple learning rates until convergence.</p> <p>Args:     x: Hyperboloid points of shape (n_points, dim+1)     c: Curvature parameter (positive scalar)     max_iters: Maximum optimization iterations per learning rate     tol: Convergence tolerance for gradient norm     lr_candidates: Learning rates to try sequentially</p> <p>Returns:     Fr\u00e9chet mean of shape (1, dim+1)</p> <p>Notes:     - Initializes with the centroid of squared Lorentzian distance     - Tries multiple learning rates sequentially until convergence     - Uses gradient descent with logmap/expmap for manifold optimization</p> Source code in <code>hyperbolix/utils/horo_pca.py</code> <pre><code>def compute_frechet_mean(\n    x: Float[Array, \"n_points dim_plus_1\"],\n    c: Float[Array, \"\"] | float,\n    max_iters: int = 5_000,\n    tol: float = 5e-6,\n    lr_candidates: tuple[float, ...] = (1e-2, 2e-2, 5e-3, 4e-2, 2.5e-3),\n) -&gt; Float[Array, \"1 dim_plus_1\"]:\n    \"\"\"Compute the Fr\u00e9chet mean of hyperboloid points using gradient descent.\n\n    The Fr\u00e9chet mean minimizes the sum of squared geodesic distances to all points.\n    We iterate through multiple learning rates until convergence.\n\n    Args:\n        x: Hyperboloid points of shape (n_points, dim+1)\n        c: Curvature parameter (positive scalar)\n        max_iters: Maximum optimization iterations per learning rate\n        tol: Convergence tolerance for gradient norm\n        lr_candidates: Learning rates to try sequentially\n\n    Returns:\n        Fr\u00e9chet mean of shape (1, dim+1)\n\n    Notes:\n        - Initializes with the centroid of squared Lorentzian distance\n        - Tries multiple learning rates sequentially until convergence\n        - Uses gradient descent with logmap/expmap for manifold optimization\n    \"\"\"\n    n_points = x.shape[0]\n\n    # Initialize mean as centroid of squared Lorentzian distance\n    # Sum all points\n    x_sum = jnp.sum(x, axis=0, keepdims=True)  # shape: (1, dim+1)\n\n    # Compute normalization: sqrt(c * |&lt;x_sum, x_sum&gt;_L|)\n    # _minkowski_inner expects 1D arrays, so squeeze and then add back dimension\n    mink_inner = hyperboloid._minkowski_inner(x_sum[0], x_sum[0])\n    denom = jnp.sqrt(c * jnp.abs(mink_inner))\n    mean_init = x_sum / denom\n\n    # Project onto hyperboloid\n    mean_init = jax.vmap(hyperboloid.proj, in_axes=(0, None))(mean_init, c)\n\n    has_converged = False\n    mean = mean_init\n\n    # Try multiple learning rates\n    for lr in lr_candidates:\n        mean = mean_init\n        for _ in range(max_iters):\n            # Compute logarithmic map of all points with respect to current mean\n            # logmap expects single points, so we vmap over the batch\n            log_x_fn = jax.vmap(hyperboloid.logmap, in_axes=(0, None, None))\n            log_x = log_x_fn(x, mean[0], c)  # shape: (n_points, dim+1)\n\n            # Sum tangent vectors and average\n            log_x_sum = jnp.sum(log_x, axis=0, keepdims=True)  # shape: (1, dim+1)\n            update = lr * log_x_sum / n_points\n\n            # Compute update norm for convergence check\n            update_norm = jnp.linalg.norm(update, axis=-1)\n\n            # Update mean using exponential map\n            mean = hyperboloid.expmap(update[0], mean[0], c)\n            mean = mean[jnp.newaxis, :]  # shape: (1, dim+1)\n\n            # Check convergence\n            if update_norm &lt; tol:\n                has_converged = True\n                break\n\n        if has_converged:\n            break\n\n    # If no learning rate converged, print warning (in JAX we just use the last candidate)\n    if not has_converged:\n        print(\n            \"compute_frechet_mean: No convergence with any learning rate. Using the best candidate mean.\",\n            flush=True,\n        )\n\n    return mean\n</code></pre>"},{"location":"api-reference/utils/#hyperbolix.utils.horo_pca.center_data","title":"center_data","text":"<pre><code>center_data(\n    x: Float[Array, \"n_points dim_plus_1\"],\n    mean: Float[Array, \"1 dim_plus_1\"],\n    c: Float[Array, \"\"] | float,\n) -&gt; Float[Array, \"n_points dim_plus_1\"]\n</code></pre> <p>Center hyperboloid points around their Fr\u00e9chet mean using Lorentz transformation.</p> <p>This function computes the Lorentz boost that maps the mean to the hyperboloid's origin and applies it to all data points.</p> <p>Args:     x: Hyperboloid points of shape (n_points, dim+1)     mean: Fr\u00e9chet mean of shape (1, dim+1)     c: Curvature parameter (positive scalar)</p> <p>Returns:     Centered hyperboloid points of shape (n_points, dim+1)</p> <p>Notes:     The Lorentz transformation is constructed from the velocity and gamma factor     of the mean point. This is more numerically stable than using logmap/expmap.</p> Source code in <code>hyperbolix/utils/horo_pca.py</code> <pre><code>def center_data(\n    x: Float[Array, \"n_points dim_plus_1\"],\n    mean: Float[Array, \"1 dim_plus_1\"],\n    c: Float[Array, \"\"] | float,\n) -&gt; Float[Array, \"n_points dim_plus_1\"]:\n    \"\"\"Center hyperboloid points around their Fr\u00e9chet mean using Lorentz transformation.\n\n    This function computes the Lorentz boost that maps the mean to the hyperboloid's\n    origin and applies it to all data points.\n\n    Args:\n        x: Hyperboloid points of shape (n_points, dim+1)\n        mean: Fr\u00e9chet mean of shape (1, dim+1)\n        c: Curvature parameter (positive scalar)\n\n    Returns:\n        Centered hyperboloid points of shape (n_points, dim+1)\n\n    Notes:\n        The Lorentz transformation is constructed from the velocity and gamma factor\n        of the mean point. This is more numerically stable than using logmap/expmap.\n    \"\"\"\n    # Extract mean components (squeeze from (1, dim+1) to (dim+1,))\n    mean_squeezed = mean[0]  # shape: (dim+1,)\n    mean_t = mean_squeezed[0:1]  # temporal component, shape: (1,)\n    mean_space = mean_squeezed[1:]  # spatial components, shape: (dim,)\n\n    # Compute Lorentz boost parameters\n    sqrt_c = jnp.sqrt(c)\n    gamma = mean_t * sqrt_c  # shape: (1,)\n    velocity = mean_space / mean_t  # shape: (dim,)\n\n    # Build Lorentz boost matrix (transposed version for right multiplication)\n    # Top-left block: gamma (scalar)\n    block_tl = gamma  # shape: (1,)\n\n    # Top-right block: -gamma * v\n    block_tr = -gamma * velocity  # shape: (dim,)\n\n    # Top row: [gamma, -gamma * v]\n    top_row = jnp.concatenate([block_tl, block_tr], axis=0)  # shape: (dim+1,)\n    top_row = top_row[jnp.newaxis, :]  # shape: (1, dim+1)\n\n    # Bottom-left block: -gamma * v^T\n    block_bl = block_tr[:, jnp.newaxis]  # shape: (dim, 1)\n\n    # Bottom-right block: I + (gamma^2 / (1 + gamma)) * v^T v\n    dim = mean_space.shape[0]\n    identity_n = jnp.eye(dim, dtype=x.dtype)\n    vTv = velocity[:, jnp.newaxis] @ velocity[jnp.newaxis, :]  # shape: (dim, dim)\n    coefficient = (gamma**2) / (1 + gamma)\n    block_br = identity_n + coefficient * vTv  # shape: (dim, dim)\n\n    # Bottom row: [-gamma * v^T, I + (gamma^2 / (1 + gamma)) * v^T v]\n    bottom_row = jnp.concatenate([block_bl, block_br], axis=1)  # shape: (dim, dim+1)\n\n    # Full Lorentz boost matrix\n    lorentz_boost = jnp.concatenate([top_row, bottom_row], axis=0)  # shape: (dim+1, dim+1)\n\n    # Apply Lorentz transformation: x @ L^T (or equivalently, L @ x^T)\n    res = x @ lorentz_boost  # shape: (n_points, dim+1)\n\n    # Project back onto hyperboloid\n    res = jax.vmap(hyperboloid.proj, in_axes=(0, None))(res, c)\n\n    return res\n</code></pre>"},{"location":"api-reference/utils/#usage-example_1","title":"Usage Example","text":"<pre><code>from flax import nnx\nfrom hyperbolix.utils.horo_pca import HoroPCA\nfrom hyperbolix.manifolds import poincare, hyperboloid\nimport jax.numpy as jnp\nimport jax\n\n# High-dimensional hyperbolic data\nkey = jax.random.PRNGKey(0)\ndata = jax.random.normal(key, (100, 10)) * 0.3\n\n# Project to Poincar\u00e9 ball\ndata_proj = jax.vmap(poincare.proj, in_axes=(0, None, None))(\n    data, 1.0, None\n)\n\n# Initialize HoroPCA\nhoro_pca = HoroPCA(\n    manifold_module=poincare,\n    n_components=3,  # Reduce to 3 dimensions\n    rngs=nnx.Rngs(0)\n)\n\n# Fit on data\nhoro_pca.fit(data_proj, c=1.0)\n\n# Transform new data\ndata_reduced = horo_pca.transform(data_proj, c=1.0)\nprint(data_reduced.shape)  # (100, 3)\n\n# Inverse transform (approximate reconstruction)\ndata_reconstructed = horo_pca.inverse_transform(data_reduced, c=1.0)\n</code></pre>"},{"location":"api-reference/utils/#manifold-support","title":"Manifold Support","text":"<p>HoroPCA supports both Poincar\u00e9 and Hyperboloid manifolds:</p> <pre><code># Poincar\u00e9 ball (conformal model)\nhoro_pca_poincare = HoroPCA(\n    manifold_module=poincare,\n    n_components=5,\n    rngs=rngs\n)\n\n# Hyperboloid (Lorentz model)\nhoro_pca_hyperboloid = HoroPCA(\n    manifold_module=hyperboloid,\n    n_components=5,\n    rngs=rngs\n)\n</code></pre>"},{"location":"api-reference/utils/#how-it-works","title":"How It Works","text":"<p>HoroPCA performs dimensionality reduction via:</p> <ol> <li>Fr\u00e9chet Mean: Compute geometric center via gradient descent</li> <li>Centering: Apply Lorentz boost to move data to origin</li> <li>Horospherical Projection: Project onto lower-dimensional horosphere</li> <li>Pseudoinverse: Use Moore-Penrose pseudoinverse for numerical stability</li> </ol> <p>This is more stable than standard PCA on manifolds and preserves hyperbolic structure better.</p>"},{"location":"api-reference/utils/#parameters","title":"Parameters","text":"<ul> <li><code>manifold_module</code>: Manifold module (poincare or hyperboloid)</li> <li><code>n_components</code>: Target dimensionality</li> <li><code>max_iter</code>: Maximum Fr\u00e9chet mean iterations (default: 100)</li> <li><code>tol</code>: Convergence tolerance (default: 1e-6)</li> </ul>"},{"location":"api-reference/utils/#performance-tips","title":"Performance Tips","text":"<p>JIT Compilation</p> <p>All utility functions support JIT compilation:</p> <pre><code>@jax.jit\ndef compute_all_distances(points, c):\n    return compute_pairwise_distances(\n        points,\n        manifold_module=poincare,\n        c=c,\n        version=0\n    )\n</code></pre> <p>Batching</p> <p>For large datasets, consider batching delta-hyperbolicity computation:</p> <pre><code># Use smaller sample_size for faster computation\ndelta, diameter, rel_delta = get_delta(\n    points,\n    manifold_module=poincare,\n    c=1.0,\n    sample_size=100,  # Reduce from 500 for speed\n    seed=42\n)\n</code></pre>"},{"location":"api-reference/utils/#references","title":"References","text":"<ul> <li>Gromov Delta: Gromov, M. (1987). \"Hyperbolic groups.\"</li> <li>HoroPCA: Chami, I., et al. (2021). \"Low-Distortion Embeddings of Hyperbolic Spaces.\"</li> </ul> <p>See also:</p> <ul> <li>Manifolds API: Core geometric operations</li> <li>Numerical Stability Guide: Best practices</li> </ul>"},{"location":"user-guide/batching-jit/","title":"Batching &amp; JIT Guide","text":"<p>Efficient JAX patterns for hyperbolic deep learning with vmap-native APIs and JIT compilation.</p>"},{"location":"user-guide/batching-jit/#overview","title":"Overview","text":"<p>Hyperbolix adopts a vmap-native API design where all manifold functions operate on single points/vectors. This design provides maximum flexibility and composability with JAX's transformation system.</p> <p>Key Design Principles</p> <ul> <li>Functions operate on single points with shape <code>(dim,)</code> or <code>(dim+1,)</code> (ambient)</li> <li>Use <code>jax.vmap</code> for batch operations</li> <li>Use <code>jax.jit</code> for compilation with appropriate static arguments</li> <li>No built-in <code>axis</code> or <code>keepdim</code> parameters \u2014 compose transformations explicitly</li> </ul>"},{"location":"user-guide/batching-jit/#the-vmap-native-api","title":"The vmap-Native API","text":""},{"location":"user-guide/batching-jit/#single-point-operations","title":"Single Point Operations","text":"<p>All manifold functions work with individual points:</p> <pre><code>import jax.numpy as jnp\nfrom hyperbolix.manifolds import poincare\n\n# Single points (intrinsic coordinates)\nx = jnp.array([0.1, 0.2])  # Shape: (2,)\ny = jnp.array([0.3, 0.4])  # Shape: (2,)\n\n# Compute distance between two points\ndistance = poincare.dist(x, y, c=1.0, version_idx=poincare.VERSION_MOBIUS_DIRECT)\nprint(distance)  # Scalar\n\n# Exponential map from origin\nv = jnp.array([0.5, 0.0])  # Tangent vector at origin\npoint = poincare.expmap_0(v, c=1.0)\nprint(point.shape)  # (2,)\n</code></pre>"},{"location":"user-guide/batching-jit/#batching-with-vmap","title":"Batching with vmap","text":"<p>Use <code>jax.vmap</code> to process batches efficiently:</p> <pre><code>import jax\n\n# Batch of points\nx_batch = jnp.array([[0.1, 0.2], [0.15, 0.25], [0.05, 0.1]])  # (3, 2)\ny_batch = jnp.array([[0.3, 0.4], [0.35, 0.45], [0.2, 0.3]])   # (3, 2)\n\n# Option 1: Explicit vmap\ndist_fn = jax.vmap(poincare.dist, in_axes=(0, 0, None, None))\ndistances = dist_fn(x_batch, y_batch, 1.0, poincare.VERSION_MOBIUS_DIRECT)\nprint(distances.shape)  # (3,)\n\n# Option 2: Inline vmap\ndistances = jax.vmap(\n    lambda x, y: poincare.dist(x, y, c=1.0, version_idx=poincare.VERSION_MOBIUS_DIRECT)\n)(x_batch, y_batch)\n</code></pre>"},{"location":"user-guide/batching-jit/#understanding-in_axes","title":"Understanding in_axes","text":"<p>The <code>in_axes</code> parameter specifies which axes to map over:</p> <pre><code># in_axes=(0, 0, None, None) means:\n# - Map over axis 0 of first argument (x_batch)\n# - Map over axis 0 of second argument (y_batch)\n# - Don't map over curvature (c) \u2014 use same value for all\n# - Don't map over version_idx \u2014 static argument\n</code></pre> <p>Common patterns:</p> <pre><code># Project batch of points\nx_batch = jax.random.normal(jax.random.PRNGKey(0), (100, 16))\nx_proj = jax.vmap(poincare.proj, in_axes=(0, None))(x_batch, 1.0)\n\n# Compute distances from single point to batch\norigin = jnp.zeros(16)\nx_batch = jax.random.normal(jax.random.PRNGKey(0), (100, 16)) * 0.3\ndistances = jax.vmap(\n    lambda x: poincare.dist(origin, x, c=1.0, version_idx=poincare.VERSION_MOBIUS_DIRECT)\n)(x_batch)\nprint(distances.shape)  # (100,)\n\n# Exponential map with batch of tangent vectors\nv_batch = jax.random.normal(jax.random.PRNGKey(0), (100, 16))\nbase_point = jnp.zeros(16)\npoints = jax.vmap(\n    lambda v: poincare.expmap(base_point, v, c=1.0)\n)(v_batch)\nprint(points.shape)  # (100, 16)\n</code></pre>"},{"location":"user-guide/batching-jit/#jit-compilation","title":"JIT Compilation","text":""},{"location":"user-guide/batching-jit/#basic-jit-usage","title":"Basic JIT Usage","text":"<p>Use <code>jax.jit</code> to compile functions for 10-100x speedup:</p> <pre><code># Without JIT\ndistance = poincare.dist(x, y, c=1.0, version_idx=poincare.VERSION_MOBIUS_DIRECT)\n\n# With JIT\ndist_jit = jax.jit(poincare.dist, static_argnames=['version_idx'])\ndistance = dist_jit(x, y, c=1.0, version_idx=poincare.VERSION_MOBIUS_DIRECT)\n</code></pre> <p>JIT Performance</p> <ul> <li>First call: Slow (compilation overhead, 100ms-1s)</li> <li>Subsequent calls: Fast (10-100x speedup)</li> <li>Most beneficial for large batches (1000+) and high dimensions (128+)</li> </ul>"},{"location":"user-guide/batching-jit/#static-vs-dynamic-arguments","title":"Static vs Dynamic Arguments","text":"<p>Static arguments are known at compile time and trigger recompilation if changed:</p> <pre><code># version_idx is static (integer constant)\ndist_jit = jax.jit(poincare.dist, static_argnames=['version_idx'])\n\n# These compile once and reuse:\nd1 = dist_jit(x1, y1, c=1.0, version_idx=0)\nd2 = dist_jit(x2, y2, c=1.5, version_idx=0)  # Reuses compilation\n\n# This triggers recompilation (different version_idx):\nd3 = dist_jit(x3, y3, c=1.0, version_idx=1)\n</code></pre> <p>Dynamic arguments can change without recompilation:</p> <pre><code># Curvature 'c' is dynamic (can vary)\nd1 = dist_jit(x1, y1, c=1.0, version_idx=0)\nd2 = dist_jit(x2, y2, c=2.5, version_idx=0)  # No recompilation needed\n</code></pre> <p>Learnable Curvature</p> <p>Keep curvature parameter <code>c</code> dynamic (not static) to support gradient-based learning of curvature values during training.</p>"},{"location":"user-guide/batching-jit/#combining-vmap-and-jit","title":"Combining vmap and jit","text":"<p>The order matters for performance:</p> <pre><code># Pattern 1: JIT then vmap (RECOMMENDED)\n@jax.jit\ndef distance_fn(x, y, c):\n    return poincare.dist(x, y, c, version_idx=poincare.VERSION_MOBIUS_DIRECT)\n\ndistances = jax.vmap(distance_fn, in_axes=(0, 0, None))(x_batch, y_batch, 1.0)\n\n# Pattern 2: vmap then JIT\ndist_batched = jax.vmap(poincare.dist, in_axes=(0, 0, None, None))\ndistances = jax.jit(dist_batched, static_argnames=['version_idx'])(\n    x_batch, y_batch, 1.0, poincare.VERSION_MOBIUS_DIRECT\n)\n\n# Pattern 3: Combined (one-liner)\ndistances = jax.jit(\n    jax.vmap(poincare.dist, in_axes=(0, 0, None, None)),\n    static_argnames=['version_idx']\n)(x_batch, y_batch, 1.0, poincare.VERSION_MOBIUS_DIRECT)\n</code></pre> <p>Best Practice</p> <p>JIT the inner function and vmap the outer function for best performance and flexibility.</p>"},{"location":"user-guide/batching-jit/#neural-network-patterns","title":"Neural Network Patterns","text":""},{"location":"user-guide/batching-jit/#forward-pass","title":"Forward Pass","text":"<p>Flax NNX layers automatically handle batching:</p> <pre><code>from flax import nnx\nfrom hyperbolix.nn_layers import HypLinearPoincare\nfrom hyperbolix.manifolds import poincare\n\n# Create layer\nlayer = HypLinearPoincare(\n    manifold_module=poincare,\n    in_dim=128,\n    out_dim=64,\n    rngs=nnx.Rngs(0)\n)\n\n# Batch input: (batch_size, in_dim)\nx_batch = jax.random.normal(nnx.Rngs(1).params(), (32, 128)) * 0.3\nx_proj = jax.vmap(poincare.proj, in_axes=(0, None))(x_batch, 1.0)\n\n# Forward pass handles batching internally\noutput = layer(x_proj, c=1.0)\nprint(output.shape)  # (32, 64)\n</code></pre>"},{"location":"user-guide/batching-jit/#activations-with-vmap","title":"Activations with vmap","text":"<p>Hyperbolic activations are functional and need explicit batching:</p> <pre><code>from hyperbolix.nn_layers import hyp_relu\nfrom hyperbolix.manifolds import hyperboloid\n\n# Single point\nx = jnp.array([1.5, 0.2, 0.3, 0.1])  # Ambient coordinates (4,)\nactivated = hyp_relu(x, c=1.0)\n\n# Batch of points - use vmap\nx_batch = jax.random.normal(jax.random.PRNGKey(0), (32, 4))\nactivated_batch = jax.vmap(lambda x: hyp_relu(x, c=1.0))(x_batch)\nprint(activated_batch.shape)  # (32, 4)\n\n# Or directly (hyp_relu supports arbitrary shapes)\nactivated_batch = hyp_relu(x_batch, c=1.0)  # Also works!\n</code></pre>"},{"location":"user-guide/batching-jit/#complete-model-with-jit","title":"Complete Model with JIT","text":"<pre><code>from hyperbolix.nn_layers import HypLinearPoincare\nfrom hyperbolix.manifolds import poincare\n\nclass HyperbolicClassifier(nnx.Module):\n    def __init__(self, rngs):\n        self.layer1 = HypLinearPoincare(poincare, 784, 256, rngs=rngs)\n        self.layer2 = HypLinearPoincare(poincare, 256, 128, rngs=rngs)\n        self.layer3 = HypLinearPoincare(poincare, 128, 10, rngs=rngs)\n\n    def __call__(self, x, c=1.0):\n        x = self.layer1(x, c)\n        # vmap activation over batch\n        x = jax.vmap(lambda xi: hyp_relu(xi, c))(x)\n\n        x = self.layer2(x, c)\n        x = jax.vmap(lambda xi: hyp_relu(xi, c))(x)\n\n        x = self.layer3(x, c)\n        return x\n\n# Create model\nmodel = HyperbolicClassifier(rngs=nnx.Rngs(0))\n\n# JIT the forward pass\n@jax.jit\ndef forward(model, x, c):\n    return model(x, c)\n\n# Use with batch\nx_batch = jax.random.normal(nnx.Rngs(1).params(), (32, 784)) * 0.1\nx_proj = jax.vmap(poincare.proj, in_axes=(0, None))(x_batch, 1.0)\nlogits = forward(model, x_proj, c=1.0)\nprint(logits.shape)  # (32, 10)\n</code></pre>"},{"location":"user-guide/batching-jit/#training-loop-patterns","title":"Training Loop Patterns","text":""},{"location":"user-guide/batching-jit/#value-and-gradient-computation","title":"Value and Gradient Computation","text":"<pre><code># Loss function for single example\ndef loss_fn_single(params, x, y, c):\n    pred = model.apply(params, x, c)\n    return jnp.mean((pred - y) ** 2)\n\n# Batched loss with vmap\ndef loss_fn_batch(params, x_batch, y_batch, c):\n    losses = jax.vmap(loss_fn_single, in_axes=(None, 0, 0, None))(\n        params, x_batch, y_batch, c\n    )\n    return jnp.mean(losses)\n\n# Compute gradients\ngrad_fn = jax.grad(loss_fn_batch)\ngrads = grad_fn(params, x_batch, y_batch, c=1.0)\n\n# Or use value_and_grad for both\n@jax.jit\ndef compute_loss_and_grads(params, x_batch, y_batch, c):\n    return jax.value_and_grad(loss_fn_batch)(params, x_batch, y_batch, c)\n\nloss, grads = compute_loss_and_grads(params, x_batch, y_batch, c=1.0)\n</code></pre>"},{"location":"user-guide/batching-jit/#efficient-training-step","title":"Efficient Training Step","text":"<pre><code>@jax.jit\ndef train_step(model, optimizer, x_batch, y_batch, c):\n    \"\"\"Single training step with JIT compilation.\"\"\"\n    def loss_fn(model):\n        preds = model(x_batch, c)\n        return jnp.mean((preds - y_batch) ** 2)\n\n    loss, grads = nnx.value_and_grad(loss_fn)(model)\n    optimizer.update(grads)\n    return loss\n\n# Training loop\nfor epoch in range(num_epochs):\n    for x_batch, y_batch in dataloader:\n        # Project to manifold\n        x_batch = jax.vmap(poincare.proj, in_axes=(0, None))(x_batch, 1.0)\n\n        # Single JIT-compiled step\n        loss = train_step(model, optimizer, x_batch, y_batch, c=1.0)\n\n        print(f\"Loss: {loss:.4f}\")\n</code></pre>"},{"location":"user-guide/batching-jit/#performance-optimization-tips","title":"Performance Optimization Tips","text":""},{"location":"user-guide/batching-jit/#1-profile-before-optimizing","title":"1. Profile Before Optimizing","text":"<pre><code>import time\n\n# Warmup JIT compilation\n_ = dist_jit(x, y, c=1.0, version_idx=0)\n\n# Time subsequent calls\nstart = time.time()\nfor _ in range(1000):\n    _ = dist_jit(x, y, c=1.0, version_idx=0)\nelapsed = time.time() - start\nprint(f\"Time per call: {elapsed/1000*1e6:.2f} \u00b5s\")\n</code></pre>"},{"location":"user-guide/batching-jit/#2-minimize-recompilation","title":"2. Minimize Recompilation","text":"<pre><code># BAD: Different shapes trigger recompilation\nd1 = dist_jit(x1, y1, c=1.0, version_idx=0)  # Compile for shape (16,)\nd2 = dist_jit(x2, y2, c=1.0, version_idx=0)  # Recompile for shape (32,)\n\n# GOOD: Use consistent shapes\nx_batch = jnp.array([[0.1, 0.2], [0.3, 0.4]])\ndistances = jax.vmap(dist_jit, in_axes=(0, 0, None, None))(\n    x_batch[:, 0], x_batch[:, 1], 1.0, poincare.VERSION_MOBIUS_DIRECT\n)\n</code></pre>"},{"location":"user-guide/batching-jit/#3-use-static-arguments-appropriately","title":"3. Use Static Arguments Appropriately","text":"<pre><code># GOOD: Keep curvature dynamic\n@jax.jit\ndef process_batch(x_batch, c):\n    return jax.vmap(\n        lambda x: poincare.proj(x, c)  # Simple projection\n    )(x_batch)\n\n# BAD: Making everything static reduces flexibility\n@jax.jit\ndef process_batch_bad(x_batch):  # c=1.0 hardcoded\n    return jax.vmap(\n        lambda x: poincare.proj(x, c=1.0)\n    )(x_batch)  # Can't change curvature without recompilation\n</code></pre>"},{"location":"user-guide/batching-jit/#4-batch-size-considerations","title":"4. Batch Size Considerations","text":"<pre><code># Small batches: Less JIT benefit\nx_small = jax.random.normal(jax.random.PRNGKey(0), (10, 128))\n# ~10-20x speedup\n\n# Large batches: Maximum JIT benefit\nx_large = jax.random.normal(jax.random.PRNGKey(0), (1000, 128))\n# ~50-100x speedup\n</code></pre>"},{"location":"user-guide/batching-jit/#5-memory-vs-computation-trade-offs","title":"5. Memory vs Computation Trade-offs","text":"<pre><code># Memory-efficient: Process in chunks\ndef process_large_batch(x_batch, chunk_size=1000):\n    n = len(x_batch)\n    results = []\n    for i in range(0, n, chunk_size):\n        chunk = x_batch[i:i+chunk_size]\n        results.append(jax.vmap(some_fn)(chunk))\n    return jnp.concatenate(results)\n\n# Compute-efficient: Process all at once (may OOM)\ndef process_all_at_once(x_batch):\n    return jax.vmap(some_fn)(x_batch)\n</code></pre>"},{"location":"user-guide/batching-jit/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"user-guide/batching-jit/#pitfall-1-forgetting-to-vmap-activations","title":"Pitfall 1: Forgetting to vmap Activations","text":"<pre><code># WRONG: Activation expects single point\nx = layer(x_batch, c=1.0)  # (batch, dim+1) for hyperboloid\nactivated = hyp_relu(x, c=1.0)  # May work but semantics unclear\n\n# CORRECT: Explicit vmap\nactivated = jax.vmap(lambda xi: hyp_relu(xi, c=1.0))(x)\n\n# ALSO CORRECT: hyp_relu handles batches\nactivated = hyp_relu(x, c=1.0)  # Directly works on (batch, dim+1)\n</code></pre>"},{"location":"user-guide/batching-jit/#pitfall-2-shape-mismatches-with-vmap","title":"Pitfall 2: Shape Mismatches with vmap","text":"<pre><code># WRONG: Incompatible in_axes\nx_batch = jnp.array([[0.1, 0.2]])  # (1, 2)\ny_batch = jnp.array([[0.3, 0.4]])  # (1, 2)\nc_batch = jnp.array([1.0, 1.5])    # (2,)\n\ndistances = jax.vmap(poincare.dist, in_axes=(0, 0, 0))(\n    x_batch, y_batch, c_batch  # Shape mismatch: (1,) vs (2,)\n)\n\n# CORRECT: Broadcast curvature or use same value\ndistances = jax.vmap(poincare.dist, in_axes=(0, 0, None))(\n    x_batch, y_batch, 1.0\n)\n</code></pre>"},{"location":"user-guide/batching-jit/#pitfall-3-static-curvature","title":"Pitfall 3: Static Curvature","text":"<pre><code># WRONG: Can't learn curvature\n@jax.jit\ndef model_forward(x, c=1.0):  # c fixed at compile time\n    return poincare.proj(x, c)\n\n# CORRECT: Keep c dynamic\n@jax.jit\ndef model_forward(x, c):  # c can vary\n    return poincare.proj(x, c)\n</code></pre>"},{"location":"user-guide/batching-jit/#benchmark-results","title":"Benchmark Results","text":"<p>Typical speedups on M1/M2 Mac or modern GPU:</p> Operation Batch Size No JIT With JIT Speedup Distance (dim=128) 100 12 ms 0.8 ms 15x Distance (dim=128) 1000 120 ms 1.5 ms 80x Expmap (dim=256) 100 18 ms 1.2 ms 15x Linear layer forward 1000 45 ms 2.1 ms 21x Full model (3 layers) 1000 150 ms 6.5 ms 23x <p>Run benchmarks yourself:</p> <pre><code>uv run pytest benchmarks/ --benchmark-only -v\n</code></pre>"},{"location":"user-guide/batching-jit/#see-also","title":"See Also","text":"<ul> <li>Manifolds API: Manifold function signatures</li> <li>NN Layers API: Layer implementations</li> <li>Training Workflows: Complete training examples</li> <li>Numerical Stability: Float precision considerations</li> </ul>"},{"location":"user-guide/manifolds/","title":"Manifolds User Guide","text":"<p>Detailed guide on working with manifolds in Hyperbolix.</p> <p>Work in Progress</p> <p>This page is under construction. See the API Reference for complete documentation.</p>"},{"location":"user-guide/manifolds/#topics","title":"Topics","text":"<ul> <li>Distance computation strategies</li> <li>Exponential and logarithmic maps</li> <li>Parallel transport</li> <li>Choosing the right manifold</li> <li>Numerical stability considerations</li> </ul> <p>[Content coming soon]</p>"},{"location":"user-guide/nn-layers/","title":"Neural Network Layers User Guide","text":"<p>Guide to building hyperbolic neural networks.</p> <p>Work in Progress</p> <p>This page is under construction. See the API Reference for complete documentation.</p>"},{"location":"user-guide/nn-layers/#topics","title":"Topics","text":"<ul> <li>Layer architecture patterns</li> <li>Hyperbolic convolutions (HCat operation)</li> <li>Activation functions</li> <li>Model composition</li> <li>Best practices</li> </ul> <p>[Content coming soon]</p>"},{"location":"user-guide/numerical-stability/","title":"Numerical Stability Guide","text":"<p>Best practices for maintaining numerical precision in hyperbolic operations.</p>"},{"location":"user-guide/numerical-stability/#overview","title":"Overview","text":"<p>Hyperbolic geometry presents unique numerical challenges due to the exponential growth of the conformal factor near the boundary and the involvement of hyperbolic functions (cosh, sinh, atanh). This guide explains these challenges and provides strategies to maintain numerical stability.</p> <p>Key Challenges</p> <ul> <li>Conformal factor explosion: \u03bb(x) grows exponentially as points approach the boundary</li> <li>Float32 limitations: ~7 significant digits, insufficient for large distances (&gt;10)</li> <li>Hyperbolic function overflow: cosh/sinh overflow for large arguments</li> <li>Division by near-zero: Operations involving 1 - c||x||\u00b2 near the boundary</li> </ul>"},{"location":"user-guide/numerical-stability/#float-precision-float32-vs-float64","title":"Float Precision: Float32 vs Float64","text":""},{"location":"user-guide/numerical-stability/#when-to-use-each","title":"When to Use Each","text":"<p>Float32 (default): - Sufficient for most applications with small to moderate distances (&lt; 5) - 2-4x faster on GPU - Lower memory footprint (important for large models) - ~7 significant decimal digits</p> <p>Float64 (high precision): - Required for large distances (&gt; 10) or near-boundary points - Better numerical stability in edge cases - ~15-16 significant decimal digits - Use for research, validation, or stability-critical applications</p> <pre><code>import jax.numpy as jnp\nfrom hyperbolix.manifolds import poincare\n\n# Float32 (default)\nx = jnp.array([0.1, 0.2], dtype=jnp.float32)\ny = jnp.array([0.8, 0.5], dtype=jnp.float32)\ndist = poincare.dist(x, y, c=1.0, version_idx=0)\n\n# Float64 (high precision)\nx = jnp.array([0.1, 0.2], dtype=jnp.float64)\ny = jnp.array([0.8, 0.5], dtype=jnp.float64)\ndist = poincare.dist(x, y, c=1.0, version_idx=0)\n</code></pre>"},{"location":"user-guide/numerical-stability/#precision-requirements-by-distance","title":"Precision Requirements by Distance","text":"Distance from Origin Float32 Accuracy Recommended Precision d &lt; 3 Excellent (&lt; 0.01% error) float32 3 \u2264 d &lt; 5 Good (&lt; 0.1% error) float32 5 \u2264 d &lt; 10 Moderate (&lt; 3% error) float64 for critical ops d \u2265 10 Poor (&gt; 3% error) float64 required <p>Quick Check</p> <p>If your embeddings have distances from the origin &gt; 7, switch to float64:</p> <pre><code>distances = jax.vmap(lambda x: poincare.dist(jnp.zeros_like(x), x, c=1.0, version_idx=0))(x_batch)\nmax_dist = jnp.max(distances)\nprint(f\"Max distance from origin: {max_dist:.2f}\")\n# If &gt; 7, consider float64\n</code></pre>"},{"location":"user-guide/numerical-stability/#the-conformal-factor-problem","title":"The Conformal Factor Problem","text":""},{"location":"user-guide/numerical-stability/#understanding-x","title":"Understanding \u03bb(x)","text":"<p>The conformal factor in Poincar\u00e9 ball geometry is:</p> \\[ \\lambda(x) = \\frac{2}{1 - c||x||^2} \\] <p>This factor appears in: - Exponential map: scales tangent vectors - Logarithmic map: scales back to tangent space - Riemannian gradient: converts Euclidean to Riemannian gradients</p>"},{"location":"user-guide/numerical-stability/#exponential-growth","title":"Exponential Growth","text":"<p>As points move toward the boundary (||x|| \u2192 1/\u221ac), \u03bb(x) explodes:</p> <pre><code>import jax.numpy as jnp\nfrom hyperbolix.manifolds import poincare\n\nc = 1.0\ndistances = [0, 1, 2, 3, 5, 7, 10]\n\nfor d in distances:\n    # Point at distance d from origin\n    x = poincare.expmap_0(jnp.array([d, 0.0]), c=c)\n    norm = jnp.linalg.norm(x)\n    lambda_x = 2.0 / (1.0 - c * norm**2)\n    print(f\"d={d:2d}: ||x||={norm:.6f}, \u03bb(x)={lambda_x:10.1f}\")\n</code></pre> <p>Output: <pre><code>d= 0: ||x||=0.000000, \u03bb(x)=       2.0\nd= 1: ||x||=0.761594, \u03bb(x)=       3.6\nd= 2: ||x||=0.964028, \u03bb(x)=      27.7\nd= 3: ||x||=0.995055, \u03bb(x)=     202.0\nd= 5: ||x||=0.999909, \u03bb(x)=   11013.2\nd= 7: ||x||=0.999991, \u03bb(x)= 1096633.2\nd=10: ||x||=1.000000, \u03bb(x)=       inf\n</code></pre></p>"},{"location":"user-guide/numerical-stability/#numerical-issues","title":"Numerical Issues","text":"<p>Problem 1: Precision loss in logmap</p> <pre><code># logmap divides by \u03bb(x), then later operations multiply by \u03bb(x)\n# With float32 and \u03bb(x) \u2248 10,000:\n# - Division by 10,000 loses 4 digits of precision\n# - Multiplication by 10,000 doesn't recover them\n# Result: ~3 digits of precision remaining (out of 7)\n</code></pre> <p>Problem 2: Cancellation in 1 - c||x||\u00b2</p> <pre><code># Near boundary: ||x||\u00b2 \u2248 0.999999\n# Computing 1 - c||x||\u00b2 loses significant digits due to catastrophic cancellation\n# Float32: 1.0 - 0.999999 = 0.000001 (but stored imprecisely!)\n</code></pre>"},{"location":"user-guide/numerical-stability/#mitigation-strategies","title":"Mitigation Strategies","text":"<p>1. Use projection after operations</p> <pre><code>from hyperbolix.manifolds import poincare\n\n# After addition or other operations\nresult = poincare.add(x, y, c=1.0)\nresult = poincare.proj(result, c=1.0)  # Project back to manifold\n</code></pre> <p>2. Keep points away from boundary</p> <pre><code># During initialization\ndef init_hyperbolic_embeddings(n_points, dim, max_norm=0.8):\n    \"\"\"Initialize embeddings safely away from boundary.\"\"\"\n    x = jax.random.normal(key, (n_points, dim)) * 0.1\n    x_proj = jax.vmap(poincare.proj, in_axes=(0, None, None))(x, c=1.0, version_idx=None)\n\n    # Clip to max_norm to avoid boundary\n    norms = jnp.linalg.norm(x_proj, axis=-1, keepdims=True)\n    x_clipped = jnp.where(norms &gt; max_norm, x_proj * max_norm / norms, x_proj)\n    return x_clipped\n</code></pre> <p>3. Use float64 for critical operations</p> <pre><code># Convert to float64 for numerically sensitive operations\nx_f64 = x.astype(jnp.float64)\ny_f64 = y.astype(jnp.float64)\n\ndist_precise = poincare.dist(x_f64, y_f64, c=1.0, version_idx=0)\n\n# Convert back if needed\ndist_f32 = dist_precise.astype(jnp.float32)\n</code></pre>"},{"location":"user-guide/numerical-stability/#hyperbolic-function-overflow","title":"Hyperbolic Function Overflow","text":""},{"location":"user-guide/numerical-stability/#the-problem","title":"The Problem","text":"<p>Standard implementations of cosh, sinh can overflow:</p> <pre><code># Standard numpy/jax\nimport jax.numpy as jnp\n\nx = jnp.array(100.0, dtype=jnp.float32)\nprint(jnp.cosh(x))  # inf (overflow!)\nprint(jnp.sinh(x))  # inf (overflow!)\n</code></pre>"},{"location":"user-guide/numerical-stability/#solution-protected-math-utils","title":"Solution: Protected Math Utils","text":"<p>Hyperbolix provides overflow-protected hyperbolic functions:</p> <pre><code>from hyperbolix.utils.math_utils import cosh, sinh, acosh, atanh\n\n# Protected versions\nx = jnp.array(100.0, dtype=jnp.float32)\nprint(cosh(x))  # Finite value (clamped to safe range)\nprint(sinh(x))  # Finite value (clamped to safe range)\n\n# Domain-protected inverse functions\ny = jnp.array(0.5, dtype=jnp.float32)\nprint(acosh(y))  # Clamped to valid domain [1, inf)\n\nz = jnp.array(0.999999, dtype=jnp.float32)\nprint(atanh(z))  # Clamped away from \u00b11 singularities\n</code></pre>"},{"location":"user-guide/numerical-stability/#smooth-clamping","title":"Smooth Clamping","text":"<p>The library uses smooth clamping via softplus instead of hard clipping:</p> <pre><code>from hyperbolix.utils.math_utils import smooth_clamp\n\n# Smooth clamp (differentiable, no gradient issues)\nx = jnp.array([-10.0, -1.0, 0.0, 1.0, 10.0])\nclamped = smooth_clamp(x, min_value=-5.0, max_value=5.0, smoothing_factor=50.0)\nprint(clamped)\n# Near boundaries: smooth transition, not abrupt cutoff\n</code></pre> <p>Benefits: - Differentiable everywhere (no gradient discontinuities) - Numerically stable (uses softplus internally) - Adjustable smoothing factor for trade-off between accuracy and gradient flow</p>"},{"location":"user-guide/numerical-stability/#version-parameters","title":"Version Parameters","text":""},{"location":"user-guide/numerical-stability/#purpose","title":"Purpose","text":"<p>Many manifold operations have multiple mathematically equivalent formulations that differ in numerical properties. The <code>version_idx</code> parameter selects which to use.</p>"},{"location":"user-guide/numerical-stability/#poincare-ball-distance-versions","title":"Poincar\u00e9 Ball Distance Versions","text":"<pre><code>from hyperbolix.manifolds import poincare\n\nx = jnp.array([0.1, 0.2])\ny = jnp.array([0.3, 0.4])\nc = 1.0\n\n# Version 0: Direct M\u00f6bius distance (FASTEST)\nd0 = poincare.dist(x, y, c, version_idx=poincare.VERSION_MOBIUS_DIRECT)\n\n# Version 1: M\u00f6bius via addition\nd1 = poincare.dist(x, y, c, version_idx=poincare.VERSION_MOBIUS)\n\n# Version 2: Metric tensor induced\nd2 = poincare.dist(x, y, c, version_idx=poincare.VERSION_METRIC_TENSOR)\n\n# Version 3: Lorentzian proxy\nd3 = poincare.dist(x, y, c, version_idx=poincare.VERSION_LORENTZIAN_PROXY)\n\nprint(f\"Version 0: {d0:.6f}\")\nprint(f\"Version 1: {d1:.6f}\")\nprint(f\"Version 2: {d2:.6f}\")\nprint(f\"Version 3: {d3:.6f}\")\n# All should be approximately equal\n</code></pre>"},{"location":"user-guide/numerical-stability/#which-version-to-use","title":"Which Version to Use?","text":"<p>General recommendation: <code>VERSION_MOBIUS_DIRECT</code> (version 0) - Fastest - Fewest intermediate operations - Best for most applications</p> <p>Special cases: - Near-boundary points (||x|| &gt; 0.9): Try <code>VERSION_LORENTZIAN_PROXY</code> (version 3) for better stability - Very high dimensions (&gt; 1000): <code>VERSION_METRIC_TENSOR</code> (version 2) may be more stable - Debugging: Compare all versions \u2014 significant differences indicate numerical issues</p>"},{"location":"user-guide/numerical-stability/#using-versions-with-jit","title":"Using Versions with JIT","text":"<pre><code>import jax\n\n# IMPORTANT: version_idx must be static for JIT\n@jax.jit\ndef compute_distances(x_batch, y_batch, c):\n    # Version baked into function body (static)\n    return jax.vmap(\n        lambda x, y: poincare.dist(x, y, c, version_idx=0)\n    )(x_batch, y_batch)\n\n# Or use static_argnames\ndist_jit = jax.jit(poincare.dist, static_argnames=['version_idx'])\nd = dist_jit(x, y, c=1.0, version_idx=0)\n</code></pre>"},{"location":"user-guide/numerical-stability/#projection-strategies","title":"Projection Strategies","text":""},{"location":"user-guide/numerical-stability/#why-project","title":"Why Project?","text":"<p>Operations like addition, linear transformations can push points off the manifold. Projection restores the manifold constraint.</p>"},{"location":"user-guide/numerical-stability/#when-to-project","title":"When to Project","text":"<p>Always project: - After M\u00f6bius addition: <code>poincare.add()</code> - After neural network layers - After parameter updates in optimization</p> <p>Usually don't need projection: - After <code>expmap</code> (already on manifold) - After <code>proj</code> (redundant)</p>"},{"location":"user-guide/numerical-stability/#projection","title":"Projection","text":"<p>Projection ensures points stay on the manifold by clipping norms:</p> <pre><code># Project to Poincar\u00e9 ball\nx_proj = poincare.proj(x, c=1.0)\n\n# Projection is numerically stable and automatically handles edge cases\n</code></pre>"},{"location":"user-guide/numerical-stability/#projection-in-training","title":"Projection in Training","text":"<pre><code>from hyperbolix.manifolds import poincare\nfrom flax import nnx\n\nclass HyperbolicModel(nnx.Module):\n    def __init__(self, rngs):\n        self.layer1 = HypLinearPoincare(poincare, 128, 64, rngs=rngs)\n        self.layer2 = HypLinearPoincare(poincare, 64, 32, rngs=rngs)\n\n    def __call__(self, x, c=1.0):\n        x = self.layer1(x, c)\n        # Project after layer (layer already includes projection internally)\n\n        x = self.layer2(x, c)\n        # Final projection\n        x = jax.vmap(lambda xi: poincare.proj(xi, c))(x)\n        return x\n</code></pre> <p>Layer Projection</p> <p>Hyperbolix layers already project internally after operations, so explicit projection between layers is optional but recommended for extra safety.</p>"},{"location":"user-guide/numerical-stability/#common-edge-cases","title":"Common Edge Cases","text":""},{"location":"user-guide/numerical-stability/#edge-case-1-points-near-the-boundary","title":"Edge Case 1: Points Near the Boundary","text":"<p>Symptoms: NaN or Inf in gradients, exploding losses</p> <p>Solution: <pre><code># Check if points are too close to boundary\ndef check_boundary_proximity(x_batch, c=1.0):\n    norms = jnp.linalg.norm(x_batch, axis=-1)\n    max_norm = 1.0 / jnp.sqrt(c)\n    proximity = norms / max_norm\n\n    if jnp.any(proximity &gt; 0.95):\n        print(f\"WARNING: Points near boundary (max proximity: {jnp.max(proximity):.4f})\")\n        return True\n    return False\n\n# Clip if needed\ndef safe_clip_to_interior(x_batch, c=1.0, safety_factor=0.9):\n    max_allowed = safety_factor / jnp.sqrt(c)\n    norms = jnp.linalg.norm(x_batch, axis=-1, keepdims=True)\n    scale = jnp.minimum(1.0, max_allowed / (norms + 1e-8))\n    return x_batch * scale\n</code></pre></p>"},{"location":"user-guide/numerical-stability/#edge-case-2-zero-or-near-zero-vectors","title":"Edge Case 2: Zero or Near-Zero Vectors","text":"<p>Symptoms: Division by zero warnings, NaN in tangent operations</p> <p>Solution: <pre><code># Manifold functions handle this internally with MIN_NORM\n# But you can add explicit checks:\n\ndef safe_normalize(v, eps=1e-8):\n    norm = jnp.linalg.norm(v)\n    return jnp.where(norm &gt; eps, v / norm, jnp.zeros_like(v))\n</code></pre></p>"},{"location":"user-guide/numerical-stability/#edge-case-3-large-learning-rates","title":"Edge Case 3: Large Learning Rates","text":"<p>Symptoms: Points shoot to boundary, training collapse</p> <p>Solution: <pre><code># Use conservative learning rates\nfrom hyperbolix.optim import riemannian_adam\n\n# For Poincar\u00e9 ball\noptimizer = riemannian_adam(learning_rate=1e-3)  # Not 1e-2 or higher!\n\n# For Hyperboloid\noptimizer = riemannian_adam(learning_rate=5e-4)  # Even more conservative\n\n# Use learning rate scheduling\nfrom optax import exponential_decay\n\nschedule = exponential_decay(\n    init_value=1e-3,\n    transition_steps=1000,\n    decay_rate=0.96,\n    staircase=True\n)\noptimizer = riemannian_adam(learning_rate=schedule)\n</code></pre></p>"},{"location":"user-guide/numerical-stability/#edge-case-4-high-curvature-values","title":"Edge Case 4: High Curvature Values","text":"<p>Symptoms: Numerical instability, rapid convergence to boundary</p> <p>Solution: <pre><code># Keep curvature moderate\nc = 1.0  # Good default\n\n# High curvature (c &gt; 1) increases numerical challenges\nc = 0.1  # Lower curvature = larger hyperbolic space = more stable\n\n# If learning curvature, clip it\ndef clip_curvature(c, min_c=0.01, max_c=10.0):\n    return jnp.clip(c, min_c, max_c)\n</code></pre></p>"},{"location":"user-guide/numerical-stability/#checking-manifold-constraints","title":"Checking Manifold Constraints","text":""},{"location":"user-guide/numerical-stability/#validation-functions","title":"Validation Functions","text":"<p>Each manifold provides <code>is_in_manifold</code> for validation:</p> <pre><code>from hyperbolix.manifolds import poincare, hyperboloid\n\n# Poincar\u00e9 ball: ||x||\u00b2 &lt; 1/c\nx = jnp.array([0.5, 0.3])\nassert poincare.is_in_manifold(x, c=1.0, atol=1e-5)\n\n# Hyperboloid: x\u2080\u00b2 - \u03a3x\u1d62\u00b2 = 1/c\nx_ambient = jnp.array([1.5, 0.2, 0.3, 0.1])  # (dim+1,)\nassert hyperboloid.is_in_manifold(x_ambient, c=1.0, atol=1e-5)\n</code></pre>"},{"location":"user-guide/numerical-stability/#automated-checking-checkify","title":"Automated Checking (Checkify)","text":"<p>Use checkify modules for runtime validation:</p> <pre><code>from hyperbolix.manifolds import poincare_checked\nimport jax.experimental.checkify as checkify\n\n# Wrap computation\n@checkify.checkify\ndef safe_distance(x, y, c):\n    return poincare_checked.dist(x, y, c, version_idx=0)\n\n# Run with error checking\nerr, result = safe_distance(x, y, c=1.0)\nerr.throw()  # Raises exception if constraint violated\n</code></pre>"},{"location":"user-guide/numerical-stability/#batch-validation","title":"Batch Validation","text":"<pre><code>def validate_batch(x_batch, c=1.0, atol=1e-5):\n    \"\"\"Check if all points in batch satisfy manifold constraint.\"\"\"\n    valid = jax.vmap(lambda x: poincare.is_in_manifold(x, c, atol))(x_batch)\n    num_valid = jnp.sum(valid)\n    total = len(x_batch)\n\n    if num_valid &lt; total:\n        print(f\"WARNING: {total - num_valid}/{total} points off manifold\")\n        # Find violating points\n        violations = jnp.where(~valid)[0]\n        print(f\"Violating indices: {violations[:10]}\")  # Show first 10\n\n    return jnp.all(valid)\n</code></pre>"},{"location":"user-guide/numerical-stability/#best-practices-summary","title":"Best Practices Summary","text":"<p>Numerical Stability Checklist</p> <ul> <li>\u2705 Use float32 for distances &lt; 7, float64 for larger</li> <li>\u2705 Project after operations that might violate constraints</li> <li>\u2705 Keep points away from boundary (max norm &lt; 0.9/\u221ac)</li> <li>\u2705 Use conservative learning rates (&lt; 1e-3 for Poincar\u00e9, &lt; 5e-4 for Hyperboloid)</li> <li>\u2705 Use protected math functions (<code>hyperbolix.utils.math_utils</code>)</li> <li>\u2705 Monitor conformal factors during training</li> <li>\u2705 Validate manifold constraints in debugging</li> <li>\u2705 Use <code>VERSION_MOBIUS_DIRECT</code> for Poincar\u00e9 distance unless issues arise</li> <li>\u2705 Clip curvature if learnable (0.01 &lt; c &lt; 10.0)</li> <li>\u2705 Initialize embeddings conservatively (small norms)</li> </ul>"},{"location":"user-guide/numerical-stability/#debugging-numerical-issues","title":"Debugging Numerical Issues","text":""},{"location":"user-guide/numerical-stability/#step-by-step-diagnostic","title":"Step-by-Step Diagnostic","text":"<ol> <li> <p>Check for NaN/Inf:    <pre><code>assert jnp.all(jnp.isfinite(x_batch)), \"NaN or Inf detected in data\"\n</code></pre></p> </li> <li> <p>Verify manifold constraints:    <pre><code>validate_batch(x_batch, c=1.0, atol=1e-5)\n</code></pre></p> </li> <li> <p>Check boundary proximity:    <pre><code>check_boundary_proximity(x_batch, c=1.0)\n</code></pre></p> </li> <li> <p>Switch to float64:    <pre><code>x_batch = x_batch.astype(jnp.float64)\n</code></pre></p> </li> <li> <p>Try different version:    <pre><code># Try VERSION_LORENTZIAN_PROXY if VERSION_MOBIUS_DIRECT fails\ndist = poincare.dist(x, y, c, version_idx=3)\n</code></pre></p> </li> <li> <p>Enable checkify:    <pre><code># Use checked manifolds for runtime assertions\nfrom hyperbolix.manifolds import poincare_checked\n</code></pre></p> </li> </ol>"},{"location":"user-guide/numerical-stability/#see-also","title":"See Also","text":"<ul> <li>Batching &amp; JIT: Performance optimization patterns</li> <li>Manifolds API: Manifold function reference</li> <li>Training Workflows: End-to-end training examples</li> <li>Mathematical Background: Theory and formulas</li> </ul>"},{"location":"user-guide/optimizers/","title":"Riemannian Optimizers User Guide","text":"<p>Guide to training with Riemannian optimizers.</p> <p>Work in Progress</p> <p>This page is under construction. See the API Reference for complete documentation.</p>"},{"location":"user-guide/optimizers/#topics","title":"Topics","text":"<ul> <li>Manifold metadata system</li> <li>Mixed Euclidean/Riemannian optimization</li> <li>Expmap vs retraction</li> <li>Learning rate scheduling</li> <li>Convergence tips</li> </ul> <p>[Content coming soon]</p>"},{"location":"user-guide/training-workflows/","title":"Training Workflows Guide","text":"<p>End-to-end training examples and patterns.</p> <p>Work in Progress</p> <p>This page is under construction.</p>"},{"location":"user-guide/training-workflows/#topics","title":"Topics","text":"<ul> <li>Complete training loops</li> <li>Loss functions</li> <li>Validation strategies</li> <li>Integration with Optax</li> <li>Multi-GPU training</li> </ul> <p>[Content coming soon]</p>"}]}